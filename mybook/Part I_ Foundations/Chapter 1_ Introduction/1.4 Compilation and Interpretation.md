# 1.4 Compilation and Interpretation

1.4 Compilation and Interpretation

At the highest level of abstraction, the compilation and execution of a program in EXAMPLE 1.7

Pure compilation a high-level language look something like this:

Source program

Compiler

Input Target program Output

The compiler translates the high-level source program into an equivalent target program (typically in machine language), and then goes away. At some arbitrary later time, the user tells the operating system to run the target program. The com- piler is the locus of control during compilation; the target program is the locus of control during its own execution. The compiler is itself a machine language pro- gram, presumably created by compiling some other high-level program. When written to a ﬁle in a format understood by the operating system, machine lan- guage is commonly known as object code. ■ An alternative style of implementation for high-level languages is known as EXAMPLE 1.8

Pure interpretation interpretation:

Interpreter Source program

Output

Input

Unlike a compiler, an interpreter stays around for the execution of the appli- cation. In fact, the interpreter is the locus of control during that execution. In effect, the interpreter implements a virtual machine whose “machine language” is the high-level programming language. The interpreter reads statements in that language more or less one at a time, executing them as it goes along. ■ In general, interpretation leads to greater ﬂexibility and better diagnostics (er- ror messages) than does compilation. Because the source code is being executed directly, the interpreter can include an excellent source-level debugger. It can also cope with languages in which fundamental characteristics of the program, such as the sizes and types of variables, or even which names refer to which variables, can depend on the input data. Some language features are almost impossible to im- plement without interpretation: in Lisp and Prolog, for example, a program can write new pieces of itself and execute them on the ﬂy. (Several scripting languages also provide this capability.) Delaying decisions about program implementation until run time is known as late binding; we will discuss it at greater length in Section 3.1.

Compilation, by contrast, generally leads to better performance. In general, a decision made at compile time is a decision that does not need to be made at run time. For example, if the compiler can guarantee that variable x will always lie at location 49378, it can generate machine language instructions that access this location whenever the source program refers to x. By contrast, an interpreter may need to look x up in a table every time it is accessed, in order to ﬁnd its loca- tion. Since the (ﬁnal version of a) program is compiled only once, but generally executed many times, the savings can be substantial, particularly if the interpreter is doing unnecessary work in every iteration of a loop. While the conceptual difference between compilation and interpretation is EXAMPLE 1.9

Mixing compilation and interpretation clear, most language implementations include a mixture of both. They typically look like this:

Source program

Translator

Intermediate program

Input Output Virtual machine

We generally say that a language is “interpreted” when the initial translator is simple. If the translator is complicated, we say that the language is “compiled.” The distinction can be confusing because “simple” and “complicated” are sub- jective terms, and because it is possible for a compiler (complicated translator) to produce code that is then executed by a complicated virtual machine (inter- preter); this is in fact precisely what happens by default in Java. We still say that a language is compiled if the translator analyzes it thoroughly (rather than effecting some “mechanical” transformation), and if the intermediate program does not bear a strong resemblance to the source. These two characteristics—thorough analysis and nontrivial transformation—are the hallmarks of compilation. ■

DESIGN & IMPLEMENTATION

1.2 Compiled and interpreted languages Certain languages (e.g., Smalltalk and Python) are sometimes referred to as “interpreted languages” because most of their semantic error checking must be performed at run time. Certain other languages (e.g., Fortran and C) are sometimes referred to as “compiled languages” because almost all of their semantic error checking can be performed statically. This terminology isn’t strictly correct: interpreters for C and Fortran can be built easily, and a com- piler can generate code to perform even the most extensive dynamic semantic checks. That said, language design has a profound effect on “compilability.”

In practice one sees a broad spectrum of implementation strategies:

Most interpreted languages employ an initial translator (a preprocessor) that re- EXAMPLE 1.10

Preprocessing moves comments and white space, and groups characters together into tokens such as keywords, identiﬁers, numbers, and symbols. The translator may also expand abbreviations in the style of a macro assembler. Finally, it may identify higher-level syntactic structures, such as loops and subroutines. The goal is to produce an intermediate form that mirrors the structure of the source, but can be interpreted more efﬁciently. ■ In some very early implementations of Basic, the manual actually suggested removing comments from a program in order to improve its performance. These implementations were pure interpreters; they would re-read (and then ignore) the comments every time they executed a given part of the program. They had no initial translator. The typical Fortran implementation comes close to pure compilation. The EXAMPLE 1.11

Library routines and linking compiler translates Fortran source into machine language. Usually, however, it counts on the existence of a library of subroutines that are not part of the original program. Examples include mathematical functions (sin, cos, log, etc.) and I/O. The compiler relies on a separate program, known as a linker, to merge the appropriate library routines into the ﬁnal program:

Fortran program

Compiler

Incomplete machine language Library routines

Linker

Machine language program

In some sense, one may think of the library routines as extensions to the hardware instruction set. The compiler can then be thought of as generating code for a virtual machine that includes the capabilities of both the hardware and the library. In a more literal sense, one can ﬁnd interpretation in the Fortran routines for formatted output. Fortran permits the use of format statements that con- trol the alignment of output in columns, the number of signiﬁcant digits and type of scientiﬁc notation for ﬂoating-point numbers, inclusion/suppression of leading zeros, and so on. Programs can compute their own formats on the ﬂy. The output library routines include a format interpreter. A similar inter- preter can be found in the printf routine of C and its descendants. ■

Many compilers generate assembly language instead of machine language. This EXAMPLE 1.12

Post-compilation assembly convention facilitates debugging, since assembly language is easier for people to read, and isolates the compiler from changes in the format of machine lan- guage ﬁles that may be mandated by new releases of the operating system (only the assembler must be changed, and it is shared by many compilers):

Source program

Compiler

Assembly language

Assembler

Machine language

■

Compilers for C (and for many other languages running under Unix) begin EXAMPLE 1.13

The C preprocessor with a preprocessor that removes comments and expands macros. The pre- processor can also be instructed to delete portions of the code itself, providing a conditional compilation facility that allows several versions of a program to be built from the same source:

Source program

Preprocessor

Modified source program

Compiler

Assembly language

■

A surprising number of compilers generate output in some high-level EXAMPLE 1.14

Source-to-source translation language—commonly C or some simpliﬁed version of the input language. Such source-to-source translation is particularly common in research languages and during the early stages of language development. One famous example was AT&T’s original compiler for C++. This was indeed a true compiler, though it generated C instead of assembler: it performed a complete analysis of the syntax and semantics of the C++ source program, and with very few excep-

tions generated all of the error messages that a programmer would see prior to running the program. In fact, programmers were generally unaware that the C compiler was being used behind the scenes. The C++ compiler did not invoke the C compiler unless it had generated C code that would pass through the second round of compilation without producing any error messages:

Source program

Compiler 1

Alternative source program (e.g., in C)

Compiler 2

Assembly language

■

Occasionally one would hear the C++ compiler referred to as a preproces- sor, presumably because it generated high-level output that was in turn com- piled. I consider this a misuse of the term: compilers attempt to “understand” their source; preprocessors do not. Preprocessors perform transformations based on simple pattern matching, and may well produce output that will gen- erate error messages when run through a subsequent stage of translation. Many compilers are self-hosting: they are written in the language they EXAMPLE 1.15

Bootstrapping compile—Ada compilers in Ada, C compilers in C. This raises an obvious question: how does one compile the compiler in the ﬁrst place? The answer is to use a technique known as bootstrapping, a term derived from the inten- tionally ridiculous notion of lifting oneself off the ground by pulling on one’s bootstraps. In a nutshell, one starts with a simple implementation—often an interpreter—and uses it to build progressively more sophisticated versions. We can illustrate the idea with an historical example. Many early Pascal compilers were built around a set of tools distributed by Niklaus Wirth. These included the following:

– A Pascal compiler, written in Pascal, that would generate output in P-code, a stack-based language similar to the bytecode of modern Java compilers – The same compiler, already translated into P-code – A P-code interpreter, written in Pascal

To get Pascal up and running on a local machine, the user of the tool set needed only to translate the P-code interpreter (by hand) into some locally available language. This translation was not a difﬁcult task; the interpreter was small. By running the P-code version of the compiler on top of the P-code

interpreter, one could then compile arbitrary Pascal programs into P-code, which could in turn be run on the interpreter. To get a faster implementa- tion, one could modify the Pascal version of the Pascal compiler to generate a locally available variety of assembly or machine language, instead of gen- erating P-code (a somewhat more difﬁcult task). This compiler could then be bootstrapped—run through itself—to yield a machine-code version of the compiler:

Pascal to machine language compiler, in Pascal

Pascal to P-code compiler, in P-code

Pascal to machine language compiler, in P-code

Pascal to machine language compiler, in machine language

In a more general context, suppose we were building one of the ﬁrst compil- ers for a new programming language. Assuming we have a C compiler on our target system, we might start by writing, in a simple subset of C, a compiler for an equally simple subset of our new programming language. Once this compiler was working, we could hand-translate the C code into (the subset of) our new language, and then run the new source through the compiler itself. After that, we could repeatedly extend the compiler to accept a larger subset

DESIGN & IMPLEMENTATION

1.3 The early success of Pascal The P-code-based implementation of Pascal, and its use of bootstrapping, are largely responsible for the language’s remarkable success in academic circles in the 1970s. No single hardware platform or operating system of that era dominated the computer landscape the way the x86, Linux, and Windows do today.8 Wirth’s toolkit made it possible to get an implementation of Pascal up and running on almost any platform in a week or so. It was one of the ﬁrst great successes in system portability.

8 Throughout this book we will use the term “x86” to refer to the instruction set architecture of the Intel 8086 and its descendants, including the various Pentium, “Core,” and Xeon processors. Intel calls this architecture the IA-32, but x86 is a more generic term that encompasses the offerings of competitors such as AMD as well.

of the new programming language, bootstrap it again, and use the extended language to implement an even larger subset. “Self-hosting” implementations of this sort are actually quite common. ■ One will sometimes ﬁnd compilers for languages (e.g., Lisp, Prolog, Smalltalk) EXAMPLE 1.16

Compiling interpreted languages that permit a lot of late binding, and are traditionally interpreted. These com- pilers must be prepared, in the general case, to generate code that performs much of the work of an interpreter, or that makes calls into a library that does that work instead. In important special cases, however, the compiler can gen- erate code that makes reasonable assumptions about decisions that won’t be ﬁnalized until run time. If these assumptions prove to be valid the code will run very fast. If the assumptions are not correct, a dynamic check will discover the inconsistency, and revert to the interpreter. ■ In some cases a programming system may deliberately delay compilation until EXAMPLE 1.17

Dynamic and just-in-time compilation the last possible moment. One example occurs in language implementations (e.g., for Lisp or Prolog) that invoke the compiler on the ﬂy, to translate newly created source into machine language, or to optimize the code for a particu- lar input set. Another example occurs in implementations of Java. The Java language deﬁnition deﬁnes a machine-independent intermediate form known as Java bytecode. Bytecode is the standard format for distribution of Java pro- grams; it allows programs to be transferred easily over the Internet, and then run on any platform. The ﬁrst Java implementations were based on byte-code interpreters, but modern implementations obtain signiﬁcantly better perfor- mance with a just-in-time compiler that translates bytecode into machine lan- guage immediately before each execution of the program:

Java program

Java compiler

Input

Java byte code

Bytecode interpreter

JIT compiler

Output

Input Machine language Output

C#, similarly, is intended for just-in-time translation. The main C# com- piler produces Common Intermediate Language (CIL), which is then translated into machine language immediately prior to execution. CIL is deliberately lan- guage independent, so it can be used for code produced by a variety of front- end compilers. We will explore the Java and C# implementations in detail in Section 16.1. ■

