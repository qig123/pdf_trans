# 4.1 The Role of the Semantic Analyzer

**180**
Chapter 4* Semantic Analysis*

there will inevitably be cases in which an error will always occur, but the compiler
cannot tell, and must delay the error message until run time; there will also be
cases in which an error can never occur, but the compiler cannot tell, and must
incur the cost of unnecessary run-time checks.
Both semantic analysis and intermediate code generation can be described in
terms of annotation, or* decoration* of a parse tree or syntax tree. The annotations
themselves are known as* attributes*. Numerous examples of static and dynamic
semantic rules will appear in subsequent chapters. In this current chapter we
focus primarily on the mechanisms a compiler uses to enforce the static rules. We
will consider intermediate code generation (including the generation of code for
dynamic semantic checks) in Chapter 15.
In Section 4.1 we consider the role of the semantic analyzer in more detail,
considering both the rules it needs to enforce and its relationship to other phases
of compilation. Most of the rest of the chapter is then devoted to the subject
of* attribute grammars*. Attribute grammars provide a formal framework for the
decoration of a tree. This framework is a useful conceptual tool even in compilers
that do not build a parse tree or syntax tree as an explicit data structure. We
introduce the notion of an attribute grammar in Section 4.2. We then consider
various ways in which such grammars can be applied in practice. Section 4.3
discusses the issue of* attribute ﬂow*, which constrains the order(s) in which nodes
of a tree can be decorated. In practice, most compilers require decoration of the
parse tree (or the evaluation of attributes that would reside in a parse tree if there
were one) to occur in the process of an LL or LR parse. Section 4.4 presents* action*
*routines* as an ad hoc mechanism for such “on-the-ﬂy” evaluation. In Section 4.5
(mostly on the companion site) we consider the management of space for parse
tree attributes.
Because they have to reﬂect the structure of the CFG, parse trees tend to be
very complicated (recall the example in Figure 1.5). Once parsing is complete, we
typically want to replace the parse tree with a syntax tree that reﬂects the input
program in a more straightforward way (Figure 1.6). One particularly common
compiler organization uses action routines during parsing solely for the purpose
of constructing the syntax tree. The syntax tree is then decorated during a sepa-
rate traversal, which can be formalized, if desired, with a separate attribute gram-
mar. We consider the decoration of syntax trees in Section 4.6.

## 4.1

**The Role of the Semantic Analyzer**
Programming languages vary dramatically in their choice of semantic rules. Lisp
dialects, for example, allow “mixed-mode” arithmetic on arbitrary numeric types,
which they will automatically promote from integer to rational to ﬂoating-point
or “bignum” (extended) precision, as required to maintain precision. Ada, by
contract, assigns a speciﬁc type to every numeric variable, and requires the pro-
grammer to convert among these explicitly when combining them in expressions.

Assertions in Java
is a statement that a speciﬁed condition is expected to be true when execution
reaches a certain point in the code. In Java one can write

**DESIGN & IMPLEMENTATION**

```
4.1 Dynamic semantic checks
In the past, language theorists and researchers in programming methodology
and software engineering tended to argue for more extensive semantic checks,
while “real-world” programmers “voted with their feet” for languages like C
and Fortran, which omitted those checks in the interest of execution speed. As
computers have become more powerful, and as companies have come to ap-
preciate the enormous costs of software maintenance, the “real-world” camp
has become much more sympathetic to checking. Languages like Ada and Java
have been designed from the outset with safety in mind, and languages like C
and C++ have evolved (to the extent possible) toward increasingly strict deﬁ-
nitions. In scripting languages, where many semantic checks are deferred until
run time in order to avoid the need for explicit types and variable declarations,
there has been a similar trend toward stricter rules. Perl, for example (one of
the older scripting languages), will typically attempt to infer a possible mean-
ing for expressions (e.g., 3 + "four") that newer languages (e.g., Python or
Ruby) will ﬂag as run-time errors.
```

```
1
Among other things, C. A. R. Hoare (1934–) invented the quicksort algorithm and the case
statement, contributed to the design of Algol W, and was one of the leaders in the development
of axiomatic semantics. In the area of concurrent programming, he reﬁned and formalized the
monitor construct (to be described in Section 13.4.1), and designed the CSP programming model
and notation. He received the ACM Turing Award in 1980.
```

**Static Analysis**

In general, compile-time algorithms that predict run-time behavior are known
as* static analysis*. Such analysis is said to be* precise* if it allows the compiler to
determine whether a given program will always follow the rules. Type checking,
for example, is static and precise in languages like Ada and ML: the compiler
ensures that no variable will ever be used at run time in a way that is inappropriate
for its type. By contrast, languages like Lisp, Smalltalk, Python, and Ruby obtain
greater ﬂexibility, while remaining completely type-safe, by accepting the run-
time overhead of dynamic type checks. (We will cover type checking in more
detail in Chapter 7.)
Static analysis can also be useful when it isn’t precise. Compilers will often
check what they can at compile time and then generate code to check the rest
dynamically. In Java, for example, type checking is mostly static, but dynamically
loaded classes and type casts may require run-time checks. In a similar vein, many

