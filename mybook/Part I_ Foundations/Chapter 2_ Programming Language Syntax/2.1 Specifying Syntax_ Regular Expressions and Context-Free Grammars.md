# 2.1 Specifying Syntax: Regular Expressions and Context-Free Grammars

**44**
Chapter 2* Programming Language Syntax*

```
string of digits. Similar syntax rules and semantic interpretations can be devised
for rational numbers, (limited-precision) real numbers, arithmetic, assignments,
control ﬂow, declarations, and indeed all of programming languages.
Distinguishing between syntax and semantics is useful for at least two reasons.
First, different programming languages often provide features with very similar
semantics but very different syntax. It is generally much easier to learn a new lan-
guage if one is able to identify the common (and presumably familiar) semantic
ideas beneath the unfamiliar syntax. Second, there are some very efﬁcient and
elegant algorithms that a compiler or interpreter can use to discover the syntactic
structure (but not the semantics!) of a computer program, and these algorithms
can be used to drive the rest of the compilation or interpretation process.
In the current chapter we focus on syntax: how we specify the structural rules
of a programming language, and how a compiler identiﬁes the structure of a
given input program. These two tasks—specifying syntax rules and ﬁguring out
how (and whether) a given program was built according to those rules—are dis-
tinct. The ﬁrst is of interest mainly to programmers, who want to write valid
programs. The second is of interest mainly to compilers, which need to analyze
those programs. The ﬁrst task relies on regular expressions and context-free gram-
mars, which specify how to generate valid programs. The second task relies on
scanners and parsers, which recognize program structure. We address the ﬁrst of
these tasks in Section 2.1, the second in Sections 2.2 and 2.3.
In Section 2.4 (largely on the companion site) we take a deeper look at the for-
mal theory underlying scanning and parsing. In theoretical parlance, a scanner is
a deterministic ﬁnite automaton (DFA) that recognizes the tokens of a program-
ming language. A parser is a deterministic push-down automaton (PDA) that
recognizes the language’s context-free syntax. It turns out that one can gener-
ate scanners and parsers automatically from regular expressions and context-free
grammars. This task is performed by tools like Unix’s lex and yacc,2 among oth-
ers. Possibly nowhere else in computer science is the connection between theory
and practice so clear and so compelling.
2.1
Specifying Syntax: Regular Expressions and
Context-Free Grammars
```

Formal speciﬁcation of syntax requires a set of rules. How complicated (expres-
sive) the syntax can be depends on the kinds of rules we are allowed to use. It
turns out that what we intuitively think of as tokens can be constructed from
individual characters using just three kinds of formal rules: concatenation, alter-
nation (choice among a ﬁnite set of alternatives), and so-called “Kleene closure”

```
2
At many sites, lex and yacc have been superseded by the GNU flex and bison tools, which
provide a superset of the original functionality.
```

```
Lexical structure of C11
tokens, including 44 keywords (double, if, return, struct, etc.); identiﬁers
(my_variable, your_type, sizeof, printf, etc.); integer (0765, 0x1f5, 501),
ﬂoating-point (6.022e23), and character (‚x‚, ‚\‚‚, ‚\0170‚) constants; string
literals ("snerk", "say \"hi\"\n"); 54 “punctuators” (+, ], ->, *=, :, ||, etc.),
and two different forms of comments. There are provisions for international
character sets, string literals that span multiple lines of source code, constants
of varying precision (width), alternative “spellings” for symbols that are missing
on certain input devices, and preprocessor macros that build tokens from smaller
pieces. Other large, modern languages (Java, Ada) are similarly complex.
■
To specify tokens, we use the notation of regular expressions. A regular expres-
sion is one of the following:
```

**1.** A character
**2.** The empty string, denoted* ϵ*
**3.** Two regular expressions next to each other, meaning any string generated by
the ﬁrst one followed by (concatenated with) any string generated by the sec-
ond one

*real** −→**integer exponent*** |*** decimal*** (*** exponent*** |*** ϵ*** )**

```
decimal −→digit * ( . digit | digit . ) digit *
```

```
exponent −→( e | E ) ( + | - | ϵ ) integer
```

```
digit −→0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
```

The symbols to the left of the* −→*signs provide names for the regular expres-
sions. One of these (*number*) will serve as a token name; the others are simply

**DESIGN & IMPLEMENTATION**

```
2.1 Contextual keywords
In addition to distinguishing between keywords and identiﬁers, some lan-
guages deﬁne so-called contextual keywords, which function as keywords in
certain speciﬁc places in a program, but as identiﬁers elsewhere. In C#, for ex-
ample, the word yield can appear immediately before return or break—a
place where an identiﬁer can never appear. In this context, it is interpreted as
a keyword; anywhere else it is an identiﬁer. It is therefore perfectly acceptable
to have a local variable named yield: the compiler can distinguish it from the
keyword by where it appears in the program.
C++11 has a small handful of contextual keywords. C# 4.0 has 26. Most
were introduced in the course of revising the language to create a new stan-
dard version. Given a large user community, any short, intuitively appealing
word is likely to have been used as an identiﬁer by someone, in some existing
program. Making that word a contextual keyword in the new version of the
language, rather than a full keyword, reduces the risk that existing programs
will suddenly fail to compile.
```

**3**
Some authors use* λ* to represent the empty string. Some use a period (**.**), rather than juxtaposi-
tion, to indicate concatenation. Some use a plus sign (**+**), rather than a vertical bar, to indicate
alternation.

### 2.1.3** Derivations and Parse Trees**

### A context-free grammar shows us how to generate a syntactically valid string of

### terminals: Begin with the start symbol. Choose a production with the start sym-

### bol on the left-hand side; replace the start symbol with the right-hand side of that

### production. Now choose a nonterminal* A* in the resulting string, choose a pro-

### duction* P* with* A* on its left-hand side, and replace* A* with the right-hand side of

### *P*. Repeat this process until no nonterminals remain.

### As an example, we can use our grammar for expressions to generate the string

**EXAMPLE** 2.6
```
Derivation of slope * x +
intercept
“slope * x + intercept”:
```

*expr* =*⇒**expr op expr*

```
=⇒expr op id
```

```
=⇒expr + id
```

```
=⇒expr op expr + id
```

```
=⇒expr op id + id
```

```
=⇒expr * id + id
```

```
=⇒
id
(slope)
* id
(x)
+
id
(intercept)
```

```
3CHECK YOUR UNDERSTANDING
1.
What is the difference between syntax and semantics?
2.
What are the three basic operations that can be used to build complex regular
expressions from simpler regular expressions?
3.
What additional operation (beyond the three of regular expressions) is pro-
vided in context-free grammars?
4.
What is Backus-Naur form? When and why was it devised?
```

5.
Name a language in which indentation affects program syntax.
6.
When discussing context-free languages, what is a* derivation*? What is a* sen-*
*tential form*?
7.
What is the difference between a* right-most* derivation and a* left-most* deriva-
tion?

8.
What does it mean for a context-free grammar to be* ambiguous*?
9.
What are* associativity* and* precedence*? Why are they signiﬁcant in parse trees?

