44
Chapter 2 Programming Language Syntax
string of digits. Similar syntax rules and semantic interpretations can be devised
for rational numbers, (limited-precision) real numbers, arithmetic, assignments,
control ﬂow, declarations, and indeed all of programming languages.
Distinguishing between syntax and semantics is useful for at least two reasons.
First, different programming languages often provide features with very similar
semantics but very different syntax. It is generally much easier to learn a new lan-
guage if one is able to identify the common (and presumably familiar) semantic
ideas beneath the unfamiliar syntax. Second, there are some very efﬁcient and
elegant algorithms that a compiler or interpreter can use to discover the syntactic
structure (but not the semantics!) of a computer program, and these algorithms
can be used to drive the rest of the compilation or interpretation process.
In the current chapter we focus on syntax: how we specify the structural rules
of a programming language, and how a compiler identiﬁes the structure of a
given input program. These two tasks—specifying syntax rules and ﬁguring out
how (and whether) a given program was built according to those rules—are dis-
tinct. The ﬁrst is of interest mainly to programmers, who want to write valid
programs. The second is of interest mainly to compilers, which need to analyze
those programs. The ﬁrst task relies on regular expressions and context-free gram-
mars, which specify how to generate valid programs. The second task relies on
scanners and parsers, which recognize program structure. We address the ﬁrst of
these tasks in Section 2.1, the second in Sections 2.2 and 2.3.
In Section 2.4 (largely on the companion site) we take a deeper look at the for-
mal theory underlying scanning and parsing. In theoretical parlance, a scanner is
a deterministic ﬁnite automaton (DFA) that recognizes the tokens of a program-
ming language. A parser is a deterministic push-down automaton (PDA) that
recognizes the language’s context-free syntax. It turns out that one can gener-
ate scanners and parsers automatically from regular expressions and context-free
grammars. This task is performed by tools like Unix’s lex and yacc,2 among oth-
ers. Possibly nowhere else in computer science is the connection between theory
and practice so clear and so compelling.
2.1
Specifying Syntax: Regular Expressions and
Context-Free Grammars
Formal speciﬁcation of syntax requires a set of rules. How complicated (expres-
sive) the syntax can be depends on the kinds of rules we are allowed to use. It
turns out that what we intuitively think of as tokens can be constructed from
individual characters using just three kinds of formal rules: concatenation, alter-
nation (choice among a ﬁnite set of alternatives), and so-called “Kleene closure”
2
At many sites, lex and yacc have been superseded by the GNU flex and bison tools, which
provide a superset of the original functionality.
2.1 Specifying Syntax
45
(repetition an arbitrary number of times). Specifying most of the rest of what
we intuitively think of as syntax requires one additional kind of rule: recursion
(creation of a construct from simpler instances of the same construct). Any set of
strings that can be deﬁned in terms of the ﬁrst three rules is called a regular set,
or sometimes a regular language. Regular sets are generated by regular expressions
and recognized by scanners. Any set of strings that can be deﬁned if we add recur-
sion is called a context-free language (CFL). Context-free languages are generated
by context-free grammars (CFGs) and recognized by parsers. (Terminology can
be confusing here. The meaning of the word “language” varies greatly, depending
on whether we’re talking about “formal” languages [e.g., regular or context-free],
or programming languages. A formal language is just a set of strings, with no
accompanying semantics.)
2.1.1 Tokens and Regular Expressions
Tokens are the basic building blocks of programs—the shortest strings of char-
acters with individual meaning. Tokens come in many kinds, including key-
words, identiﬁers, symbols, and constants of various types. Some kinds of token
(e.g., the increment operator) correspond to only one string of characters. Oth-
ers (e.g., identiﬁer) correspond to a set of strings that share some common form.
(In most languages, keywords are special strings of characters that have the right
form to be identiﬁers, but are reserved for special purposes.) We will use the word
“token” informally to refer to both the generic kind (an identiﬁer, the increment
operator) and the speciﬁc string (foo, ++); the distinction between these should
be clear from context.
Some languages have only a few kinds of token, of fairly simple form. Other
languages are more complex.
C, for example, has more than 100 kinds of
EXAMPLE 2.2
Lexical structure of C11
tokens, including 44 keywords (double, if, return, struct, etc.); identiﬁers
(my_variable, your_type, sizeof, printf, etc.); integer (0765, 0x1f5, 501),
ﬂoating-point (6.022e23), and character (‚x‚, ‚\‚‚, ‚\0170‚) constants; string
literals ("snerk", "say \"hi\"\n"); 54 “punctuators” (+, ], ->, *=, :, ||, etc.),
and two different forms of comments. There are provisions for international
character sets, string literals that span multiple lines of source code, constants
of varying precision (width), alternative “spellings” for symbols that are missing
on certain input devices, and preprocessor macros that build tokens from smaller
pieces. Other large, modern languages (Java, Ada) are similarly complex.
■
To specify tokens, we use the notation of regular expressions. A regular expres-
sion is one of the following:
1. A character
2. The empty string, denoted ϵ
3. Two regular expressions next to each other, meaning any string generated by
the ﬁrst one followed by (concatenated with) any string generated by the sec-
ond one
46
Chapter 2 Programming Language Syntax
4. Two regular expressions separated by a vertical bar (|), meaning any string
generated by the ﬁrst one or any string generated by the second one
5. A regular expression followed by a Kleene star, meaning the concatenation of
zero or more strings generated by the expression in front of the star
Parentheses are used to avoid ambiguity about where the various subexpres-
sions start and end.3
Consider, for example, the syntax of numeric constants accepted by a simple
EXAMPLE 2.3
Syntax of numeric
constants
hand-held calculator:
number −→integer | real
integer −→digit digit *
real −→integer exponent | decimal ( exponent | ϵ )
decimal −→digit * ( . digit | digit . ) digit *
exponent −→( e | E ) ( + | - | ϵ ) integer
digit −→0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9
The symbols to the left of the −→signs provide names for the regular expres-
sions. One of these (number) will serve as a token name; the others are simply
DESIGN & IMPLEMENTATION
2.1 Contextual keywords
In addition to distinguishing between keywords and identiﬁers, some lan-
guages deﬁne so-called contextual keywords, which function as keywords in
certain speciﬁc places in a program, but as identiﬁers elsewhere. In C#, for ex-
ample, the word yield can appear immediately before return or break—a
place where an identiﬁer can never appear. In this context, it is interpreted as
a keyword; anywhere else it is an identiﬁer. It is therefore perfectly acceptable
to have a local variable named yield: the compiler can distinguish it from the
keyword by where it appears in the program.
C++11 has a small handful of contextual keywords. C# 4.0 has 26. Most
were introduced in the course of revising the language to create a new stan-
dard version. Given a large user community, any short, intuitively appealing
word is likely to have been used as an identiﬁer by someone, in some existing
program. Making that word a contextual keyword in the new version of the
language, rather than a full keyword, reduces the risk that existing programs
will suddenly fail to compile.
3
Some authors use λ to represent the empty string. Some use a period (.), rather than juxtaposi-
tion, to indicate concatenation. Some use a plus sign (+), rather than a vertical bar, to indicate
alternation.
2.1 Specifying Syntax
47
for convenience in building larger expressions.4 Note that while we have allowed
deﬁnitions to build on one another, nothing is ever deﬁned in terms of itself,
even indirectly. Such recursive deﬁnitions are the distinguishing characteristic of
context-free grammars, described in Section 2.1.2. To generate a valid number,
we expand out the sub-deﬁnitions and then scan the resulting expression from left
to right, choosing among alternatives at each vertical bar, and choosing a number
of repetitions at each Kleene star. Within each repetition we may make different
choices at vertical bars, generating different substrings.
■
Character Sets and Formatting Issues
Upper- and lowercase letters in identiﬁers and keywords are considered distinct in
some languages (e.g., Perl, Python, and Ruby; C and its descendants), and identi-
cal in others (e.g., Ada, Common Lisp, and Fortran). Thus foo, Foo, and FOO all
represent the same identiﬁer in Ada, but different identiﬁers in C. Modula-2 and
Modula-3 require keywords and predeﬁned (built-in) identiﬁers to be written in
uppercase; C and its descendants require them to be written in lowercase. A few
languages allow only letters and digits in identiﬁers. Most allow underscores. A
few (notably Lisp) allow a variety of additional characters. Some languages (e.g.,
Java and C#) have standard (but optional) conventions on the use of upper- and
lowercase letters in names.5
With the globalization of computing, non-Latin character sets have become
increasingly important. Many modern languages, including C, C++, Ada 95,
Java, C#, and Fortran 2003 have introduced explicit support for multibyte char-
acter sets, generally based on the Unicode and ISO/IEC 10646 international stan-
dards. Most modern programming languages allow non-Latin characters to ap-
pear within comments and character strings; an increasing number allow them
in identiﬁers as well. Conventions for portability across character sets and for lo-
calization to a given character set can be surprisingly complex, particularly when
various forms of backward compatibility are required (the C99 Rationale devotes
ﬁve full pages to this subject [Int03a, pp. 19–23]); for the most part we ignore
such issues here.
Some language implementations impose limits on the maximum length of
identiﬁers, but most avoid such unnecessary restrictions. Most modern languages
are also more or less free format, meaning that a program is simply a sequence of
tokens: what matters is their order with respect to one another, not their physical
position within a printed line or page. “White space” (blanks, tabs, carriage re-
turns, and line and page feed characters) between tokens is usually ignored, except
to the extent that it is needed to separate one token from the next.
4
We have assumed here that all numeric constants are simply “numbers.” In many programming
languages, integer and real constants are separate kinds of token. Their syntax may also be more
complex than indicated here, to support such features are multiple lengths or nondecimal bases.
5
For the sake of consistency we do not always obey such conventions in this book: most examples
follow the common practice of C programmers, in which underscores, rather than capital letters,
separate the “subwords” of names.
48
Chapter 2 Programming Language Syntax
There are a few noteworthy exceptions to these rules. Some language imple-
mentations limit the maximum length of a line, to allow the compiler to store the
current line in a ﬁxed-length buffer. Dialects of Fortran prior to Fortran 90 use
a ﬁxed format, with 72 characters per line (the width of a paper punch card, on
which programs were once stored), and with different columns within the line re-
served for different purposes. Line breaks serve to separate statements in several
other languages, including Go, Haskell, Python, and Swift. Haskell and Python
also give special signiﬁcance to indentation. The body of a loop, for example, con-
sists of precisely those subsequent lines that are indented farther than the header
of the loop.
Other Uses of Regular Expressions
Many readers will be familiar with regular expressions from the grep family of
tools in Unix, the search facilities of various text editors, or such scripting lan-
guages and tools as Perl, Python, Ruby, awk, and sed. Most of these provide a rich
set of extensions to the notation of regular expressions. Some extensions, such as
shorthand for “zero or one occurrences” or “anything other than white space,”
do not change the power of the notation. Others, such as the ability to require a
second occurrence, later in the input string, of the same character sequence that
matched an earlier part of the expression, increase the power of the notation, so
that it is no longer restricted to generating regular sets. Still other extensions are
designed not to increase the expressiveness of the notation but rather to tie it to
other language facilities. In many tools, for example, one can bracket portions of
a regular expression in such a way that when a string is matched against it the con-
tents of the corresponding substrings are assigned into named local variables. We
will return to these issues in Section 14.4.2, in the context of scripting languages.
2.1.2 Context-Free Grammars
Regular expressions work well for deﬁning tokens. They are unable, however, to
specify nested constructs, which are central to programming languages. Consider
EXAMPLE 2.4
Syntactic nesting in
expressions
for example the structure of an arithmetic expression:
DESIGN & IMPLEMENTATION
2.2 Formatting restrictions
Formatting limitations inspired by implementation concerns—as in the
punch-card-oriented rules of Fortran 77 and its predecessors—have a ten-
dency to become unwanted anachronisms as implementation techniques im-
prove. Given the tendency of certain word processors to “ﬁll” or auto-format
text, the line break and indentation rules of languages like Haskell, Occam, and
Python are somewhat controversial.
2.1 Specifying Syntax
49
expr −→id | number | - expr | ( expr )
| expr op expr
op −→+ | - | * | /
Here the ability to deﬁne a construct in terms of itself is crucial. Among other
things, it allows us to ensure that left and right parentheses are matched, some-
thing that cannot be accomplished with regular expressions (see Section C 2.4.3
for more details). The arrow symbol (−→) means “can have the form”; for brevity
it is sometimes pronounced “goes to.”
■
Each of the rules in a context-free grammar is known as a production. The
symbols on the left-hand sides of the productions are known as variables, or non-
terminals. There may be any number of productions with the same left-hand side.
Symbols that are to make up the strings derived from the grammar are known as
terminals (shown here in typewriter font). They cannot appear on the left-hand
side of any production. In a programming language, the terminals of the context-
free grammar are the language’s tokens. One of the nonterminals, usually the one
on the left-hand side of the ﬁrst production, is called the start symbol. It names
the construct deﬁned by the overall grammar.
The notation for context-free grammars is sometimes called Backus-Naur
Form (BNF), in honor of John Backus and Peter Naur, who devised it for the
deﬁnition of the Algol-60 programming language [NBB+63].6 Strictly speaking,
the Kleene star and meta-level parentheses of regular expressions are not allowed
in BNF, but they do not change the expressive power of the notation, and are
commonly included for convenience. Sometimes one sees a “Kleene plus” (+) as
well; it indicates one or more instances of the symbol or group of symbols in front
of it.7 When augmented with these extra operators, the notation is often called
extended BNF (EBNF). The construct
EXAMPLE 2.5
Extended BNF (EBNF)
id list −→id ( , id )*
is shorthand for
id list −→id
id list −→id list , id
“Kleene plus” is analogous. Note that the parentheses here are metasymbols. In
Example 2.4 they were part of the language being deﬁned, and were written in
ﬁxed-width font.8
6
John Backus (1924–2007) was also the inventor of Fortran. He spent most of his professional
career at IBM Corporation, and was named an IBM Fellow in 1987. He received the ACM Turing
Award in 1977.
7
Some authors use curly braces ({ }) to indicate zero or more instances of the symbols inside.
Some use square brackets ([ ]) to indicate zero or one instances of the symbols inside—that is, to
indicate that those symbols are optional.
8
To avoid confusion, some authors place quote marks around any single character that is part of
the language being deﬁned: id list −→id ( ‘,’ id )* ; expr −→‘(’ expr ‘ )’. In both regular
and extended BNF, many authors use ::= instead of −→.
50
Chapter 2 Programming Language Syntax
Like the Kleene star and parentheses, the vertical bar is in some sense superﬂu-
ous, though it was provided in the original BNF. The construct
op −→+ | - | * | /
can be considered shorthand for
op −→+
op −→-
op −→*
op −→/
which is also sometimes written
op −→+
−→-
−→*
−→/
■
Many tokens, such as id and number above, have many possible spellings (i.e.,
may be represented by many possible strings of characters). The parser is obliv-
ious to these; it does not distinguish one identiﬁer from another. The semantic
analyzer does distinguish them, however; the scanner must save the spelling of
each such “interesting” token for later use.
2.1.3 Derivations and Parse Trees
A context-free grammar shows us how to generate a syntactically valid string of
terminals: Begin with the start symbol. Choose a production with the start sym-
bol on the left-hand side; replace the start symbol with the right-hand side of that
production. Now choose a nonterminal A in the resulting string, choose a pro-
duction P with A on its left-hand side, and replace A with the right-hand side of
P. Repeat this process until no nonterminals remain.
As an example, we can use our grammar for expressions to generate the string
EXAMPLE 2.6
Derivation of slope * x +
intercept
“slope * x + intercept”:
expr =⇒expr op expr
=⇒expr op id
=⇒expr + id
=⇒expr op expr + id
=⇒expr op id + id
=⇒expr * id + id
=⇒
id
(slope)
* id
(x)
+
id
(intercept)
2.1 Specifying Syntax
51
expr
expr
op
expr
id(intercept)
+
expr
expr
op
id(slope)
id(x)
*


![Figure 2.1 Parse tree...](images/page_84_caption_Figure%202.1%20Parse%20tree%20for%20slope%20_%20x%20%2B%20intercept%20%28grammar%20in%20Example%202.4%29.png)
*Figure 2.1 Parse tree for slope * x + intercept (grammar in Example 2.4).*

expr
expr
op
expr
id(slope)
*
expr
expr
op
id(x)
+
id(intercept)


![Figure 2.2 Alternative (less...](images/page_84_caption_Figure%202.2%20Alternative%20%28less%20desirable%29%20parse%20tree%20for%20slope%20_%20x%20%2B%20intercept%20%28grammar%20in%20Example%202.4.png)
*Figure 2.2 Alternative (less desirable) parse tree for slope * x + intercept (grammar in Example 2.4). The fact that more than one tree exists implies that our grammar is ambiguous.*

The =⇒metasymbol is often pronounced “derives.” It indicates that the right-
hand side was obtained by using a production to replace some nonterminal in the
left-hand side. At each line we have underlined the symbol A that is replaced in
the following line.
■
A series of replacement operations that shows how to derive a string of termi-
nals from the start symbol is called a derivation. Each string of symbols along the
way is called a sentential form. The ﬁnal sentential form, consisting of only ter-
minals, is called the yield of the derivation. We sometimes elide the intermediate
steps and write expr =⇒∗slope * x + intercept, where the metasymbol =⇒∗
means “derives after zero or more replacements.” In this particular derivation, we
have chosen at each step to replace the right-most nonterminal with the right-
hand side of some production. This replacement strategy leads to a right-most
derivation. There are many other possible derivations, including left-most and
options in-between.
We saw in Chapter 1 that we can represent a derivation graphically as a parse
tree. The root of the parse tree is the start symbol of the grammar. The leaves of
the tree are its yield. Each internal node, together with its children, represents the
use of a production.
A parse tree for our example expression appears in Figure 2.1. This tree is
EXAMPLE 2.7
Parse trees for slope * x
+ intercept
not unique. At the second level of the tree, we could have chosen to turn the
operator into a * instead of a +, and to further expand the expression on the
right, rather than the one on the left (see Figure 2.2). A grammar that allows the
52
Chapter 2 Programming Language Syntax
construction of more than one parse tree for some string of terminals is said to be
ambiguous. Ambiguity turns out to be a problem when trying to build a parser:
it requires some extra mechanism to drive a choice between equally acceptable
alternatives.
■
A moment’s reﬂection will reveal that there are inﬁnitely many context-free
grammars for any given context-free language.9 Some grammars, however, are
much more useful than others. In this text we will avoid the use of ambiguous
grammars (though most parser generators allow them, by means of disambiguat-
ing rules). We will also avoid the use of so-called useless symbols: nonterminals
that cannot generate any string of terminals, or terminals that cannot appear in
the yield of any derivation.
When designing the grammar for a programming language, we generally try
to ﬁnd one that reﬂects the internal structure of programs in a way that is useful
to the rest of the compiler. (We shall see in Section 2.3.2 that we also try to ﬁnd
one that can be parsed efﬁciently, which can be a bit of a challenge.) One place
in which structure is particularly important is in arithmetic expressions, where
we can use productions to capture the associativity and precedence of the various
operators. Associativity tells us that the operators in most languages group left
to right, so that 10 - 4 - 3 means (10 - 4) - 3 rather than 10 - (4 - 3).
Precedence tells us that multiplication and division in most languages group more
tightly than addition and subtraction, so that 3 + 4 * 5 means 3 + (4 * 5) rather
than (3 + 4) * 5. (These rules are not universal; we will consider them again in
Section 6.1.1.)
Here is a better version of our expression grammar:
EXAMPLE 2.8
Expression grammar with
precedence and
associativity
1.
expr −→term | expr add op term
2.
term −→factor | term mult op factor
3.
factor −→id | number | - factor | ( expr )
4.
add op −→+ | -
5.
mult op −→* | /
This grammar is unambiguous. It captures precedence in the way factor, term,
and expr build on one another, with different operators appearing at each level.
It captures associativity in the second halves of lines 1 and 2, which build subexprs
and subterms to the left of the operator, rather than to the right. In Figure 2.3, we
can see how building the notion of precedence into the grammar makes it clear
that multiplication groups more tightly than addition in 3 + 4 * 5, even without
parentheses. In Figure 2.4, we can see that subtraction groups more tightly to the
left, so that 10 - 4 - 3 would evaluate to 3, rather than to 9.
■
9
Given a speciﬁc grammar, there are many ways to create other equivalent grammars. We could,
for example, replace A with some new symbol B everywhere it appears in the right-hand side of
a production, and then create a new production B −→A.
2.1 Specifying Syntax
53
expr
expr
add_op
term
+
term
term
factor
mult_op
number(5)
*
factor
factor
number(4)
number(3)


![Figure 2.3 Parse tree...](images/page_86_caption_Figure%202.3%20Parse%20tree%20for%203%20%2B%204%20_%205%2C%20with%20precedence%20%28grammar%20in%20Example%202.8%29.png)
*Figure 2.3 Parse tree for 3 + 4 * 5, with precedence (grammar in Example 2.8).*

expr
expr
add_op
term
-
add_op
expr
term
factor
-
number(3)
term
factor
number(4)
factor
number(10)


![Figure 2.4 Parse tree...](images/page_86_caption_Figure%202.4%20Parse%20tree%20for%2010%20%E2%80%93%204%20%E2%80%93%203%2C%20with%20left%20associativity%20%28grammar%20in%20Example%202.8%29.png)
*Figure 2.4 Parse tree for 10 – 4 – 3, with left associativity (grammar in Example 2.8).*

3CHECK YOUR UNDERSTANDING
1.
What is the difference between syntax and semantics?
2.
What are the three basic operations that can be used to build complex regular
expressions from simpler regular expressions?
3.
What additional operation (beyond the three of regular expressions) is pro-
vided in context-free grammars?
4.
What is Backus-Naur form? When and why was it devised?
5.
Name a language in which indentation affects program syntax.
6.
When discussing context-free languages, what is a derivation? What is a sen-
tential form?
7.
What is the difference between a right-most derivation and a left-most deriva-
tion?
8.
What does it mean for a context-free grammar to be ambiguous?
9.
What are associativity and precedence? Why are they signiﬁcant in parse trees?
