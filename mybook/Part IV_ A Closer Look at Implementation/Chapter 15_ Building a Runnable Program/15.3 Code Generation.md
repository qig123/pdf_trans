# 15.3 Code Generation

### **784**

### Chapter 15* Building a Runnable Program*

push a
r2 := a
push b
r3 := b
push c
r4 := c
add
r1 := r2 + r3
add
r1 := r1 + r4
push 2
r1 := r1 / 2
–– s
divide
pop s
push s
push s
r2 := r1 −r2
–– s −a
push a
subtract
push s
r3 := r1 −r3
–– s −b
push b
subtract
push s
r4 := r1 −r4
–– s −c
push c
subtract
multiply
r3 := r3 × r4
multiply
r2 := r2 × r3
multiply
r1 := r1 × r2
push sqrt
call sqrt
call


![Figure 15.4 Stack-based versus...](images/page_817_vector_336.png)
*Figure 15.4 Stack-based versus three-address IF. Shown are two versions of code to compute the area of a triangle using Heron’s formula. At left is a stylized version of Java bytecode or CLI Common Intermediate Language. At right is corresponding pseudo-assembler for a machine with three-address instructions. The bytecode requires a larger number of instructions, but occupies less space.*

### the push operation and two to specify the sqrt routine). This gives us a total of

### 23 instructions in 25 bytes.

### By contrast, three-address code for the same formula keeps a, b, c, and s in

### registers, and requires only 13 instructions. Unfortunately, in typical notation

### each instruction but the last will be four bytes in length (the last will be eight),

### and our 13 instructions will occupy 56 bytes.

### ■

## 15.3

### **Code Generation**

### The back-end structure of Figure 15.1 is too complex to present in any detail in a

**EXAMPLE** 15.5
Simpler compiler structure
### single chapter. To limit the scope of our discussion, we will content ourselves in

### this chapter with producing correct but naive code. This choice will allow us to

### consider a signiﬁcantly simpler middle and back end. Starting with the structure

### of Figure 15.1, we drop the machine-independent code improver and then merge

### intermediate and target code generation into a single phase. This merged phase

Parser (syntax analysis)

Semantic analysis

**Back end**

Syntax tree with
additional annotations

Assembly language

Naive register allocation

Target code generation


![Figure 15.5 A simpler,...](images/page_818_vector_263.png)
*Figure 15.5 A simpler, nonoptimizing compiler structure, assumed in Section 15.3. The target code generation phase closely resembles the intermediate code generation phase of Figure 15.1.*

generates pure, linear assembly language; because we are not performing code
improvements that alter the program’s control ﬂow, there is no need to represent
that ﬂow explicitly in a control ﬂow graph. We also adopt a much simpler register
allocation algorithm, which can operate directly on the syntax tree prior to code
generation, eliminating the need for virtual registers and the subsequent mapping
onto architectural registers. Finally, we drop instruction scheduling. The result-
ing compiler structure appears in Figure 15.5. Its code generation phase closely
resembles the intermediate code generation of Figure 15.1.
■

15.3.1** An Attribute Grammar Example**

Like semantic analysis, intermediate code generation can be formalized in terms
of an attribute grammar, though it is most commonly implemented via hand-
written ad hoc traversal of a syntax tree. We present an attribute grammar here
for the sake of clarity.
In Figure 1.7, we presented naive x86 assembly language for the GCD pro-
gram. We will use our attribute grammar example to generate a similar version
here, but for a RISC-like machine, and in pseudo-assembly notation. Because this
notation is now meant to represent target code, rather than medium- or low-level
intermediate code, we will assume a ﬁxed, limited register set reminiscent of real
machines. We will reserve several registers (a1, a2, sp, rv) for special purposes;
others (r1 . . r*k*) will be available for temporary values and expression evaluation.
Figure 15.6 contains a fragment of our attribute grammar. To save space, we
**EXAMPLE** 15.6

An attribute grammar for
code generation
have shown only those productions that actually appear in Figure 15.2. As in

