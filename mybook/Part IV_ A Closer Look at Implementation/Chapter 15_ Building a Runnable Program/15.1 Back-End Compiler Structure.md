# 15.1 Back-End Compiler Structure

## **15**

## **Building a Runnable Program**

**As noted in Section 1.6,** the various phases of compilation are commonly
grouped into a* front end* responsible for the analysis of source code, a* back end*
responsible for the synthesis of target code, and often a “middle end” responsible
for language- and machine-independent code improvement. Chapters 2 and 4
discussed the work of the front end, culminating in the construction of a syntax
tree. The current chapter turns to the work of the back end, and speciﬁcally to
code generation, assembly, and linking. We will continue with code improvement
in Chapter 17.
In Chapters 6 through 10, we often discussed the code that a compiler would
generate to implement various language features. Now we will look at how the
compiler produces that code from a syntax tree, and how it combines the out-
put of multiple compilations to produce a runnable program. We begin in Sec-
tion 15.1 with a more detailed overview of the work of program synthesis than
was possible in Chapter 1. We focus in particular on one of several plausible ways
of dividing that work into phases. In Section 15.2 we then consider the many
possible forms of intermediate code passed between these phases. On the com-
panion site we provide a bit more detail on two concrete examples—the GIMPLE
and RTL formats used by the GNU compilers. We will consider two additional
intermediate forms in Chapter 16: Java bytecode and the Common Intermedi-
ate Language (CIL) used by Microsoft and other implementors of the Common
Language Infrastructure.
In Section 15.3 we discuss the generation of assembly code from an abstract
syntax tree, using attribute grammars as a formal framework. In Section 15.4 we
discuss the internal organization of binary object ﬁles and the layout of programs
in memory. Section 15.5 describes assembly. Section 15.6 considers linking.
## 15.1

**Back-End Compiler Structure**
As we noted in Chapter 4, there is less uniformity in back-end compiler structure
than there is in front-end structure. Even such unconventional compilers as text

**775**

Phases of compilation
piler. The ﬁrst three phases (scanning, parsing, and semantic analysis) are lan-
guage dependent; the last two (target code generation and machine-speciﬁc code
improvement) are machine dependent, and the middle two (intermediate code
generation and machine-independent code improvement) are (to ﬁrst approxi-
mation) dependent on neither the language nor the machine. The scanner and
parser drive a set of action routines that build a syntax tree. The semantic analyzer
traverses the tree, performing all static semantic checks and initializing various at-
tributes (mainly symbol table pointers and indications of the need for dynamic
checks) of use to the back end.
■
While certain code improvements can be performed on syntax trees, a less hi-
erarchical representation of the program makes most code improvement easier.
Our example compiler therefore includes an explicit phase for intermediate code
generation. The code generator begins by grouping the nodes of the tree into
*basic blocks*, each of which consists of a maximal-length set of operations that
should execute sequentially at run time, with no branches in or out. It then cre-
ates a* control ﬂow graph* in which the nodes are basic blocks and the arcs represent
interblock control ﬂow. Within each basic block, operations are represented as
instructions for an idealized machine with an unlimited number of registers. We
will call these* virtual registers*. By allocating a new one for every computed value,
the compiler can avoid creating artiﬁcial connections between otherwise inde-
pendent computations too early in the compilation process.
In Example 1.20 we used a simple greatest common divisor (GCD) program
**EXAMPLE** 15.2

GCD program abstract
syntax tree (reprise)
to illustrate the phases of compilation. The syntax tree for this program appeared
in Figure 1.6; it is reproduced here (in slightly altered form) as Figure 15.2. A cor-
responding control ﬂow graph appears in Figure 15.3. We will discuss techniques
to generate this graph in Section 15.3 and Exercise 15.6. Additional examples of
control ﬂow graphs will appear in Chapter 17.
■

Flow graph with pseudo-
instructions in basic blocks

Machine-
dependent

Modified flow graph

(Almost) assembly language

Real assembly language

Parser (syntax analysis)

Semantic analysis

Intermediate
code generation

Machine-independent
code improvement

Target code generation

Machine-specific
code improvement

**Back end**


![Figure 15.1 A plausible...](images/page_810_vector_360.png)
*Figure 15.1 A plausible set of compiler phases. Here we have shown a sharper separation between semantic analysis and intermediate code generation than we considered in Chapter 1 (see Figure 1.3). Machine-independent code improvement employs an intermediate form that resembles the assembly language for an idealized machine with an unlimited number of reg- isters. Machine-speciﬁc code improvement—register allocation and instruction scheduling in particular—employs the assembly language of the target machine. The dashed line shows a common “break point” between the front end and back end of a two-pass compiler. In some implementations, machine-independent code improvement may be located in a separate “middle end” pass.*

```
The machine-independent code improvement phase of compilation performs
a variety of transformations on the control ﬂow graph. It modiﬁes the instruction
sequence within each basic block to eliminate redundant loads, stores, and arith-
metic computations; this is local code improvement. It also identiﬁes and removes
a variety of redundancies across the boundaries between basic blocks within a
subroutine; this is global code improvement. As an example of the latter, an ex-
pression whose value is computed immediately before an if statement need not
be recomputed within the code that follows the else. Likewise an expression that
appears within the body of a loop need only be evaluated once if its value will not
change in subsequent iterations. Some global improvements change the number
of basic blocks and/or the arcs among them.
```

null
null

null

program


![Figure 15.2 Syntax tree...](images/page_811_vector_315.png)
*Figure 15.2 Syntax tree and symbol table for the GCD program. The only difference from Figure 1.6 is the addition of explicit null nodes to indicate empty argument lists and to terminate statement lists.*

### It is worth noting that “global” code improvement typically considers only the

### current subroutine, not the program as a whole. Much recent research in com-

### piler technology has been aimed at “truly global” techniques, known as* inter-*

### *procedural code improvement*. Since programmers are generally unwilling to give

### up separate compilation (recompiling hundreds of thousands of lines of code is a

### very time-consuming operation), a practical interprocedural code improver must

### do much of its work at link time. One of the (many) challenges to be overcome

### is to develop a division of labor and an intermediate representation that allow the

### compiler to do as much work as possible during (separate) compilation, but leave

### enough of the details undecided that the link-time code improver is able to do its

### job.

### Following machine-independent code improvement, the next phase of com-

### pilation is target code generation. This phase strings the basic blocks together

### into a linear program, translating each block into the instruction set of the target

### machine and generating branch instructions (or “fall-throughs”) that correspond

### to the arcs of the control ﬂow graph. The output of this phase differs from real

### assembly language primarily in its continued reliance on virtual registers. So long

### as the pseudoinstructions of the intermediate form are reasonably close to those

### of the target machine, this phase of compilation, though tedious, is more or less

### straightforward.

