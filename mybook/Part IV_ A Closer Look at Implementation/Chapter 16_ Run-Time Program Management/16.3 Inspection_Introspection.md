16.3 Inspection/Introspection
837
16.3
Inspection/Introspection
Symbol table metadata makes it easy for utility programs—just-in-time and dy-
namic compilers, optimizers, debuggers, proﬁlers, and binary rewriters—to in-
spect a program and reason about its structure and types. We consider debuggers
and proﬁlers in particular in Sections 16.3.2 and 16.3.3. There is no reason, how-
ever, why the use of metadata should be limited to outside tools, and indeed it is
not: Lisp has long allowed a program to reason about its own internal structure
and types (this sort of reasoning is sometimes called introspection). Java and C#
provide similar functionality through a reﬂection API that allows a program to
peruse its own metadata. Reﬂection appears in several other languages as well,
including Prolog (Sidebar 12.2) and all the major scripting languages. In a dy-
namically typed language such as Lisp, reﬂection is essential: it allows a library
or application function to type check its own arguments. In a statically typed
language, reﬂection supports a variety of programming idioms that were not tra-
ditionally feasible.
16.3.1 Reﬂection
Trivially, reﬂection can be useful when printing diagnostics. Suppose we are try-
EXAMPLE 16.18
Finding the concrete type
of a reference variable
ing to debug an old-style (nongeneric) queue in Java, and we want to trace the
objects that move through it. In the dequeue method, just before returning an
object rtn of type Object, we might write
System.out.println("Dequeued a " + rtn.getClass().getName());
If the dequeued object is a boxed int, we will see
Dequeued a java.lang.Integer
■
More signiﬁcantly, reﬂection is useful in programs that manipulate other pro-
grams. Most program development environments, for example, have mecha-
nisms to organize and “pretty-print” the classes, methods, and variables of a pro-
gram. In a language with reﬂection, these tools have no need to examine source
code: if they load the already-compiled program into their own address space,
they can use the reﬂection API to query the symbol table information created by
the compiler. Interpreters, debuggers, and proﬁlers can work in a similar fashion.
In a distributed system, a program can use reﬂection to create a general-purpose
serialization mechanism, capable of transforming an almost arbitrary structure
into a linear stream of bytes that can be sent over a network and reassembled at
the other end. (Both Java and C# include such mechanisms in their standard li-
brary, implemented on top of the basic language.) In the increasingly dynamic
838
Chapter 16 Run-Time Program Management
world of Internet applications, one can even create conventions by which a pro-
gram can “query” a newly discovered object to see what methods it implements,
and then choose which of these to call.
There are dangers, of course, associated with the undisciplined use of reﬂec-
tion. Because it allows an application to peek inside the implementation of a class
(e.g., to list its private members), reﬂection violates the normal rules of abstrac-
tion and information hiding. It may be disabled by some security policies (e.g., in
sandboxed environments). By limiting the extent to which target code can differ
from the source, it may preclude certain forms of code improvement.
Perhaps the most common pitfall of reﬂection, at least for object-oriented lan-
EXAMPLE 16.19
What not to do with
reﬂection
guages, is the temptation to write case (switch) statements driven by type in-
formation:
procedure rotate(s : shape)
case shape.type of
–– don’t do this in Java!
square: rotate square(s)
triangle: rotate triangle(s)
circle:
–– no-op
. . .
While this kind of code is common (and appropriate) in Lisp, in an object-
oriented language it is much better written with subtype polymorphism:
s.rotate()
–– virtual method call
■
Java Reﬂection
Java’s root class, Object, supports a getClass method that returns an instance
of java.lang.Class. Objects of this class in turn support a large number of re-
ﬂection operations, among them the getName method we used in Example 16.18.
A call to getName returns the fully qualiﬁed name of the class, as it is embedded
EXAMPLE 16.20
Java class-naming
conventions
in the package hierarchy. For array types, naming conventions are taken from the
JVM:
int[] A = new int[10];
System.out.println(A.getClass().getName());
// prints "[I"
String[] C = new String[10];
System.out.println(C.getClass().getName());
// "[Ljava.lang.String;"
Foo[][] D = new Foo[10][10];
System.out.println(D.getClass().getName());
// "[[LFoo;"
Here Foo is assumed to be a user-deﬁned class in the default (outermost) package.
A left square bracket indicates an array type; it is followed by the array’s element
type. The built-in types (e.g., int) are represented in this context by single-letter
names (e.g., I). User-deﬁned types are indicated by an L, followed by the fully
qualiﬁed class name and terminated by a semicolon. Notice the similarity of the
second example (C) to entry #12 in the constant pool of Figure 16.1: that entry
16.3 Inspection/Introspection
839
gives the parameter types (in parentheses) and return type (V means void) of
main. As every Java programmer knows, main expects an array of strings.
■
A call to o.getClass() returns information on the concrete type of the object
referred to by o, not on the abstract type of the reference o. If we want a Class
EXAMPLE 16.21
Getting information on a
particular class
object for a particular type, we can create a dummy object of that type:
Object o = new Object();
System.out.println(o.getClass().getName());
// "java.lang.Object"
Alternatively, we can append the pseudo ﬁeld name .class to the name of the
type itself:
System.out.println(Object.class.getName());
// "java.lang.Object"
In the reverse direction, we can use static method forName of class Class to
obtain a Class object for a type with a given (fully qualiﬁed) character string
name:
Class stringClass = Class.forName("java.lang.String");
Class intArrayClass = Class.forName("[I");
Method forName works only for reference types. For built-ins, one can either use
the .class syntax or the .TYPE ﬁeld of one of the standard wrapper classes:
Class intClass = Integer.TYPE;
■
Given a Class object c, one can call c.getSuperclass() to obtain a Class
object for c’s parent. In a similar vein, c.getClasses() will return an array of
Class objects, one for each public class declared within c’s class. Perhaps more
interesting, c.getMethods(), c.getFields(), and c.getConstructors() will
return arrays of objects representing all c’s public methods, ﬁelds, and construc-
tors (including those inherited from ancestor classes). The elements of these
arrays are instances of classes Method, Field, and Constructor, respectively.
These are declared in package java.lang.reflect, and serve roles analogous to
that of Class. The many methods of these classes allow one to query almost any
aspect of the Java type system, including modiﬁers (static, private, final,
abstract, etc.), type parameters of generics (but not of generic instances—those
are erased), interfaces implemented by classes, exceptions thrown by methods,
and much more. Perhaps the most conspicuous thing that is not available through
the Java reﬂection API is the bytecode that implements methods. Even this, how-
ever, can be examined using third-party tools such as the Apache Byte Code En-
gineering Library (BCEL) or ObjectWeb’s ASM, both of which are open source.
Figure 16.4 shows Java code to list the methods declared in (but not inherited
EXAMPLE 16.22
Listing the methods of a
Java class
by) a given class. Also shown is output for AccessibleObject, the parent class
of Method, Field, and Constructor. (The primary purpose of this class is to
840
Chapter 16 Run-Time Program Management
import static java.lang.System.out;
public static void listMethods(String s)
throws java.lang.ClassNotFoundException {
Class c = Class.forName(s);
// throws if class not found
for (Method m : c.getDeclaredMethods()) {
out.print(Modifier.toString(m.getModifiers()) + " ");
out.print(m.getReturnType().getName() + " ");
out.print(m.getName() + "(");
boolean first = true;
for (Class p : m.getParameterTypes()) {
if (!first) out.print(", ");
first = false;
out.print(p.getName());
}
out.println(") ");
}
}
Sample output for listMethods("java.lang.reflect.AccessibleObject"):
public java.lang.annotation.Annotation getAnnotation(java.lang.Class)
public boolean isAnnotationPresent(java.lang.Class)
public [Ljava.lang.annotation.Annotation; getAnnotations()
public [Ljava.lang.annotation.Annotation; getDeclaredAnnotations()
public static void setAccessible([Ljava.lang.reflect.AccessibleObject;, boolean)
public void setAccessible(boolean)
private static void setAccessible0(java.lang.reflect.AccessibleObject, boolean)
public boolean isAccessible()


![Figure 16.4 Java reﬂection...](images/page_873_caption_Figure%2016.4%20Java%20re%EF%AC%82ection%20code%20to%20list%20the%20methods%20of%20a%20given%20class.%20Sample%20output%20is%20shown%20below%20b.png)
*Figure 16.4 Java reﬂection code to list the methods of a given class. Sample output is shown below below the code.*

control whether the reﬂection interface can be used to override access control
[private, protected] for the given object.)
■
One can even use reﬂection to call a method of an object whose class is not
EXAMPLE 16.23
Calling a method with
reﬂection
known at compile time. Suppose that someone has created a stack containing a
single integer:
Stack s = new Stack();
s.push(new Integer(3));
Now suppose we are passed this stack as a parameter u of Object type. We can
use reﬂection to explore the concrete type of u. In the process we will discover
that its second method, named pop, takes no arguments and returns an Object
result. We can call this method using Method.invoke:
16.3 Inspection/Introspection
841
Method uMethods[] = u.getClass().getMethods();
Method method1 = uMethods[1];
Object rtn = method1.invoke(u);
// u.pop()
A call to rtn.getClass().getName() will return java.lang.Integer. A call
to ((Integer) rtn).intValue() will return the value 3 that was originally
pushed into s.
■
Other Languages
C#’s reﬂection API is similar to that of Java: System.Type is analogous to
java.lang.Class; System.Reflection is analogous to java.lang.reflect.
The pseudo function typeof plays the role of Java’s pseudo ﬁeld .class. More
substantive differences stem from the fact that PE assemblies contain a bit more
information than is found in Java class ﬁles.
We can ask for names of for-
mal parameters in C#, for example, not just their types.
More signiﬁcantly,
the use of reiﬁcation instead of erasure for generics means that we can re-
trieve precise information on the type parameters used to instantiate a given
object. Perhaps the biggest difference is that .NET provides a standard library,
System.Reflection.Emit, to create PE assemblies and to populate them with
CIL. The functionality of Reflection.Emit is roughly comparable to that of the
BCEL and ASM tools mentioned in the previous subsection. Because it is part of
the standard library, however, it is available to any program running on the CLI.
All of the major scripting languages (Perl, PHP, Tcl, Python, Ruby, JavaScript)
provide extensive reﬂection mechanisms. The precise set of capabilities varies
some from language to language, and the syntax varies quite a bit, but all allow
a program to explore its own structure and types. From the programmer’s point
of view, the principal difference between reﬂection in Java and C# on the one
hand, and in scripting languages on the other, is that the scripting languages—
like Lisp—are dynamically typed. In Ruby, for example, we can discover the class
EXAMPLE 16.24
Reﬂection facilities in Ruby
of an object, the methods of a class or object, and the number of parameters
expected by each method, but the parameters themselves are untyped until the
method is called. In the following code, method p prints its argument to standard
output, followed by a newline:
squares = {2=>4, 3=>9}
p squares.class
# Hash
p Hash.public_instance_methods.length
# 146 -- Hashes have many methods
p squares.public_methods.length
# 146 -- those same methods
m = Hash.public_instance_methods[12]
p m
# ":store"
p squares.method(m).arity
# 2 -- key and value to be stored
As in Java and C#, we can also invoke a method whose name was not known
at compile time:
842
Chapter 16 Run-Time Program Management
squares.store(1, 1)
# static invocation
p squares
# {2=>4, 3=>9, 1=>1}
squares.send(m, 0, 0)
# dynamic invocation
p squares
# {2=>4, 3=>9, 1=>1, 0=>0}
■
As suggested at the beginning of this section, reﬂection is in some sense more
“natural” in scripting languages (and in Lisp and Prolog) than it is in Java or
C#: detailed symbol table information is needed at run time to perform dynamic
type checks; in an interpreted implementation, it is also readily available. Lisp
programmers have known for decades that reﬂection was useful for many ad-
ditional purposes. The designers of Java and C# clearly felt these purposes were
valuable enough to justify adding reﬂection (with considerably higher implemen-
tation complexity) to a compiled language with static typing.
Annotations and Attributes
Both Java and C# allow the programmer to extend the metadata saved by the
compiler. In Java, these extensions take the form of annotations attached to decla-
rations. Several annotations are built into the programming language. These play
the role of pragmas. In Example C 7.65, for example, we noted that the Java com-
piler will generate warnings when a generic class is assigned into a variable of the
equivalent nongeneric class. The warning indicates that the code is not statically
type-safe, and that an error message is possible at run time. If the programmer
is certain that the error cannot arise, the compile-time warning can be disabled
by preﬁxing the method in which the assignment appears with the annotation
@SuppressWarnings("unchecked").
In general, a Java annotation resembles an interface whose methods take no
parameters, throw no exceptions, and return values of one of a limited number
of predeﬁned types. An example of a user-deﬁned annotation appears in Fig-
EXAMPLE 16.25
User-deﬁned annotations
in Java
ure 16.5. If we run the program it will print
author:
Michael Scott
date:
July, 2015
revision:
0.1
docString: Illustrates the use of annotations
■
The C# equivalent of Figure 16.5 appears in Figure 16.6. Here user-deﬁned
EXAMPLE 16.26
User-deﬁned annotations
in C#
annotations (known as attributes in C#) are classes, not interfaces, and the syntax
for attaching an attribute to a declaration uses square brackets instead of an @
sign.
■
In effect, annotations (attributes) serve as compiler-supported comments,
with well-deﬁned structure and an API that makes them accessible to automated
perusal. As we have seen, they may be read by the compiler (as pragmas) or by
reﬂective programs. They may also be read by independent tools. Such tools can
be surprisingly versatile.
An obvious use is the automated creation of documentation. Java annotations
EXAMPLE 16.27
javadoc
(ﬁrst introduced in Java 5) were inspired at least in part by experience with the
16.3 Inspection/Introspection
843
import static java.lang.System.out;
import java.lang.annotation.*;
@Retention(RetentionPolicy.RUNTIME)
@interface Documentation{
String author();
String date();
double revision();
String docString();
}
@Documentation(
author = "Michael Scott",
date = "July, 2015",
revision = 0.1,
docString = "Illustrates the use of annotations"
)
public class Annotate {
public static void main(String[] args) {
Class<Annotate> c = Annotate.class;
Documentation a = c.getAnnotation(Documentation.class);
out.println("author:
" + a.author());
out.println("date:
" + a.date());
out.println("revision:
" + a.revision());
out.println("docString: " + a.docString());
}
}


![Figure 16.5 User-deﬁned annotations...](images/page_876_caption_Figure%2016.5%20User-de%EF%AC%81ned%20annotations%20in%20Java.%20Retention%20is%20a%20built-in%20annotation%20for%20annota-%20tions.%20I.png)
*Figure 16.5 User-deﬁned annotations in Java. Retention is a built-in annotation for annota- tions. It indicates here that Documentation annotations should be saved in the class ﬁle pro- duced by the Java compiler, where they will be available to run-time reﬂection.*

earlier javadoc tool, which produces HTML-formatted documentation based on
structured comments in Java source code. The @Documented annotation, when
attached to the declaration of a user-deﬁned annotation, indicates that javadoc
should include the annotation when creating its reports. One can easily imagine
more sophisticated documentation systems that tracked the version history and
bug reports for a program over time.
■
The various communication technologies in .NET make extensive use of at-
EXAMPLE 16.28
Intercomponent
communication
tributes to indicate which methods should be available for remote execution, how
their parameters should be marshalled into messages, which classes need serial-
ization code, and so forth. Automatic tools use these attributes to create appro-
priate stubs for remote communication, as described (in language-neutral terms)
in Section C 13.5.4.
■
In a similar vein, the .NET LINQ mechanism uses attributes to deﬁne the map-
EXAMPLE 16.29
Attributes for LINQ
ping between classes in a user program and tables in a relational database, allow-
844
Chapter 16 Run-Time Program Management
using System;
using System.Reflection;
[AttributeUsage(AttributeTargets.Class)]
// Documentation attribute can applied only to classes
public class DocumentationAttribute : System.Attribute {
public string author;
public string date;
// these should perhaps be properties
public double revision;
public string docString;
public DocumentationAttribute(string a, string d, double r, string s) {
author = a;
date = d;
revision = r;
docString = s;
}
}
[Documentation("Michael Scott",
"July, 2015", 0.1, "Illustrates the use of attributes")]
public class Attr {
public static void Main(string[] args) {
System.Reflection.MemberInfo tp = typeof(Attr);
object[] attrs =
tp.GetCustomAttributes(typeof(DocumentationAttribute), false);
// false means don't search ancestor classes and interfaces
DocumentationAttribute a = (DocumentationAttribute) attrs[0];
Console.WriteLine("author:
" + a.author);
Console.WriteLine("date:
" + a.date);
Console.WriteLine("revision:
" + a.revision);
Console.WriteLine("docString: " + a.docString);
}
}


![Figure 16.6 User-deﬁned attributes...](images/page_877_caption_Figure%2016.6%20User-de%EF%AC%81ned%20attributes%20in%20C%23.%20This%20code%20is%20roughly%20equivalent%20to%20the%20Java%20version%20in%20Fig.png)
*Figure 16.6 User-deﬁned attributes in C#. This code is roughly equivalent to the Java version in Figure 16.5. AttributeUsage is a predeﬁned attribute indicating properties of the attribute to whose declaration it is attached.*

ing an automatic tool to generate SQL queries that implement iterators and other
language-level operations.
■
In an even more ambitious vein, independent tools can be used to modify or
analyze programs based on annotations. One could imagine inserting logging
code into certain annotated methods, or building a testing harness that called
annotated methods with speciﬁed arguments and checked for expected results
(Exercise 16.11). JML, the Java Modeling Language, allows the programmer to
EXAMPLE 16.30
The Java Modeling
Language
specify preconditions, post-conditions, and invariants for classes, methods, and
statements, much like those we considered under “Assertions” in Section 4.1.
JML builds on experience with an earlier multilanguage, multi-institution project
known as Larch. Like javadoc, JML uses structured comments rather than the
newer compiler-supported annotations to express its speciﬁcations, so they are
not automatically included in class ﬁles. A variety of tools can be used, however,
16.3 Inspection/Introspection
845
to verify that a program conforms to its speciﬁcations, either statically (where
possible) or at run time (via insertion of semantic checks).
■
Java 5 introduced a program called apt designed to facilitate the construction
EXAMPLE 16.31
Java annotation processors
of annotation processing tools. The functionality of this tool was subsequently
integrated into Sun’s Java 6 compiler. Its key enabling feature is a set of APIs
(in javax.annotation.processing) that allow an annotation processor class to
be added to a program in such a way that the compiler will run it at compile
time. Using reﬂection, the class can peruse the static structure of the program
(including annotations and full information on generics) in much the same way
that traditional reﬂection mechanisms allow a running program to peruse its own
types and structure.
■
16.3.2 Symbolic Debugging
Most programmers are familiar with symbolic debuggers: they are built into most
programming language interpreters, virtual machines, and integrated program
development environments. They are also available as stand-alone tools, of which
the best known is probably GNU’s gdb. The adjective symbolic refers to a debug-
ger’s understanding of high-level language syntax—the symbols in the original
program. Early debuggers understood assembly language only.
In a typical debugging session, the user starts a program under the control of
the debugger, or attaches the debugger to an already running program. The de-
bugger then allows the user to perform two main kinds of operations. One kind
inspects or modiﬁes program data; the other controls execution: starting, stop-
ping, stepping, and establishing breakpoints and watchpoints. A breakpoint speci-
ﬁes that execution should stop if it reaches a particular location in the source code.
A watchpoint speciﬁes that execution should stop if a particular variable is read
or written. Both breakpoints and watchpoints can typically be made conditional,
so that execution stops only if a particular Boolean predicate evaluates to true.
Both data and control operations depend critically on symbolic information. A
symbolic debugger needs to be able both to parse source language expressions and
to relate them to symbols in the original program. In gdb, for example, the com-
mand print a.b[i] needs to parse the to-be-printed expression; it also needs to
recognize that a and i are in scope at the point where the program is currently
stopped, and that b is an array-typed ﬁeld whose index range includes the current
value of i. Similarly, the command break 123 if i+j == 3 needs to parse the
expression i+j; it also needs to recognize that there is an executable statement at
line 123 in the current source ﬁle, and that i and j are in scope at that line.
Both data and control operations also depend on the ability to manipulate a
program from outside: to stop and start it, and to read and write its data. This
control can be implemented in at least three ways. The easiest occurs in inter-
preters. Since an interpreter has direct access to the program’s symbol table and
is “in the loop” for the execution of every statement, it is a straightforward matter
to move back and forth between the program and the debugger, and to give the
latter access to the former’s data.
846
Chapter 16 Run-Time Program Management
The technology of dynamic binary rewriting (as in Dynamo and Pin) can also
be used to implement debugger control [ZRA+08]. This technology is relatively
new, however, and is not widely employed in production debugging tools.
For compiled programs, the third implementation of debugger control is by far
the most common. It depends on support from the operating system. In Unix,
it employs a kernel service known as ptrace. The ptrace kernel call allows a
debugger to “grab” (attach to) an existing process or to start a process under its
control. The tracing process (the debugger) can intercept any signals sent to the
traced process by the operating system and can read and write its registers and
memory. If the traced process is currently running, the debugger can stop it by
sending it a signal. If it is currently stopped, the debugger can specify the address
at which it should resume execution, and can ask the kernel to run it for a single
instruction (a process known as single stepping) or until it receives another signal.
Perhaps the most mysterious parts of debugging from the user’s perspective are
the mechanisms used to implement breakpoints, watchpoints, and single step-
ping. The default implementation, which works on any modern processor, relies
DESIGN & IMPLEMENTATION
16.6 DWARF
To enable symbolic debugging, the compiler must include symbol table in-
formation in each object ﬁle, in a format the debugger can understand. The
DWARF format, used by many systems (Linux among them) is among the
most versatile available [DWA10, Eag12]. Originally developed by Brian Rus-
sell of Bell Labs in the late 1980s, DWARF is now maintained by the inde-
pendent DWARF Committee, led by Michael Eager. Version 5 is expected to
appear in late 2015.
Unlike many proprietary formats, DWARF is designed to accommodate a
very wide range of (statically typed) programming languages and an equally
wide variety of machine architectures. Among other things, it encodes the
representation of all program types, the names, types, and scopes of all pro-
gram objects (in the broad sense of the term), the layout of all stack frames,
and the mapping from source ﬁles and line numbers to instruction addresses.
Much emphasis has been placed on terse encoding. Program objects are
described hierarchically, in a manner reminiscent of an AST. Character string
names and other repeated elements are captured exactly once, and then ref-
erenced indirectly. Integer constants and references employ a variable-length
encoding, so small values take fewer bits. Perhaps most important, stack lay-
outs and source-to-address mappings are encoded not as explicit tables, but as
ﬁnite automata that generate the tables, line by line. For the tiny gcd program
of Example 1.20, a human-readable representation of the DWARF debugging
information (as produced by Linux’s readelf tool) would ﬁll more than four
full pages of this book. The binary encoding in the object ﬁle takes only 571
bytes.
16.3 Inspection/Introspection
847
on the ability to modify the memory space of the traced process—in particular,
the portion containing the program’s code. As an example, suppose the traced
EXAMPLE 16.32
Setting a breakpoint
process is currently stopped, and that before resuming it the debugger wishes to
set a breakpoint at the beginning of function foo. It does so by replacing the ﬁrst
instruction of the function’s prologue with a special kind of trap.
Trap instructions are the normal way a process requests a service from the op-
erating system. In this particular case, the kernel interprets the trap as a request to
stop the currently running process and return control to the debugger. To resume
the traced process in the wake of the breakpoint, the debugger puts back the orig-
inal instruction, asks the kernel to single-step the traced process, replaces the in-
struction yet again with a trap (to reenable the breakpoint), and ﬁnally resumes
the process. For a conditional breakpoint, the debugger evaluates the condition’s
predicate when the breakpoint occurs. If the breakpoint is unconditional, or if
the condition is true, the debugger jumps to its command loop and waits for user
input. If the predicate is false, it resumes the traced process automatically and
transparently. If the breakpoint is set in an inner loop, where control will reach it
frequently, but the condition is seldom true, the overhead of switching back and
forth between the traced process and the debugger can be very high.
■
Some processors provide hardware support to make breakpoints a bit faster.
The x86, for example, has four debugging registers that can be set (in kernel mode)
EXAMPLE 16.33
Hardware breakpoints
to contain an instruction address. If execution reaches that address, the processor
simulates a trap instruction, saving the debugger the need to modify the address
space of the traced process and eliminating the extra kernel calls (and the extra
round trip between the traced process and the debugger) needed to restore the
original instruction, single-step the process, and put the trap back in place. In a
similar vein, many processors, including the x86, can be placed in single-stepping
mode, which simulates a trap after every user-mode instruction. Without such
support, the debugger (or the kernel) must implement single-stepping by repeat-
edly placing a (temporary) breakpoint at the next instruction.
■
Watchpoints are a bit trickier. By far the easiest implementation depends on
hardware support. Suppose we want to drop into the debugger whenever the
program modiﬁes some variable x. The debugging registers of the x86 and other
EXAMPLE 16.34
Setting a watchpoint
modern processors can be set to simulate a trap whenever the program writes to
x’s address. When the processor lacks such hardware support, or when the user
asks the debugger to set more breakpoints or watchpoints than the hardware can
support, there are several alternatives, none of them attractive. Perhaps the most
obvious is to single step the process repeatedly, checking after each instruction to
see whether x has been modiﬁed. If the processor also lacks a single-step mode,
the debugger will want to place its temporary breakpoints at successive store in-
structions rather than at every instruction (it may be able to skip some of the store
instructions if it can prove they cannot possibly reach the address of x). Alterna-
tively, the debugger can modify the address space of the traced process to make
x’s page unwritable. The process will then take a segmentation fault on every
write to that page, allowing the debugger to intervene. If the write is actually to
848
Chapter 16 Run-Time Program Management
x, the debugger jumps to its command loop. Otherwise it performs the write on
the process’s behalf and asks the kernel to resume it.
■
Unfortunately, the overhead of repeated context switches between the traced
process and the debugger dramatically impacts the performance of software
watchpoints: slowdowns of 1000× are not uncommon. Debuggers based on dy-
namic binary rewriting have the potential to support arbitrary numbers of watch-
points at speeds close to those admitted by hardware watchpoint registers. The
idea is straightforward: the traced program runs as partial execution traces in a
trace cache managed by the debugger. As it generates each trace, the debugger
adds instructions at every store, in-line, to check whether it writes to x’s address
and, if so, to jump back to the command loop.
16.3.3 Performance Analysis
Before placing a debugged program into production use, one often wants to
understand—and if possible improve—its performance. Tools to proﬁle and an-
alyze programs are both numerous and varied—far too much so to even survey
them here. We focus therefore on the run-time technologies, described in this
chapter, that feature prominently in many analysis tools.
Perhaps the simplest way to measure, at least approximately, the amount of
EXAMPLE 16.35
Statistical sampling
time spent in each part of the code is to sample the program counter (PC) pe-
riodically. This approach was exempliﬁed by the classic prof tool in Unix. By
linking with a special prof library, a program could arrange to receive a periodic
timer signal—once a millisecond, say—in response to which it would increment a
counter associated with the current PC. After execution, the prof post-processor
would correlate the counters with an address map of the program’s code and pro-
duce a statistical summary of the percentage of time spent in each subroutine and
loop.
■
While simple, prof had some serious limitations. Its results were only approx-
imate, and could not capture ﬁne-grain costs. It also failed to distinguish among
calls to a given routine from multiple locations. If we want to know which of A, B,
EXAMPLE 16.36
Call graph proﬁling
and C is the biggest contributor to program run time, it is not particularly helpful
to learn that all three of them call D, where most of the time is actually spent. If
we want to know whether it is A’s Ds, B’s Ds, or C’s Ds that are so expensive, we
can use the (slightly) more recent gprof tool, which relies on compiler support to
instrument procedure prologues. As the instrumented program runs, it logs the
number of times that D is called from each location. The gprof post-processor
then assumes that the total time spent in D can accurately be apportioned among
the call sites according to the relative number of calls. More sophisticated tools
log not only the caller and callee but also the stack backtrace (the contents of the
dynamic chain), allowing them to cope with the case in which D consumes twice
as much time when called from A as it does when called from B or C (see Exer-
cise 16.13).
■
If our program is underperforming for algorithmic reasons, it may be enough
to know where it is spending the bulk of its time. We can focus our attention on
16.3 Inspection/Introspection
849
improving the source code in the places it will matter most. If the program is
underperforming for other reasons, however, we generally need to know why. Is
it cache misses due to poor locality, perhaps? Branch mispredictions? Poor use
of the processor pipeline? Tools to address these and similar questions generally
rely on more extensive instrumentation of the code or on some sort of hardware
support.
As an example of instrumentation, consider the task of identifying basic blocks
EXAMPLE 16.37
Finding basic blocks with
low IPC
that execute an unusually small number of instructions per cycle. To ﬁnd such
blocks we can combine (1) the aggregate time spent in each block (obtained by
statistical sampling), (2) a count of the number of times each block executes (ob-
tained via instrumentation), and (3) static knowledge of the number of instruc-
tions in each block. If basic block i contains ki instructions and executes ni times
during a run of a program, it contributes kini dynamic instructions to that run.
Let N = 
i kini be the total number of instructions in the run. If statistical
sampling indicates that block i accounts for xi% of the time in the run and xi is
signiﬁcantly larger than (kini)/N, then something strange is going on—probably
an unusually large number of cache misses.
■
Most modern processors provide a set of performance counters that can be used
to good effect by performance analysis tools. The Intel Haswell processor, for ex-
EXAMPLE 16.38
Haswell performance
counters
ample, has built-in counters for clock ticks (both total and when running) and
instructions executed. It also has four general-purpose counters, which can be
conﬁgured by the kernel to count any of more than 250 different kinds of events,
including branch mispredictions; TLB (address translation) misses; and various
kinds of cache misses, interrupts, executed instructions, and pipeline stalls. Fi-
nally, it has counters for the number of Joules of energy consumed by the pro-
cessor cores, caches, and memory. Unfortunately, performance counters are gen-
erally a scarce resource (one might often wish for many more of them). Their
number, type, and mode of operation varies greatly from processor to processor;
direct access to them is usually available only in kernel mode; and operating sys-
tems do not always export that access to user-level programs with a convenient or
uniform interface. Tools to access the counters and interpret their values are avail-
able from most manufacturers—Intel among them. Portable tools are an active
topic of research.
■
3CHECK YOUR UNDERSTANDING
27. What is reﬂection? What purposes does it serve?
28. Describe an inappropriate use of reﬂection.
29. Name an aspect of reﬂection supported by the CLI but not by the JVM.
30. Why is reﬂection more difﬁcult to implement in Java or C# than it is in Perl
or Ruby?
31. What are annotations (Java) or attributes (C#)? What are they used for?
