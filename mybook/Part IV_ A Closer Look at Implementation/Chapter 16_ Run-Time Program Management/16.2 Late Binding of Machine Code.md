# 16.2 Late Binding of Machine Code

**822**
Chapter 16* Run-Time Program Management*

## 16.2

**Late Binding of Machine Code**
In the traditional conception (Example 1.7), compilation is a one-time activity,
sharply distinguished from program execution. The compiler produces a tar-
get program, typically in machine language, which can subsequently be executed
many times for many different inputs.
In some environments, however, it makes sense to bring compilation and ex-
ecution closer together in time. A* just-in-time* (JIT) compiler translates a pro-
gram from source or intermediate form into machine language immediately be-
fore each separate run of the program. We consider JIT compilation further in the
ﬁrst subsection below. We also consider language systems that may compile new
pieces of a program—or recompile old pieces—after the program begins its exe-
cution. In Sections 16.2.2 and 16.2.3, we consider* binary translation* and* binary*
*rewriting* systems, which perform compiler-like operations on programs* without*
access to source code. Finally, in Section 16.2.4, we consider systems that may
download program components from remote locations. All these systems serve
to delay the binding of a program to its machine code.

16.2.1** Just-in-Time and Dynamic Compilation**

To promote the Java language and virtual machine, Sun Microsystems coined the
slogan “write once, run anywhere”—the idea being that programs distributed
as Java bytecode could run on a very wide range of platforms. Source code, of
course, is also portable, but byte code is much more compact, and can be inter-
preted without additional preprocessing. Unfortunately, interpretation tends to
be expensive. Programs running on early Java implementations could be as much
as an order of magnitude slower than compiled code in other languages. Just-in-
time compilation is, to ﬁrst approximation, a technique to retain the portability of
bytecode while improving execution speed. Like both interpretation and dynamic
linking (Section 15.7), JIT compilation also beneﬁts from the delayed discovery of
program components: program code is not bloated by copies of widely shared li-
braries, and new versions of libraries are obtained automatically when a program
that needs them is run.
Because a JIT system compiles programs immediately prior to execution, it
can add signiﬁcant delay to program start-up time. Implementors face a difﬁcult
tradeoff: to maximize beneﬁts with respect to interpretation, the compiler should
produce good code; to minimize start-up time, it should produce that code very
quickly. In general, JIT compilers tend to focus on the simpler forms of target
code improvement. Speciﬁcally, they often limit themselves to the so-called* local*
improvements, which operate within individual control-ﬂow constructs. Im-
provements at the* global* (whole method) and* interprocedural* (whole program)
level may be expensive to consider.

```
C o = new C( args );
o.bar();
// no question what type this is
```

Other times it is not:

HotSpot is Oracle’s principal JVM and JIT compiler for desktop and server sys-
tems. It was ﬁrst released in 1999, and is available as open source.
HotSpot takes its name from its use of dynamic compilation to improve the
performance of hot code paths. Newly loaded class ﬁles are initially interpreted.
Methods that are executed frequently are selected by the JVM for compilation and
are subsequently patched into the program on the ﬂy. The compiler is aggressive
about in-lining small routines, and will do so in a deep, iterative fashion, repeat-
edly in-lining routines that are called from the code it just ﬁnished in-lining. As
described in the preceding discussion of dynamic compilation, the compiler will
also in-line routines that are safe only for the current set of class ﬁles, and will
dynamically “deoptimize” in-lined calls that have been rendered unsafe by the
loading of new derived classes.
The HotSpot compiler can be conﬁgured to operate in either “client” or
“server” mode. Client mode is optimized for lower start-up latency. It is ap-
propriate for systems in which a human user frequently starts new programs. It
translates Java bytecode to static single assignment (SSA) form (a medium-level
IF described in Section C 17.4.1) and performs a few straightforward machine-
independent optimizations. It then translates to a low-level IF, on which it per-

Given the C# declaration, we can write

```
y = square_func(3);
// 9
```

But just as Lisp allows a function to be represented as a list, so too does C# allow
a lambda expression to be represented as a syntax tree:

```
Expression<Func<int, int>> square_tree = x => x * x;
```

```
Various methods of library class Expression can now be used to explore and
manipulate the tree. When desired, the tree can be converted to CIL code:
```

```
square_func = square_tree.Compile();
```

These operations are roughly analogous to the following in Scheme:

pN

loop head


![Figure 16.3 Creation of...](images/page_865_vector_360.png)
*Figure 16.3 Creation of a partial execution trace. Procedure print matching (shown at top) is often called with a particular predicate, p, which is usually false. The control ﬂow graph (left, with hot blocks in bold and the hot path in grey) can be reorganized at run time to improve instruction-cache locality and to optimize across abstraction boundaries (right).*

loop-termination and predicate-checking tests) jump either to other traces or, if
appropriate ones have not yet been created, back into Dynamo.
By identifying and optimizing traces, Dynamo is able to signiﬁcantly improve
locality in the instruction cache, and to apply standard code improvement tech-
niques across the boundaries between separately compiled modules and dynam-
ically loaded libraries. In Figure 16.3, for example, it will perform register allo-
cation jointly across print matchings and the predicate p. It can even perform
instruction scheduling across basic blocks if it inserts appropriate* compensating*
*code* on branches out of the trace. An instruction in block test2, for example,
can be moved into the loop footer if a copy is placed on the branch to the right.
Traces have proved to be a very powerful technique. They are used not only by dy-
namic optimizers, but by dynamic translators like Rosetta as well, and by binary
instrumentation tools like Pin (to be discussed in Section 16.2.3).
■

