16
Run-Time Program Management
Every nontrivial implementation of a high-level programming language makes
extensive use of libraries. Some library routines are very simple: they may copy
memory from one place to another, or perform arithmetic functions not directly
supported by the hardware. Others are more sophisticated. Heap management
routines, for example, maintain signiﬁcant amounts of internal state, as do li-
braries for buffered or graphical I/O.
In general, we use the term run-time system (or sometimes just runtime, with-
out the hyphen) to refer to the set of libraries on which the language implemen-
tation depends for correct operation. Some parts of the runtime, like heap man-
agement, obtain all the information they need from subroutine arguments, and
can easily be replaced with alternative implementations. Others, however, require
more extensive knowledge of the compiler or the generated program. In simpler
cases, this knowledge is really just a set of conventions (e.g., for the subroutine
calling sequence) that the compiler and runtime both respect. In more complex
cases, the compiler generates program-speciﬁc metadata that the runtime must
inspect to do its job. A tracing garbage collector (Section 8.5.3), for example, de-
pends on metadata identifying all the “root pointers” in the program (all global,
static, and stack-based pointer or reference variables), together with the type of
every reference and of every allocated block.
Many examples of compiler/runtime integration have been discussed in previ-
ous chapters; we review these in Sidebar 16.1. The length and complexity of the
list generally means that the compiler and the run-time system must be developed
together.
Some languages (notably C) have very small run-time systems: most of the
user-level code required to execute a given source program is either generated
directly by the compiler or contained in language-independent libraries. Other
languages have extensive run-time systems. C#, for example, is heavily dependent
on a run-time system deﬁned by the Common Language Infrastructure (CLI)
standard [Int12a].
Like any run-time system, the CLI depends on data generated by the com-
EXAMPLE 16.1
The CLI as a run-time
system and virtual machine
piler (e.g., type descriptors, lists of exception handlers, and certain content from
the symbol table). It also makes extensive assumptions about the structure of
807
808
Chapter 16 Run-Time Program Management
DESIGN & IMPLEMENTATION
16.1 Run-time systems
Many of the most interesting topics in language implementation revolve
around the run-time system, and have been covered in previous chapters. To
set the stage for virtual machines, we review those topics here.
Garbage Collection (Section 8.5.3). As noted in the chapter introduction,
a tracing garbage collector must be able to ﬁnd all the “root pointers” in the
program, and to identify the type of every reference and every allocated block.
A compacting collector must be able to modify every pointer in the program.
A generational collector must have access to a list of old-to-new pointer ref-
erences, maintained by write barriers in the main program. A collector for a
language like Java must call appropriate finalize methods. And in imple-
mentations that support concurrent or incremental collection, the main pro-
gram and the collector must agree on some sort of locking protocol to preserve
the consistency of the heap.
Variable Numbers of Arguments (Section 9.3.3). Several languages allow the
programmer to declare functions that take an arbitrary number of arguments,
of arbitrary type. In C, a call to va_arg(my_args, arg_type) must return the
next argument in the previously identiﬁed list my_args. To ﬁnd the argument,
va_arg must understand which arguments are passed in which registers, and
which arguments are passed on the stack (with what alignment, padding, and
offset). If the code for va_arg is generated entirely in-line, this knowledge may
be embedded entirely in the compiler. If any of the code is in library routines,
however, those routines are compiler-speciﬁc, and thus a (simple) part of the
run-time system.
Exception Handling (Section 9.4). Exception propagation requires that we
“unwind” the stack whenever control escapes the current subroutine. Code
to deallocate the current frame may be generated by the compiler on a
subroutine-by-subroutine basis. Alternatively, a general-purpose routine to
deallocate any given frame may be part of the run-time system. In a similar
vein, the closest exception handler around a given point in the program may
be found by compiler-generated code that maintains a stack of active handlers,
or by a general-purpose run-time routine that inspects a table of program-
counter-to-handler mappings generated at compile time. The latter approach
avoids any run-time cost when entering and leaving a protected region (try
block).
Event Handling (Section 9.6). Events are commonly implemented as “spon-
taneous” subroutine calls in a single-threaded program, or as “callbacks” in
a separate, dedicated thread of a concurrent program. Depending on imple-
mentation strategy, they may be able to exploit knowledge of the compiler’s
Chapter 16 Run-Time Program Management
809
subroutine calling conventions. They also require synchronization between
the main program and the event handler, to protect the consistency of shared
data structures. A truly asynchronous call—one that may interrupt execution
of the main program at any point—may need to save the entire register set of
the machine. Calls that occur only at well-deﬁned “safe points” in the program
(implemented via polling) may be able to save a smaller amount of state. In
either case, calls to any handler not at the outermost level of lexical nesting may
need to interpret a closure to establish the proper referencing environment.
Coroutine and Thread Implementation (Sections 9.5 and 13.2.4). Code to
create a coroutine or thread must allocate and initialize a stack, establish a
referencing environment, perform any set-up needed to handle future excep-
tions, and invoke a speciﬁed start-up routine. Routines like transfer, yield,
reschedule, and sleep on (as well as any scheduler-based synchronization
mechanisms) must likewise understand a wealth of details about the imple-
mentation of concurrency.
Remote Procedure Call (Section C 13.5.4). Remote procedure call (RPC)
merges aspects of events and threads: from the server’s point of view, an RPC
is an event executed by a separate thread in response to a request from a client.
Whether built into the language or implementedvia a stub compiler, it requires
a run-time system (dispatcher) with detailed knowledge of calling conventions,
concurrency, and storage management.
Transactional Memory (Section 13.4.4). A software implementation of trans-
actional memory must buffer speculative updates, track speculative reads, de-
tect conﬂicts with other transactions, and validate its view of memory before
performing any operation that might be compromised by inconsistency. It
must also be prepared to roll back its updates if aborted, or to make them per-
manent if committed. These operations typically require library calls at the
beginning and end of every transaction, and at most read and write instruc-
tions in between. Among other things, these calls must understand the layout
of objects in memory, the meaning of metadata associated with objects and
transactions, and the policy for arbitrating between conﬂicting transactions.
Dynamic Linking (Section C 15.7). In any system with separate compilation,
the compiler generates symbol table information that the linker uses to resolve
external references. In a system with fully dynamic (lazy) linking, external ref-
erences are (temporarily) ﬁlled with pointers to the linker, which must then
be part of the run-time system. When the program tries to call a routine that
has not yet been linked, it actually calls the linker, which resolves the refer-
ence dynamically. Speciﬁcally, the linker looks up symbol table information
describing the routine to be called. It then patches, in a manner consistent
with the language’s subroutine calling conventions, the linkage tables that will
govern future calls.
810
Chapter 16 Run-Time Program Management
compiler-generated code (e.g., parameter-passing conventions, synchronization
mechanisms, and the layout of run-time stacks). The coupling between compiler
and runtime runs deeper than this, however: the CLI programming interface is
so complete as to fully hide the underlying hardware.1 Such a runtime is known
as a virtual machine. Some virtual machines—notably the Java Virtual Machine
(JVM)—are language-speciﬁc. Others, including the CLI, are explicitly intended
for use with multiple languages. In conjunction with developmentof their version
of the CLI,2 Microsoft introduced the term managed code to referto programs that
run on top of a virtual machine.
■
Virtual machines are part of a growing trend toward run-time management
and manipulation of programs using compiler technology. This trend is the sub-
ject of this chapter. We consider virtual machines in more detail in Section 16.1.
To avoid the overhead of emulating a non-native instruction set, many virtual ma-
chines use a just-in-time (JIT) compiler to translate their instruction set to that of
the underlying hardware. Some may even invoke the compiler after the program
is running, to compile newly discovered components or to optimize code based
on dynamically discovered properties of the program, its input, or the underly-
ing system. Using related technology, some language implementations perform
binary translation to retarget programs compiled for one machine to run on an-
other machine, or binary rewriting to instrument or optimize programs that have
already been compiled for the current machine. We consider these various forms
of late binding of machine code in Section 16.2. Finally, in Section 16.3, we con-
sider run-time mechanisms to inspect or modify the state of a running program.
Such mechanisms are needed by symbolic debuggers and by proﬁling and perfor-
mance analysis tools. They may also support reﬂection, which allows a program
to inspect and reason about its own state at run time.
16.1
Virtual Machines
A virtual machine (VM) provides a complete programming environment: its ap-
plication programming interface (API) includes everything required for correct
execution of the programs that run above it. We typically reserve use of the term
“VM” to environments whose level of abstraction is comparable to that of a com-
puter implemented in hardware. (A Smalltalk or Python interpreter, for example,
is usually not described as a virtual machine, because its level of abstraction is too
high, but this is a subjective call.)
Every virtual machine API includes an instruction set architecture (ISA) in
which to express programs. This may be the same as the instruction set of some
1
In particular, the CLI deﬁnes the instruction set of compiler’s target language: the Common
Intermediate Language (CIL) described in Section C 16.1.2.
2
CLI is an ECMA and ISO standard. CLR—the Common Language Runtime—is Microsoft’s im-
plementation of the CLI. It is part of the .NET framework.
16.1 Virtual Machines
811
existing physical machine, or it may be an artiﬁcial instruction set designed to
be easier to implement in software and to generate with a compiler. Other por-
tions of the VM API may support I/O, scheduling, or other services provided by
a library or by the operating system (OS) of a physical machine.
In practice, virtual machines tend to be characterized as either system VMs or
process VMs. A system VM faithfully emulates all the hardware facilities needed
to run a standard OS, including both privileged and unprivileged instructions,
memory-mapped I/O, virtual memory, and interrupt facilities. By contrast, a
process VM provides the environment needed by a single user-level process: the
unprivileged subset of the instruction set and a library-level interface to I/O and
other services.
System VMs are often managed by a virtual machine monitor (VMM) or hyper-
visor, which multiplexes a single physical machine among a collection of “guest”
operating systems, each of which runs in its own virtual machine. The ﬁrst widely
available VMM was IBM’s CP/CMS, which debuted in 1967. Rather than build an
operating system capable of supporting multiple users, IBM used the CP (“con-
trol program”) VMM to create a collection of virtual machines, each of which ran
a lightweight, single-user operating system (CMS). In recent years, VMMs have
played a central role in the rise of cloud computing, by allowing a hosting center
to share physical machines among a large number of (mutually isolated) guest
OSes. The center can monitor and manage its workload more easily if customer
workloads were running on bare hardware—it can even migrate running OSes
from one machine to another, to balance load among customers or to clear ma-
chines for hardware maintenance. System VMs are also increasingly popular on
personal computers, where products like VMware Fusion and Parallels Desktop
allow users to run programs on top of more than one OS at once.
It is process VMs, however, that have had the greatest impact on program-
ming language design and implementation. As with system VMs, the technology
is decades old: the P-code VM described in Example 1.15, for example, dates
from the early 1970s. Process VMs were originally conceived as a way to increase
program portability and to quickly “bootstrap” languages on new hardware. The
traditional downside was poor performance due to interpretation of the abstract
instruction set. The tradeoff between portability and performance remained valid
through the late 1990s, when early versions of Java were typically an order of mag-
nitude slower than traditionally compiled languages like Fortran or C. With the
introduction of just-in-time compilation, however, modern implementations of
the Java Virtual Machine (JVM) and the Common Language Infrastructure (CLI)
have come to rival the performance of traditional languages on native hardware.
We will consider these systems in Sections 16.1.1 and 16.1.2.
Both the JVM and the CLI use a stack-based intermediate form (IF): Java byte-
code and CLI Common Intermediate Language (CIL), respectively. As described
in Section 15.2.2, the lack of named operands means that stack-based IF can be
very compact—a feature of particular importance for code (e.g., applets) dis-
tributed over the Internet. At the same time, the need to compute everything in
stack order means that intermediate results cannot generally be saved in registers
812
Chapter 16 Run-Time Program Management
and reused. In many cases, stack-based code for an expression will occupy fewer
bytes, but specify more instructions, than corresponding code for a register-based
machine.
16.1.1 The Java Virtual Machine
Development of the language that eventually became Java began in 1990–1991,
when Patrick Naughton, James Gosling, and Mike Sheridan of Sun Microsystems
began work on a programming system for embedded devices. An early version of
this system was up and running in 1992, at which time the language was known
as Oak. In 1994, after unsuccessful attempts to break into the market for cable
TV set-top boxes, the project was retargeted to web browsers, and the name was
changed to Java.
The ﬁrst public release of Java occurred in 1995. At that time code in the
JVM was entirely interpreted. A JIT compiler was added in 1998, with the release
of Java 2. Though not standardized by any of the usual agencies (ANSI, ISO,
ECMA), Java is sufﬁciently well deﬁned to admit a wide variety of compilers and
JVMs. Oracle’s javac compiler and HotSpot JVM, released as open source in
2006, are by far the most widely used. The Jikes RVM (Research Virtual Machine)
is a self-hosting JVM, written in Java itself, and widely used for VM research.
Several companies have their own proprietary JVMs and class libraries, designed
to provide a competitive edge on particular machines or in particular markets.
Architecture Summary
The interface provided by the JVM was designed to be an attractive target for
a Java compiler. It provides direct support for all (and only) the built-in and
DESIGN & IMPLEMENTATION
16.2 Optimizing stack-based IF
As we shall see in Section C 16.1.2, code for the CLI was not intended for inter-
pretation; it is almost always JIT compiled. As a result, the extra instructions
sometimes needed to capture an expression in stack-based form are not a se-
rious problem: reasonably straightforward code improvement algorithms (to
be discussed in Chapter 17) allow the JIT compiler to transform the left side of
Figure 15.4 into good machine code at load time. In the judgment of the CLI
designers, the simplicity and compactness of the stack-based code outweigh
the cost of the code improvement. For Java, the need for compact mobile code
(e.g., browser applets) was a compelling advantage, even in early implementa-
tions that were interpreted rather than JIT compiled.
The higher level of abstraction of stack-based code also enhances portabil-
ity. Three-address instructions might be a good ﬁt for execution on SPARC
machines, but not on the x86 (a two-address machine).
16.1 Virtual Machines
813
reference types deﬁned by the Java language. It also enforces both deﬁnite as-
signment (Section 6.1.3) and type safety. Finally, it includes built-in support for
many of Java’s language features and standard library packages, including excep-
tions, threads, garbage collection, reﬂection, dynamic loading, and security.
Of course, nothing requires that Java bytecode be produced from Java source.
Compilers targeting the JVM exist for many other languages, including Ruby,
JavaScript, Python, and Scheme (all of which are traditionally interpreted), as well
as C, Ada, Cobol, and others, which are traditionally compiled.3 There are even
assemblers that allow programmers to write Java bytecode directly. The princi-
pal requirement, for both compilers and assemblers, is that they generate correct
class ﬁles. These have a special format understood by the JVM, and must satisfy a
variety of structural and semantic constraints.
At start-up time, a JVM is typically given the name of a class ﬁle containing
the static method main. It loads this class into memory, veriﬁes that it satisﬁes a
variety of required constraints, allocates any static ﬁelds, links it to any preloaded
library routines, and invokes any initialization code provided by the programmer
for classes or static ﬁelds. Finally, it calls main in a single thread. Additional
classes (needed by the initial class) may be loaded either immediately or lazily on
demand. Additional threads may be created via calls to the (built-in) methods of
class Thread. The three following subsections provide additional details on JVM
storage management, the format of class ﬁles, and the Java bytecode instruction
set.
Storage Management
Storage allocation mechanisms in the JVM mirror those of the Java language.
There is a global constant pool, a set of registers and a stack for each thread, a
method area to hold executable bytecode, and a heap for dynamically allocated
objects.
Global data
The method area is analogous to the code (“text”) segment of a tra-
ditional executable ﬁle, as described in Section 15.4. The constant pool contains
both program constants and a variety of symbol table information needed by the
JVM and other tools. Like the code of methods, the constant pool is read-only
to user programs. Each entry begins with a one-byte tag that indicates the kind
of information contained in the rest of the entry. Possibilities include the various
built-in types; character-string names; and class, method, and ﬁeld references.
Consider, for example, the trivial “Hello, world” program:
EXAMPLE 16.2
Constants for “Hello,
world”
3
Compilation of type-unsafe code, as in C, is problematic; we will return to this issue in Sec-
tion C 16.1.2.
814
Chapter 16 Run-Time Program Management
class Hello {
public static void main(String args[]) {
System.out.println("Hello, world!");
}
};
When compiled with OpenJDK’s javac compiler, the constant pool for this pro-
gram has 28 separate entries, shown in Figure 16.1. Entry 18 contains the text
of the output string; entry 3 indicates that this text is indeed a Java string. Many
of the additional entries (7, 11, 14, 21–24, 26, 27) give the textual names of ﬁles,
classes, methods, and ﬁelds. Others (9, 10, 13) are the names of structures else-
where in the class ﬁle; by pointing to these entries, the structures can be self-
descriptive. Four of the entries (8, 12, 25, 28) are type signatures for methods
and ﬁelds. In the format shown here, “V” indicates void; “Lname;” is a fully
qualiﬁed class. For methods, parentheses surround the list of argument types; the
return type follows. Most of the remaining entries are references to classes (5, 6,
16, 19), ﬁelds (2), and methods (1, 4). The ﬁnal three entries (15, 17, 20) give
name and type for ﬁelds and methods. The surprising amount of information for
such a tiny program stems from Java’s rich naming structure, the use of library
classes, and the deliberate retention of symbol table information to support lazy
linking, reﬂection, and debugging.
■
Per-thread data
A program running on the JVM begins with a single thread.
Additional threads are created by allocating and initializing a new object of the
build-in class Thread, and then calling its start method. Each thread has a small
set of base registers, a stack of method call frames, and an optional traditional
stack on which to call native (non-Java) methods.
Each frame on the method call stack contains an array of local variables, an
operand stack for evaluation of the method’s expressions, and a reference into the
constant pool that identiﬁes information needed for dynamic linking of called
methods. Space for formal parameters is included among the local variables.
Variables that are not live at the same time can share a slot in the array; this means
that the same slot may be used at different times for data of different types.
Because Java bytecode is stack oriented, operands and results of arithmetic
and logic instructions are kept in the operand stack of the current method frame,
rather than in registers. Implicitly, the JVM instruction set requires four registers
per thread, to hold the program counter and references to the current frame, the
top of the operand stack, and the base of the local variable array.
Slots in the local variable array and the operand stack are always 32 bits wide.
Data of smaller types are padded; long and double data take two slots each. The
maximum depth required for the operand stack can be determined statically by
the compiler, making it easy to preallocate space in the frame.
Heap
In keeping with the type system of the Java language, a datum in the local
variable array or the operand stack is always either a reference or a value of a
built-in scalar type. Structured data (objects and arrays) must always lie in the
16.1 Virtual Machines
815
const #1 = Method
#6.#15;
//
java/lang/Object."<init>":()V
const #2 = Field
#16.#17;
//
java/lang/System.out:Ljava/io/PrintStream;
const #3 = String
#18;
//
Hello, world!
const #4 = Method
#19.#20;
//
java/io/PrintStream.println:(Ljava/lang/String;)V
const #5 = class
#21;
//
Hello
const #6 = class
#22;
//
java/lang/Object
const #7 = Asciz
<init>;
const #8 = Asciz
()V;
const #9 = Asciz
Code;
const #10 = Asciz
LineNumberTable;
const #11 = Asciz
main;
const #12 = Asciz
([Ljava/lang/String;)V;
const #13 = Asciz
SourceFile;
const #14 = Asciz
Hello.java;
const #15 = NameAndType #7:#8;
//
"<init>":()V
const #16 = class
#23;
//
java/lang/System
const #17 = NameAndType #24:#25;
//
out:Ljava/io/PrintStream;
const #18 = Asciz
Hello, world!;
const #19 = class
#26;
//
java/io/PrintStream
const #20 = NameAndType #27:#28;
//
println:(Ljava/lang/String;)V
const #21 = Asciz
Hello;
const #22 = Asciz
java/lang/Object;
const #23 = Asciz
java/lang/System;
const #24 = Asciz
out;
const #25 = Asciz
Ljava/io/PrintStream;;
const #26 = Asciz
java/io/PrintStream;
const #27 = Asciz
println;
const #28 = Asciz
(Ljava/lang/String;)V;


![Figure 16.1 Content of...](images/page_848_caption_Figure%2016.1%20Content%20of%20the%20JVM%20constant%20pool%20for%20the%20program%20in%20Example%2016.2.%20The%20%E2%80%9CAsciz%E2%80%9D%20entries%20%28z.png)
*Figure 16.1 Content of the JVM constant pool for the program in Example 16.2. The “Asciz” entries (zero-terminated ASCII) contain null-terminated character-string names. Most other entries pair an indication of the kind of constant with a reference to one or more additional entries. This output was produced by Sun’s javap tool.*

heap. They are allocated, dynamically, using the new and newarray instructions.
They are reclaimed automatically via garbage collection. The choice of collection
algorithm is left to the implementor of the JVM.
To facilitate sharing among threads, the Java language provides the equivalent
of monitors with a lock and a single, implicit condition variable per object, as
described in Section 13.4.3. The JVM provides direct support for this style of
synchronization. Each object in the heap has an associated mutual exclusion lock;
in a typical implementation, the lock maintains a set of threads waiting for entry
to the monitor. In addition, each object has an associated set of threads that
are waiting for the monitor’s condition variable.4 Locks are acquired with the
monitorenter instruction and released with the monitorexit instruction. Most
4
To save space, a JIT compiler will typically omit the monitor information for any object it can
prove is never used for synchronization.
816
Chapter 16 Run-Time Program Management
JVMs insist that these calls appear in matching nested pairs, and that every lock
acquired within a given method be released within the same method (any correct
compiler for the Java language will follow these rules).
Consistency of access to shared objects is governed by the Java memory model,
which we considered brieﬂy in Section 13.3.3. Informally, each thread behaves as
if it kept a private cache of the heap. When a thread releases a monitor or writes a
volatile variable, the JVM must ensure that all previous updates to the thread’s
cache have been written back to memory. When a thread enters a monitor or
reads a volatile variable, the JVM must (in effect) clear the thread’s cache so
that subsequent reads cause locations to be reloaded from memory. Of course,
actual implementations don’t perform explicit write-backs or invalidations; they
start with the memory model provided by the hardware’s cache coherence pro-
tocol and use memory barrier (fence) instructions where needed to avoid unac-
ceptable orderings.
Class Files
Physically, a JVM class ﬁle is stored as a stream of bytes. Typically these occupy
some real ﬁle provided by the operating system, but they could just as easily be a
record in a database. On many systems, multiple class ﬁles may be combined into
a Java archive (.jar) ﬁle.
Logically, a class ﬁle has a well-deﬁned hierarchical structure. It begins with a
“magic number” (0x_cafe_babe), as described in Sidebar 14.4. This is followed
by
Major and minor version numbers of the JVM for which the ﬁle was created
The constant pool
Indices into the constant pool for the current class and its superclass
Tables describing the class’s superinterfaces, ﬁelds, and methods
Because the JVM is both cleaner and more abstract than a real machine, the
Java class ﬁle structure is both cleaner and more abstract than a typical object
ﬁle (Section 15.4). Conspicuously missing is the extensive relocation informa-
tion required to cope with the many ways that addresses are embedded into in-
structions on a typical real machine. In place of this, bytecode instructions in a
class ﬁle contain references to symbolic names in the constant pool. These be-
come references into the method area when code is dynamically linked. (Alter-
natively, they may become real machine addresses, appropriately encoded, when
the code is JIT compiled.) At the same time, class ﬁles contain extensive informa-
tion not typically found in an executable object ﬁle. Examples include access ﬂags
for classes, ﬁelds, and methods (public, private, protected, static, final,
synchronized, native, abstract, strictfp); symbol table information that
is built into the structure of the ﬁle (rather than an optional add-on); and special
instructions for such high-level notions as throwing an exception or entering or
leaving a monitor.
16.1 Virtual Machines
817
Bytecode
The bytecode for a method (or for a constructor or a class initializer) appears in
an entry in the class ﬁle’s method table. It is accompanied by the following:
An indication of the number of local variables, including parameters
The maximum depth required in the operand stack
A table of exception handler information, each entry of which indicates
– The bytecode range covered by this handler
– The address (index in the code) of the handler itself
– The type of exception caught (an index into the constant pool)
Optional information for debuggers: speciﬁcally, a table mapping bytecode
addresses to line numbers in the original source code and/or a table indicat-
ing which source code variable(s) occupy which JVM local variables at which
points in the bytecode.
Instruction Set
Java bytecode was designed to be both simple and compact.
Orthogonality was a strictly secondary concern. Every instruction begins with
a single-byte opcode. Arguments, if any, occupy subsequent bytes, with values
given in big-endian order. With two exceptions, used for switch statements, ar-
guments are unaligned, for compactness. Most instructions, however, actually
don’t need an argument. Where typical hardware performs arithmetic on val-
ues in named registers, bytecode pops arguments from, and pushes result to, the
operand stack of the current method frame. Moreover, even loads and stores can
often use a single byte. There are, for example, special one-byte integer store in-
structions for each of the ﬁrst four entries in the local variable array. Similarly,
there are special instructions to push the values −1, 0, 1, 2, 3, 4, and 5 onto the
operand stack.
As of Java 8, the JVM deﬁnes 205 of the 256 possible opcode values. Five of
these serve special purposes (unused, nop, debugger breakpoints, implementa-
tion dependent). The remainder can be organized into the following categories:
Load/store: move values back and forth between the operand stack and the local
variable array.
Arithmetic: perform integer or ﬂoating point operations on values in the
operand stack.
Type conversion: “widen” or “narrow” values among the built-in types (byte,
char, short, int, long, float, and double). Narrowing may result in a loss
of precision but never an exception.
Object management: create or query the properties of objects and arrays; access
ﬁelds and array elements.
Operand stack management: push and pop; duplicate; swap.
Control transfer: perform conditional, unconditional, or multiway branches
(switch).
818
Chapter 16 Run-Time Program Management
Method calls: call and return from ordinary and static methods (including
constructors and initializers) of classes and interfaces. An invokedynamic
instruction, introduced in the Java 7 JVM, allows run-time customization of
linkage conventions for individual call sites. It is used both for Java 8 lambda
expressions and for the implementation of dynamically typed languages on top
of the JVM.
Exceptions:
throw (no instructions required for catch).
Monitors: enter and exit (wait, notify, and notifyAll are invoked via method
calls).
As a concrete example, consider the following deﬁnitions for an integer set,
EXAMPLE 16.3
Bytecode for a list insert
operation
represented as a sorted linked list:
public class LLset {
node head;
class node {
public int val;
public node next;
}
public LLset() {
// constructor
head = new node();
// head node contains no real data
head.next = null;
}
...
}
An insert method for this class appears in Figure 16.2. Java source is on the
left; a symbolic representation of the corresponding bytecode is on the right. The
line at the top of the bytecode indicates a maximum depth of 3 for the operand
stack and four entries in the local variable array, the ﬁrst two of which are argu-
ments: the this pointer and the integer v. Perusal of the code reveals numerous
examples of the special one-byte load and store instructions, and of instructions
that operate implicitly on the operand stack.
■
Veriﬁcation
Safety was one of the principal concerns in the deﬁnition of the
Java language and virtual machine. Many of the things that can “go wrong”
while executing machine code compiled from a more traditional language can-
not go wrong when executing bytecode compiled from Java. Some aspects of
safety are obtained by limiting the expressiveness of the byte-code instruction
set or by checking properties at load time. One cannot jump to a nonexistent
address, for example, because method calls specify their targets symbolically by
name, and branch targets are speciﬁed as indices within the code attribute of the
current method. Similarly, where hardware allows displacement addressing from
the frame pointer to access memory outside the current stack frame, the JVM
checks at load time to make sure that references to local variables (speciﬁed by
constant indices into the local variable array) are within the bounds declared.
16.1 Virtual Machines
819
Code:
Stack=3, Locals=4, Args_size=2
0:
aload_0
// this
1:
getfield
#4; //Field head:LLLset$node;
4:
astore_2
5:
aload_2
// n
6:
getfield
#5; //Field LLset$node.next:LLLset$node;
9:
ifnull
31
// conditional branch
12:
aload_2
13:
getfield
#5; //Field LLset$node.next:LLLset$node;
16:
getfield
#6; //Field LLset$node.val:I
19:
iload_1
// v
20:
if_icmpge
31
23:
aload_2
24:
getfield
#5; //Field LLset$node.next:LLLset$node;
27:
astore_2
28:
goto
5
31:
aload_2
32:
getfield
#5; //Field LLset$node.next:LLLset$node;
35:
ifnull
49
38:
aload_2
39:
getfield
#5; //Field LLset$node.next:LLLset$node;
42:
getfield
#6; //Field LLset$node.val:I
45:
iload_1
46:
if_icmple
76
49:
new
#2; //class LLset$node
52:
dup
53:
aload_0
54:
invokespecial #3; //Method LLset$node."<init>":(LLLset;)V
57:
astore_3
58:
aload_3
// t
59:
iload_1
60:
putfield
#6; //Field LLset$node.val:I
63:
aload_3
64:
aload_2
65:
getfield
#5; //Field LLset$node.next:LLLset$node;
68:
putfield
#5; //Field LLset$node.next:LLLset$node;
71:
aload_2
72:
aload_3
73:
putfield
#5; //Field LLset$node.next:LLLset$node;
76:
return
public void insert(int v) {
node n = head;
while (n.next != null
&& n.next.val < v) {
n = n.next;
}
if (n.next == null
|| n.next.val > v) {
node t = new node();
t.val = v;
t.next = n.next;
n.next = t;
} // else v already in set
}


![Figure 16.2 Java source...](images/page_852_caption_Figure%2016.2%20Java%20source%20and%20bytecode%20for%20a%20list%20insertion%20method.%20Output%20on%20the%20right%20was%20produced%20b.png)
*Figure 16.2 Java source and bytecode for a list insertion method. Output on the right was produced by Oracle’s javac (compiler) and javap (disassembler) tools, with additional comments inserted by hand.*

820
Chapter 16 Run-Time Program Management
Other aspects of safety are guaranteed by the JVM during execution. Field
access and method call instructions throw an exception if given a null reference.
Similarly, array load and store instructions throw an exception if the index is not
within the bounds of the array.
When it ﬁrst loads a class ﬁle, the JVM checks the top-level structure of the ﬁle.
Among other things, it veriﬁes that the ﬁle begins with the appropriate “magic
number,” that the speciﬁed sizes of the various sections of the ﬁle are all within
bounds, and that these sizes add up to the size of the overall ﬁle. When it links
the class ﬁle into the rest of the program, the JVM checks additional constraints.
It veriﬁes that all items in the constant pool are well formed, and that nothing
inherits from a final class. More signiﬁcantly, it performs a host of checks on
the bytecode of the class’s methods. Among other things, the bytecode veriﬁer
ensures that every variable is initialized before it is read, that every operation
is type-safe, and that the operand stacks of methods never overﬂow or under-
ﬂow. All three of these checks require data ﬂow analysis to determine that desired
properties (initialization status, types of slots in the local stack frame, depth of
the operand stack) are the same on every possible path to a given point in the
program. We will consider data ﬂow in more detail in Section C 17.4.
DESIGN & IMPLEMENTATION
16.3 Veriﬁcation of class ﬁles and bytecode
Java compilers are required to generate code that satisﬁes all the constraints de-
ﬁned by the Java class ﬁle speciﬁcation. These include well-formedness of the
internal data structures, type safety, deﬁnite assignment, and lack of underﬂow
or overﬂow in the operand stack. A JVM, however, has no way to tell whether
a given class ﬁle was generated by a correct compiler. To protect itself from
potentially incorrect (or even malicious) class ﬁles, a JVM must verify that any
code it runs follows all the rules. Under normal operation, this means that cer-
tain checks (e.g., data ﬂow for deﬁnite assignment) are performed twice: once
by the Java compiler, to provide compile-time error messages to the program-
mer, and again by the JVM, to protect against buggy compilers or alternative
sources of bytecode.
To improve program start-up times and avoid unnecessary work, most
JVMs delay the loading (and veriﬁcation) of class ﬁles until some method in
that ﬁle is actually called (this is the Java equivalent of the lazy linking described
in Section C 15.7.2). In order to effect this delay, the JVM must wait until a call
occurs to verify the last few properties of the code at the call site (i.e., that it
refers to a method that really exists, and that the caller is allowed to call).
16.1.2 The Common Language Infrastructure
As early as the mid-1980s, Microsoft recognized the need for interoperability
among programming languages running on Windows platforms. In a series of
16.1 Virtual Machines
821
product offerings spanning a decade and a half, the company developed increas-
ingly sophisticated versions of its Component Object Model (COM), ﬁrst to com-
municate with, then to call, and ﬁnally to share data with program components
written in multiple languages.
With the success of Java, it became clear by the mid to late 1990s that a sys-
tem combining a JVM-style run-time system with the language interoperability
of COM could have enormous technical and commercial potential. Microsoft’s
.NET project set out to realize this potential.
It includes a JVM-like virtual
machine whose speciﬁcation—the Common Language Infrastructure (CLI)—is
standardized by ECMA and the ISO. While development of the CLI has clearly
been driven by Microsoft, other implementations—notablyfrom the open-source
Mono project, led by Xamarin, Inc.—are available for non-Windows platforms.
IN MORE DEPTH
We consider the CLI in more detail on the companion site. Among other things,
we describe the Common Type System, which governs cross-language interoper-
ability; the architecture of the virtual machine, including its support for generics;
the Common Intermediate Language (CIL—the CLI analogue of Java bytecode);
and Portable Executable (PE) assemblies, the CLI analogue of .jar ﬁles.
3CHECK YOUR UNDERSTANDING
1.
What is a run-time system? How does it differ from a “mere” library?
2.
List some of the major tasks that may be performed by a run-time system.
3.
What is a virtual machine? What distinguishes it from interpreters of other
sorts?
4.
Explain the distinction between system and process VMs. What other terms
are sometimes used for system VMs?
5.
What is managed code?
6.
Why do many virtual machines use a stack-based intermediate form?
7.
Give several examples of special-purpose instructions provided by Java byte-
code.
8.
Summarize the architecture of the Java Virtual Machine.
9.
Summarize the content of a Java class ﬁle.
10. Explain the validity checks performed on a class ﬁle at load time.
822
Chapter 16 Run-Time Program Management
16.2
Late Binding of Machine Code
In the traditional conception (Example 1.7), compilation is a one-time activity,
sharply distinguished from program execution. The compiler produces a tar-
get program, typically in machine language, which can subsequently be executed
many times for many different inputs.
In some environments, however, it makes sense to bring compilation and ex-
ecution closer together in time. A just-in-time (JIT) compiler translates a pro-
gram from source or intermediate form into machine language immediately be-
fore each separate run of the program. We consider JIT compilation further in the
ﬁrst subsection below. We also consider language systems that may compile new
pieces of a program—or recompile old pieces—after the program begins its exe-
cution. In Sections 16.2.2 and 16.2.3, we consider binary translation and binary
rewriting systems, which perform compiler-like operations on programs without
access to source code. Finally, in Section 16.2.4, we consider systems that may
download program components from remote locations. All these systems serve
to delay the binding of a program to its machine code.
16.2.1 Just-in-Time and Dynamic Compilation
To promote the Java language and virtual machine, Sun Microsystems coined the
slogan “write once, run anywhere”—the idea being that programs distributed
as Java bytecode could run on a very wide range of platforms. Source code, of
course, is also portable, but byte code is much more compact, and can be inter-
preted without additional preprocessing. Unfortunately, interpretation tends to
be expensive. Programs running on early Java implementations could be as much
as an order of magnitude slower than compiled code in other languages. Just-in-
time compilation is, to ﬁrst approximation, a technique to retain the portability of
bytecode while improving execution speed. Like both interpretation and dynamic
linking (Section 15.7), JIT compilation also beneﬁts from the delayed discovery of
program components: program code is not bloated by copies of widely shared li-
braries, and new versions of libraries are obtained automatically when a program
that needs them is run.
Because a JIT system compiles programs immediately prior to execution, it
can add signiﬁcant delay to program start-up time. Implementors face a difﬁcult
tradeoff: to maximize beneﬁts with respect to interpretation, the compiler should
produce good code; to minimize start-up time, it should produce that code very
quickly. In general, JIT compilers tend to focus on the simpler forms of target
code improvement. Speciﬁcally, they often limit themselves to the so-called local
improvements, which operate within individual control-ﬂow constructs. Im-
provements at the global (whole method) and interprocedural (whole program)
level may be expensive to consider.
16.2 Late Binding of Machine Code
823
Fortunately, the cost of JIT compilation is typically lessened by the existence
of an earlier source-to-byte-code compiler that does much of the “heavy lifting.”5
Scanning is unnecessary in a JIT compiler, since bytecode is not textual. Parsing
is trivial, since class ﬁles have a simple, self-descriptive structure. Many of the
properties that a source-to-byte-code compiler must infer at signiﬁcant expense
(type safety, agreement of actual and formal parameter lists) are embedded di-
rectly in the structure of the bytecode (objects are labeled with their type, calls are
made through method descriptors); others can be veriﬁed with simple data ﬂow
analysis. Certain forms of machine-independent code improvement may also be
performed by the source-to-byte-code compiler (these are limited to some degree
by stack-based expression evaluation).
All these factors allow a JIT compiler to be faster—and to produce better
code—than one might initially expect. In addition, since we are already com-
mitted to invoking the JIT compiler at run time, we can minimize its impact on
program start-up latency by running it a bit at a time, rather than all at once:
Like a lazy linker (Section C 15.7.2), a JIT compiler may perform its work in-
crementally. It begins by compiling only the class ﬁle that contains the program
entry point (i.e., main), leaving hooks in the code that call into the run-time
system wherever the program is supposed to call a method in another class ﬁle.
After this small amount of preparation, the program begins execution. When
execution falls into the runtime through an unresolved hook, the runtime in-
vokes the compiler to load the new class ﬁle and to link it into the program.
To eliminate the latency of compiling even the original class ﬁle, the language
implementation may incorporate both an interpreter and a JIT compiler. Exe-
cution begins in the interpreter. In parallel, the compiler translates portions of
the program into machine code. When the interpreter needs to call a method,
it checks to see whether a compiled version is available yet, and if so calls that
version instead of interpreting the bytecode. We will return to this technique
below, in the context of the HotSpot Java compiler and JVM.
When a class ﬁle is JIT compiled, the language implementation can cache the
resulting machine code for later use. This amounts to guessing, speculatively,
that the versions of library routines employed in the current run of the pro-
gram will still be current when the program is run again. Because languages
like Java and C# require the appearance of late binding of library routines, this
guess must be checked in each subsequent run. If the check succeeds, using a
cached copy saves almost the entire cost of JIT compilation.
Finally, JIT compilation affords the opportunity to perform certain kinds of
code improvement that are usually not feasible in traditional compilers. It is cus-
tomary, for example, for software vendors to ship a single compiled version of an
5
While a JIT compiler could, in principle, operate on source code, we assume throughout this
discussion that it works on a medium-level IF like Java bytecode or CIL.
824
Chapter 16 Run-Time Program Management
application for a given instruction set architecture, even though implementations
of that architecture may differ in important ways, including pipeline width and
depth; the number of physical (renaming) registers; and the number, size, and
speed of the various levels of cache. A JIT compiler may be able to identify the
processor implementation on which it is running, and generate code that is tuned
for that speciﬁc implementation. More important, a JIT compiler may be able
to in-line calls to dynamically linked library routines. This optimization is par-
ticularly important in object-oriented programs, which tend to call many small
methods. For such programs, dynamic in-lining can have a dramatic impact on
performance.
Dynamic Compilation
We have noted that a language implementation may choose to delay JIT compi-
lation to reduce the impact on program start-up latency. In some cases, compila-
tion must be delayed, either because the source or bytecode was not created or dis-
covered until run time, or because we wish to perform optimizations that depend
on information gathered during execution. In these cases, we say the language
implementation employs dynamic compilation. Common Lisp systems have used
dynamic compilation for many years: the language is typically compiled, but a
program can extend itself at run time. Optimization based on run-time statistics
is a more recent innovation.
Most programs spend most of their time in a relatively small fraction of the
code. Aggressive code improvement on this fraction can yield disproportion-
ately large improvements in program performance. A dynamic compiler can use
statistics gathered by run-time proﬁling to identify hot paths through the code,
which it then optimizes in the background. By rearranging the code to make
hot paths contiguous in memory, it may also improve the performance of the in-
struction cache. Additional run-time statistics may suggest opportunities to un-
roll loops (Exercise C 5.21), assign frequently used expressions to registers (Sec-
tions C 5.5.2 and C 17.8), and schedule instructions to minimize pipeline stalls
(Sections C 5.5.1 and C 17.6).
In some situations, a dynamic compiler may even be able to perform opti-
mizations that would be unsafe if implemented statically. Consider, for example,
EXAMPLE 16.4
When is in-lining safe?
the in-lining of methods from dynamically linked libraries. If foo is a static
method of class C, then calls to C.foo can safely be in-lined. Similarly, if bar
is a final method of class C (one that cannot be overridden), and o is an ob-
ject of class C, then calls to o.bar can safely be in-lined. But what if bar is not
final? The compiler can still in-line calls to o.bar if it can prove that o will
never refer to an instance of a class derived from C (which might have a different
implementation of bar). Sometimes this is easy:
C o = new C( args );
o.bar();
// no question what type this is
Other times it is not:
16.2 Late Binding of Machine Code
825
static void f(C o) {
o.bar();
}
Here the compiler can in-line the call only if it knows that f will never be passed
an instance of a class derived from C. A dynamic compiler can perform the op-
timization if it veriﬁes that there exists no class derived from C anywhere in the
(current version of the) program. It must keep notes of what it has done, how-
ever: if dynamic linking subsequently extends the program with code that deﬁnes
a new class D derived from C, the in-line optimization may need to be undone. ■
In some cases, a dynamic compiler may choose to perform optimizations that
may be unsafe even in the current program, provided that proﬁling suggests
they will be proﬁtable and run-time checks can determine whether they are safe.
Suppose, in the previous example, there already exists a class D derived from C,
EXAMPLE 16.5
Speculative optimization
but proﬁling indicates that every call to f so far has passed an instance of class C.
Suppose further that f makes many calls to methods of parameter o, not just the
one shown in the example. The compiler might choose to generate code along
the following lines:
static void f(C o) {
if (o.getClass() == C.class) {
... // code with in-lined calls -- much faster
} else {
... // code without in-lined calls
}
}
■
An Example System: the HotSpot Java Compiler
HotSpot is Oracle’s principal JVM and JIT compiler for desktop and server sys-
tems. It was ﬁrst released in 1999, and is available as open source.
HotSpot takes its name from its use of dynamic compilation to improve the
performance of hot code paths. Newly loaded class ﬁles are initially interpreted.
Methods that are executed frequently are selected by the JVM for compilation and
are subsequently patched into the program on the ﬂy. The compiler is aggressive
about in-lining small routines, and will do so in a deep, iterative fashion, repeat-
edly in-lining routines that are called from the code it just ﬁnished in-lining. As
described in the preceding discussion of dynamic compilation, the compiler will
also in-line routines that are safe only for the current set of class ﬁles, and will
dynamically “deoptimize” in-lined calls that have been rendered unsafe by the
loading of new derived classes.
The HotSpot compiler can be conﬁgured to operate in either “client” or
“server” mode. Client mode is optimized for lower start-up latency. It is ap-
propriate for systems in which a human user frequently starts new programs. It
translates Java bytecode to static single assignment (SSA) form (a medium-level
IF described in Section C 17.4.1) and performs a few straightforward machine-
independent optimizations. It then translates to a low-level IF, on which it per-
826
Chapter 16 Run-Time Program Management
forms instruction scheduling and register allocation. Finally, it translates this IF
to machine code.
Server mode is optimized to generate faster code. It is appropriate for systems
that need maximum throughput and can tolerate slower start-up. It applies most
classic global and interprocedural code improvement techniques to the SSA ver-
sion of the program (many of these are described in Chapter 17), as well as other
improvements speciﬁc to Java. Many of these improvements make use of proﬁling
statistics.
Particularly when running in server mode, HotSpot can rival the performance
of traditional compilers for C and C++. In effect, aggressive in-lining and proﬁle-
driven optimization serve to “buy back” both the start-up latency of JIT compi-
lation and the overhead of Java’s run-time semantic checks.
Other Example Systems
Like HotSpot, Microsoft’s CIL-to-machine-code compiler performs dynamic op-
EXAMPLE 16.6
Dynamic compilation in
the CLR
timization of hot code paths. The .NET source-to-CIL compilers are also ex-
plicitly available to programs through the System.CodeDom.Compiler API. A
program running on the CLR can directly invoke the compiler to translate C# (or
Visual Basic, or other .NET languages) into CIL PE assemblies. These can then
be loaded into the running program. As they are loaded, the CLR JIT compiler
translates them to machine code. As noted in Sections 3.6.4 and 11.2, C# includes
lambda expressions reminiscent of those in functional languages:
Func<int, int> square_func = x => x * x;
Here square_func is a function from integers to integers that multiplies its pa-
rameter (x) by itself, and returns the product. It is analogous to the following in
Scheme:
(let ((square-func (lambda (x) (* x x)))) ...
Given the C# declaration, we can write
y = square_func(3);
// 9
But just as Lisp allows a function to be represented as a list, so too does C# allow
a lambda expression to be represented as a syntax tree:
Expression<Func<int, int>> square_tree = x => x * x;
Various methods of library class Expression can now be used to explore and
manipulate the tree. When desired, the tree can be converted to CIL code:
square_func = square_tree.Compile();
These operations are roughly analogous to the following in Scheme:
16.2 Late Binding of Machine Code
827
(let* ((square-tree '(lambda (x) (* x x)))
; note the quote mark
(square-func (eval square-tree (scheme-report-environment 5))))
...
The difference in practice is that while Scheme’s eval checks the syntactic valid-
ity of the lambda expression and creates the metadata needed for dynamic type
checking, the typical implementation leaves the function in list (tree) form, and
interprets it when called. C#’s Compile is expected to produce CIL code; when
called it will be JIT compiled and directly executed.
■
Many Lisp dialects and implementations have employed an explicit mix of in-
terpretation and compilation. Common Lisp includes a compile function that
EXAMPLE 16.7
Dynamic compilation in
CMU Common Lisp
takes the name of an existing (interpretable) function as argument. As a side ef-
fect, it invokes the compiler on that function, after which the function will (pre-
sumably) run much faster:
(defun square (x) (* x x))
; outermost level function declaration
(square 3)
; 9
(compile 'square)
(square 3)
; also 9 (but faster :-)
CMU Common Lisp, a widely used open-source implementation of the lan-
guage, incorporates two interpreters and a compiler with two back ends. The so-
called “baby” interpreter understands a subset of the language, but works stand-
alone. It handles simple expressions at the read-eval-print loop, and is used to
bootstrap the system. The “grown-up” interpreter understands the whole lan-
guage, but needs access to the front end of the compiler. The bytecode compiler
translates source code to an intermediate form reminiscent of Java bytecode or
CIL. The native code compiler translates to machine code. In general, programs
are run by the “grown-up” interpreter unless the programmer invokes the com-
piler explicitly from the command line or from within a Common Lisp program
(with compile). The compiler, when invoked, produces native code unless other-
wise instructed. Documentation indicates that the bytecode version of the com-
piler runs twice as fast as the native code version. Bytecode is portable and 6×
denser than native code. It runs 50× slower than native code, but 10× faster than
the interpreter.
■
Like most scripting languages, Perl 5 compiles its input to an internal syntax
EXAMPLE 16.8
Compilation of Perl
tree format, which it then interprets. In several cases, the interpreter may need
to call back into the compiler during execution. Features that force such dynamic
compilation include eval, which compiles and then interprets a string; require,
which loads a library package; and the ee version of the substitution command,
which performs expression evaluation on the replacement string:
$foo = "abc";
$foo =~ s/b/2 + 3/ee;
# replace b with the value of 2 + 3
print "$foo\n";
# prints a5c
828
Chapter 16 Run-Time Program Management
Perl can also be directed, via library calls or the perlcc command-line script
(itself written in Perl), to translate source code to either bytecode or machine
code.
In the former case, the output is an “executable” ﬁle beginning with
#! /usr/bin/perl (see the Sidebar 14.4 for a discussion of the #! convention).
If invoked from the shell, this ﬁle will feed itself back into Perl 5, which will notice
that the rest of the ﬁle contains bytecode instead of source, and will perform a
quick reconstruction of the syntax tree, ready for interpretation.
If directed to produce machine code, perlcc generates a C program, which it
then runs through the C compiler. The C program builds an appropriate syntax
tree and passes it directly to the Perl interpreter, bypassing both the compiler
and the byte-code-to-syntax-tree reconstruction. Both the bytecode and machine
code back ends are considered experimental; they do not work for all programs.
Perl 6, still under development as of 2015, is intended to be JIT compiled. Its
virtual machine, called Parrot, is unusual in providing a large register set, rather
than a stack, for expression evaluation. Like Perl itself—but unlike the JVM and
CLR—Parrot allows variables to be treated as different types in different contexts.
Work is underway to target other scripting languages to Parrot, with the eventual
goal or providing interoperability similar to that of the .NET languages.
■
3CHECK YOUR UNDERSTANDING
11. What is a just-in-time (JIT) compiler? What are its potential advantages over
interpretation or conventional compilation?
12. Why might one prefer bytecode over source code as the input to a JIT com-
piler?
13. What distinguishes dynamic compilation from just-in-time compilation?
14. What is a hot path? Why is it signiﬁcant?
15. Underwhat circumstances can a JIT compiler expand virtual methods in-line?
16. What is deoptimization? When and why is it needed?
17. Explain the distinction between the function and expression tree representa-
tions of a lambda expression in C#.
18. Summarize the relationship between compilation and interpretation in Perl.
16.2.2 Binary Translation
Just-in-time and dynamic compilers assume the availability of source code or of
bytecode that retains all of the semantic information of the source. There are
times, however, when it can be useful to recompile object code. This process is
known as binary translation. It allows already-compiled programs to be run on
a machine with a different instruction set architecture. Some readers may recall
16.2 Late Binding of Machine Code
829
Apple’s Rosetta system, which allowed programs compiled for older PowerPC-
based Macintosh computers to run on newer x86-based Macs. Rosetta built on
experience with a long line of similar translators.
The principal challenge for binary translation is the loss of information in the
original source-to-object-code translation. Object code typically lacks both type
information and the clearly delineated subroutines and control-ﬂow constructs
of source code and bytecode. While most of this information appears in the com-
piler’s symbol table, and may sometimes be included in the object ﬁle for debug-
ging purposes, vendors usually delete it before shipping commercial products,
and a binary translator cannot assume it will be present.
The typical binary translator reads an object ﬁle and reconstructs a control
ﬂow graph of the sort described in Section 15.1.1. This task is complicated by
the lack of explicit information about basic blocks. While branches (the ends
of basic blocks) are easy to identify, beginnings are more difﬁcult: since branch
targets are sometimes computed at run time or looked up in dispatch tables or
virtual function tables, the binary translator must consider the possibility that
control may sometimes jump into the middle of a “probably basic” block. Since
translated code will generally not lie at the same address as the original code,
computed branches must be translated into code that performs some sort of table
lookup, or falls back on interpretation.
Static binary translation is not always possible for arbitrary object code. In ad-
dition to computed branches, problems include self-modifying code (programs
that write to their own instruction space), dynamically generated code (e.g., for
single-pointer closures, as described in Example C 9.61), and various forms of in-
trospection, in which a program examines and reasons about its own state (we
will consider this more fully in Section 16.3). Fortunately, many common id-
ioms can be identiﬁed and treated as special cases, and for the (comparatively
rare) cases that can’t be handled statically, a binary translator can always delay
some translation until run time, fall back on interpretation, or simply inform the
user that translation is not possible. In practice, binary translation has proved
remarkably successful.
Where and When to Translate
Most binary translators operate in user space, and limit themselves to the non-
privileged subset of the machine’s instruction set. A few are built at a lower level.
When Apple converted from the Motorola 680x0 processor to the PowerPC in
EXAMPLE 16.9
The Mac 68K emulator
1994, they built a 68K interpreter into the operating system. A subsequent re-
lease the following year augmented the interpreter with a rudimentary binary
translator that would cache frequently executed instruction sequences in a small
(256KB) buffer. By placing the interpreter (emulator) in the lowest levels of the
operating system, Apple was able to signiﬁcantly reduce its time to market: only
the most performance-critical portions of the OS were rewritten for the PowerPC,
leaving the rest as 68K code. Additional portions were rewritten over time.
■
In the late 1990s, Transmeta Corp. developed an unusual system capable of
EXAMPLE 16.10
The Transmeta Crusoe
processor
running unmodiﬁed x86 operating systems and applications by means of binary
830
Chapter 16 Run-Time Program Management
translation. Their Crusoe and Efﬁceon processors, sold from 2000 to 2005, ran
proprietary “Code Morphing” software directly on top of a wide-instruction-
word ISA (distantly related to the Itanium). This software, designed in conjunc-
tion with the hardware, translated x86 code to native code on the ﬂy, and was
entirely invisible to systems running above it.
■
Binary translators display even more diversity in their choice of what and when
to translate. In the simplest case, translation is a one-time, off-line activity akin to
conventional compilation. In the late 1980s, for example, Hewlett Packard Corp.
EXAMPLE 16.11
Static binary translation
developed a binary translator to retarget programs from their “Classic” HP 3000
line to the PA-RISC processor. The translator depended on the lack of dynamic
linking in the operating system: all pieces of the to-be-translated program could
be found in a single executable.
■
In a somewhat more ambitious vein, Digital Equipment Corp. (DEC) in the
EXAMPLE 16.12
Dynamic binary translation
early 1990s constructed a pair of translators for their newly developed Alpha pro-
cessor: one (mx) to translate Unix programs originally compiled for MIPS-based
workstations, the other (VEST) to translate VMS programs originally compiled
for the VAX. Because VMS supported an early form of shared libraries, it was
not generally possible to statically identify all the pieces of a program. VEST
and mx were therefore designed as “open-ended” systems that could intervene, at
run time, to translate newly loaded libraries. Like the HP system, DEC’s transla-
tors saved new, translated versions of their applications to disk, for use in future
runs.
■
In a subsequent project in the mid-1990s, DEC developed a system to execute
EXAMPLE 16.13
Mixed interpretation and
translation
shrink-wrapped Windows software on Alpha processors. (Early versions of Mi-
crosoft’s Windows NT operating system were available for both the x86 and the
DESIGN & IMPLEMENTATION
16.4 Emulation and interpretation
While the terms interpretation and emulation are often used together, the con-
cepts are distinct. Interpretation, as we have seen, is a language implemen-
tation technique: an interpreter is a program capable of executing programs
written in the to-be-implemented language. Emulation is an end goal: faith-
fully imitating the behavior of some existing system (typically a processor or
processor/OS pair) on some other sort of system. An emulator may use an in-
terpreter to execute the emulated processor’s instruction set. Alternatively, it
may use binary translation, special hardware (e.g., a ﬁeld-programmable gate
array—FPGA), or some combination of these.
Emulation and interpretation are also distinct from simulation. A simulator
models some complex system by capturing “important” behavior and ignor-
ing “unimportant” detail. Meteorologists, for example, simulate the Earth’s
weather systems, but they do not emulate them. An emulator is generally con-
sidered correct if it does exactly what the original system does. For a simulator,
one needs some notion of accuracy: how close is close enough?
16.2 Late Binding of Machine Code
831
Alpha, but most commercial software came in x86-only versions.) DEC’s FX!32
included both a binary translator and a highly optimized interpreter. When the
user tried to run an x86 executable, FX!32 would ﬁrst interpret it, collecting usage
statistics. Later, in the background, it would translate hot code paths to native
code, and store them in a database. Once translated code was available (later in
the same execution or during future runs), the interpreter would run it in lieu of
the original.
■
Modern emulation systems typically take an intermediate approach. A fast,
EXAMPLE 16.14
Transparent dynamic
translation
simple translator creates native versions of basic blocks or subroutines on de-
mand, and caches them for repeated use within a given execution. For the sake of
transparency (to avoid modiﬁcation of programs on disk), and to accommodate
dynamic linking, translated code is usually not retained from one execution to
the next. Systems in this style include Apple’s Rosetta; HP’s Aries, which retar-
gets PA-RISC code to the Itanium; and Intel’s IA-32 EL, which retargets x86 code
to the Itanium.
■
Among the most widely used emulators today is the open-source QEMU
EXAMPLE 16.15
Translation and
virtualization
(quick emulation) system. In the language of Section 16.1, QEMU is a system
virtual machine. Like other system VMs, it runs as a user-level process that emu-
lates bare hardware on which to run a guest operating system (and that system’s
workload). Unlike most system VMs, QEMU can emulate hardware very different
from that of the underlying physical machine. For the sake of good performance,
it makes heavy use of binary translation, converting large blocks of guest system
instructions into equivalent blocks of native instructions.
■
Dynamic Optimization
In a long-running program, a dynamic translator may revisit hot paths and opti-
mize them more aggressively. A similar strategy can also be applied to programs
that don’t need translation—that is, to programs that already exist as machine
code for the underlying architecture. This sort of dynamic optimization has been
reported to improve performance by as much as 20% over already-optimized
code, by exploiting run-time proﬁling information.
Much of the technology of dynamic optimization was pioneered by the Dy-
EXAMPLE 16.16
The Dynamo dynamic
optimizer
namo project at HP Labs in the late 1990s. Dynamo was designed to transparently
enhance the performance of applications for the PA-RISC instruction set. A sub-
sequent version, DynamoRIO, was written for the x86. Dynamo’s key innovation
was the concept of a partial execution trace: a hot path whose basic blocks can be
reorganized, optimized, and cached as a linear sequence.
An example of such a trace appears in Figure 16.3. Procedure print matching
takes a set and a predicate as argument, and prints all elements of the set that
match the predicate. At run time, Dynamo may discover that the procedure is
frequently called with a particular predicate p that is almost never true. The hot
path through the ﬂow graph (left side of the ﬁgure) can then be turned into the
trace at the right. If print matching is sometimes called with a different predi-
cate p, it will use a separate copy of the code. Branches out of the trace (in the
832
Chapter 16 Run-Time Program Management
procedure print_matching(S : set, p : predicate)
       foreach e in S
              if p(e)
                     print e
prologue
loop head
loop head
test1
p1
test1
p1
test2
pN
pN
print1
test2
printN
loop foot
loop foot
epilogue


![Figure 16.3 Creation of...](images/page_865_caption_Figure%2016.3%20Creation%20of%20a%20partial%20execution%20trace.%20Procedure%20print%20matching%20%28shown%20at%20top%29%20is%20often%20.png)
*Figure 16.3 Creation of a partial execution trace. Procedure print matching (shown at top) is often called with a particular predicate, p, which is usually false. The control ﬂow graph (left, with hot blocks in bold and the hot path in grey) can be reorganized at run time to improve instruction-cache locality and to optimize across abstraction boundaries (right).*

loop-termination and predicate-checking tests) jump either to other traces or, if
appropriate ones have not yet been created, back into Dynamo.
By identifying and optimizing traces, Dynamo is able to signiﬁcantly improve
locality in the instruction cache, and to apply standard code improvement tech-
niques across the boundaries between separately compiled modules and dynam-
ically loaded libraries. In Figure 16.3, for example, it will perform register allo-
cation jointly across print matchings and the predicate p. It can even perform
instruction scheduling across basic blocks if it inserts appropriate compensating
code on branches out of the trace. An instruction in block test2, for example,
can be moved into the loop footer if a copy is placed on the branch to the right.
Traces have proved to be a very powerful technique. They are used not only by dy-
namic optimizers, but by dynamic translators like Rosetta as well, and by binary
instrumentation tools like Pin (to be discussed in Section 16.2.3).
■
16.2 Late Binding of Machine Code
833
16.2.3 Binary Rewriting
While the goal of a binary optimizer is to improve the performance of a program
without altering its behavior, one can also imagine tools designed to change that
behavior. Binary rewriting is a general technique to modify existing executable
programs, typically to insert instrumentation of some kind. The most common
form of instrumentation collects proﬁling information. One might count the
number of times that each subroutine is called, for example, or the number of
times that each loop iterates (Exercise 16.5). Such counts can be stored in a buffer
in memory, and dumped at the end of execution. Alternatively, one might log
all memory references. Such a log will generally need to be sent to a ﬁle as the
program runs—it will be too long to ﬁt in memory.
In addition to proﬁling, binary rewriting can be used to
Simulate new architectures: operations of interest to the simulator are replaced
with code that jumps into a special run-time library (other code runs at native
speed).
Evaluate the coverage of test suites, by identifying paths through the code that
are not explored by a series of tests.
Implement model checking for parallel programs, a process that exposes race
conditions (Example 13.2) by forcing a program through different interleav-
ings of operations in different threads.
“Audit” the quality of a compiler’s optimizations. For example, one might
check whether the value loaded into a register is always the same as the value
that was already there (such loads suggest that the compiler may have failed to
realize that the load was redundant).
Insert dynamic semantic checks into a program that lacks them. Binary rewrit-
ing can be used not only for simple checks like null-pointer dereference and
arithmetic overﬂow, but for a wide variety of memory access errors as well,
including uninitialized variables, dangling references, memory leaks, “double
deletes” (attempts to deallocate an already deallocated block of memory), and
access off the ends of dynamically allocated arrays.
More ambitiously, as described in Sidebar 16.5, binary rewriting can be used to
“sandbox” untrusted code so that it can safely be executed in the same address
space as the rest of the application.
Many of the techniques used by rewriting tools were pioneered by the ATOM
EXAMPLE 16.17
The ATOM binary rewriter
binary rewriter for the Alpha processor. Developed by researchers at DEC’s West-
ern Research Lab in the early 1990s, ATOM was a static tool that modiﬁed a pro-
gram for subsequent execution.
To use ATOM, a programmer would write instrumentation and analysis sub-
routines in C. Instrumentation routines would be called by ATOM during the
rewriting process. By calling back into ATOM, these routines could arrange for
the rewritten application to call analysis routines at instructions, basic blocks,
subroutines, or control ﬂow edges of the programmer’s choosing. To make room
834
Chapter 16 Run-Time Program Management
for inserted calls, ATOM would move original instructions of the instrumented
program; to facilitate such movement, the program had to be provided as a set of
relocatable modules. No other changes were made to the instrumented program;
in particular, data addresses were always left unchanged.
■
An Example System: the Pin Binary Rewriter
For modern processors, ATOM has been supplanted by Pin, a binary rewriter de-
veloped by researchers at Intel in the early 2000s, and distributed as open source.
Designed to be largely machine independent, Pin is available not only for the x86,
x86-64, and Itanium, but also for ARM.
Pin was directly inspired by ATOM, and has a similar programming interface.
In particular, it retains the notions of instrumentation and analysis routines. It
also borrows ideas from Dynamo and other dynamic translation tools. Most sig-
niﬁcantly, it uses an extended version of Dynamo’s trace mechanism to instru-
ment previously unmodiﬁed programs at run time; the on-disk representation
of the program never changes. Pin can even be attached to an already-running
application, much like the symbolic debuggers we will study in Section 16.3.2.
Like Dynamo, Pin begins by writing an initial trace of basic blocks into a run-
time trace cache. It ends the trace when it reaches an unconditional branch, a
predeﬁned maximum number of conditional branches, or a predeﬁned maxi-
mum number of instructions. As it writes, it inserts calls to analysis routines (or
in-line versions of short routines) at appropriate places in the code. It also main-
tains a mapping between original program addresses and addresses in the trace,
so it can modify address-speciﬁc instructions accordingly. Once it has ﬁnished
creating a trace, Pin simply jumps to its ﬁrst instruction. Conditional branches
that exit the trace are set to link to other traces, or to jump back into Pin.
Indirect branches are handled with particular care. Based on run-time pro-
ﬁling, Pin maintains a set of predictions for the targets of such branches, sorted
most likely ﬁrst. Each prediction consists of an address in the original program
(which serves as a key) and an address to jump to in the trace cache. If none
of the predictions match, Pin falls back to table lookup in its mapping between
original and trace cache addresses. If match is still not found, Pin falls back on an
instruction set interpreter, allowing it to handle even dynamically generated code.
To reduce the need to save registers when calling analysis routines, and to facil-
itate in-line expansion of those routines, Pin performs its own register allocation
for the instructions of each trace, using similar allocations whenever possible for
traces that link to one another. In multithreaded programs, one register is stat-
ically reserved to point to a thread-speciﬁc buffer, where registers can be spilled
when necessary. Condition codes are not saved across calls to analysis routines
unless their values are needed afterward. For routines that can be called any-
where within a basic block, Pin hunts for a location where the cost of saving and
restoring is minimized.
16.2 Late Binding of Machine Code
835
16.2.4 Mobile Code and Sandboxing
Portability is one of the principal motivations for late binding of machine code.
Code that has been compiled for one machine architecture or operating system
cannot generally be run on another. Code in a byte code (Java bytecode, CIL) or
scripting language (JavaScript, Visual Basic), however, is compact and machine
independent: it can easily be moved over the Internet and run on almost any
platform. Such mobile code is increasingly common. Every major browser sup-
ports JavaScript; most enable the execution of Java applets as well. Visual Basic
macros are commonly embedded not only in pages meant for viewing with In-
ternet Explorer, but also in Excel, Word, and Outlook documents distributed via
email. Cell phone apps may use mobile code to distribute games, productivity
tools, and interactive media that run within an existing process.
In some sense, mobile code is nothing new: almost all our software comes from
other sources; we download it over the Internet or perhaps install it from a DVD.
Historically, this usage model has relied on trust (we assume that software from a
well-known company will be safe) and on the very explicit and occasional nature
of installation. What has changed in recent years is the desire to download code
frequently, from potentially untrusted sources, and often without the conscious
awareness of the user.
Mobile code carries a variety of risks. It may access and reveal conﬁdential
information (spyware). It may interfere with normal use of the computer in an-
noying ways (adware). It may damage existing programs or data, or save copies of
itself that run without the user’s intent (malware of various kinds). In particular
DESIGN & IMPLEMENTATION
16.5 Creating a sandbox via binary rewriting
Binary rewriting provides an attractive means to implement a sandbox. While
there is in general no way to ensure that code does what it is supposed to do
(one is seldom sure of that even with one’s own code), a binary rewriter can
Verify the address of every load and store, to make sure untrusted code ac-
cesses only its own data, and to avoid alignment faults
Similarly verify every branch and call, to prevent control from leaving the
sandbox by any means other than returning
Verify all opcodes, to prevent illegal instruction faults
Double-check the parameters to any arithmetic instruction that may gener-
ate a fault
Audit (or forbid) all system calls
Instrument backward jumps to limit the amount of time that untrusted
code can run (and in particular to preclude any inﬁnite loops)
836
Chapter 16 Run-Time Program Management
egregious cases, it may use the host machine as a “zombie” from which to launch
attacks on other users.
To protect against unwanted behavior, both accidental and malicious, mobile
code must be executed in some sort of sandbox, as described in Sidebar 14.6.
Sandbox creation is difﬁcult because of the variety of resources that must be pro-
tected. At a minimum, one needs to monitor or limit access to processor cycles,
memory outside the code’s own instructions and data, the ﬁle system, network
interfaces, other devices (passwords, for example, may be stolen by snooping the
keyboard), the window system (e.g., to disable pop-up ads), and any other poten-
tially dangerous services provided by the operating system.
Sandboxing mechanisms lie at the boundary between language implementa-
tion and operating systems. Traditionally, OS-provided virtual memory tech-
niques might be used to limit access to memory, but this is generally too expensive
for many forms of mobile code. The two most common techniques today—both
of which rely on technology discussed in this chapter—are binary rewriting and
execution in an untrusting interpreter. Both cases are complicated by an inherent
tension between safety and utility: the less we allow untrusted code to do, the less
useful it can be. No single policy is likely to work in all cases. Applets may be
entirely safe if all they can do is manipulate the image in a window, but macros
embedded in a spreadsheet may not be able to do their job without changing the
user’s data. A major challenge for future work is to ﬁnd a way to help users—
who cannot be expected to understand the technical details—to make informed
decisions about what and what not to allow in mobile code.
3CHECK YOUR UNDERSTANDING
19. What is binary translation? When and why is it needed?
20. Explain the tradeoffs between static and dynamic binary translation.
21. What is emulation? How is it related to interpretation and simulation?
22. What is dynamic optimization? How can it improve on static optimization?
23. What is binary rewriting? How does it differ from binary translation and dy-
namic optimization?
24. Describe the notion of a partial execution trace. Why is it important to dy-
namic optimization and rewriting?
25. What is mobile code?
26. What is sandboxing? When and why is it needed? How can it be implemented?
16.3 Inspection/Introspection
837
16.3
Inspection/Introspection
Symbol table metadata makes it easy for utility programs—just-in-time and dy-
namic compilers, optimizers, debuggers, proﬁlers, and binary rewriters—to in-
spect a program and reason about its structure and types. We consider debuggers
and proﬁlers in particular in Sections 16.3.2 and 16.3.3. There is no reason, how-
ever, why the use of metadata should be limited to outside tools, and indeed it is
not: Lisp has long allowed a program to reason about its own internal structure
and types (this sort of reasoning is sometimes called introspection). Java and C#
provide similar functionality through a reﬂection API that allows a program to
peruse its own metadata. Reﬂection appears in several other languages as well,
including Prolog (Sidebar 12.2) and all the major scripting languages. In a dy-
namically typed language such as Lisp, reﬂection is essential: it allows a library
or application function to type check its own arguments. In a statically typed
language, reﬂection supports a variety of programming idioms that were not tra-
ditionally feasible.
16.3.1 Reﬂection
Trivially, reﬂection can be useful when printing diagnostics. Suppose we are try-
EXAMPLE 16.18
Finding the concrete type
of a reference variable
ing to debug an old-style (nongeneric) queue in Java, and we want to trace the
objects that move through it. In the dequeue method, just before returning an
object rtn of type Object, we might write
System.out.println("Dequeued a " + rtn.getClass().getName());
If the dequeued object is a boxed int, we will see
Dequeued a java.lang.Integer
■
More signiﬁcantly, reﬂection is useful in programs that manipulate other pro-
grams. Most program development environments, for example, have mecha-
nisms to organize and “pretty-print” the classes, methods, and variables of a pro-
gram. In a language with reﬂection, these tools have no need to examine source
code: if they load the already-compiled program into their own address space,
they can use the reﬂection API to query the symbol table information created by
the compiler. Interpreters, debuggers, and proﬁlers can work in a similar fashion.
In a distributed system, a program can use reﬂection to create a general-purpose
serialization mechanism, capable of transforming an almost arbitrary structure
into a linear stream of bytes that can be sent over a network and reassembled at
the other end. (Both Java and C# include such mechanisms in their standard li-
brary, implemented on top of the basic language.) In the increasingly dynamic
838
Chapter 16 Run-Time Program Management
world of Internet applications, one can even create conventions by which a pro-
gram can “query” a newly discovered object to see what methods it implements,
and then choose which of these to call.
There are dangers, of course, associated with the undisciplined use of reﬂec-
tion. Because it allows an application to peek inside the implementation of a class
(e.g., to list its private members), reﬂection violates the normal rules of abstrac-
tion and information hiding. It may be disabled by some security policies (e.g., in
sandboxed environments). By limiting the extent to which target code can differ
from the source, it may preclude certain forms of code improvement.
Perhaps the most common pitfall of reﬂection, at least for object-oriented lan-
EXAMPLE 16.19
What not to do with
reﬂection
guages, is the temptation to write case (switch) statements driven by type in-
formation:
procedure rotate(s : shape)
case shape.type of
–– don’t do this in Java!
square: rotate square(s)
triangle: rotate triangle(s)
circle:
–– no-op
. . .
While this kind of code is common (and appropriate) in Lisp, in an object-
oriented language it is much better written with subtype polymorphism:
s.rotate()
–– virtual method call
■
Java Reﬂection
Java’s root class, Object, supports a getClass method that returns an instance
of java.lang.Class. Objects of this class in turn support a large number of re-
ﬂection operations, among them the getName method we used in Example 16.18.
A call to getName returns the fully qualiﬁed name of the class, as it is embedded
EXAMPLE 16.20
Java class-naming
conventions
in the package hierarchy. For array types, naming conventions are taken from the
JVM:
int[] A = new int[10];
System.out.println(A.getClass().getName());
// prints "[I"
String[] C = new String[10];
System.out.println(C.getClass().getName());
// "[Ljava.lang.String;"
Foo[][] D = new Foo[10][10];
System.out.println(D.getClass().getName());
// "[[LFoo;"
Here Foo is assumed to be a user-deﬁned class in the default (outermost) package.
A left square bracket indicates an array type; it is followed by the array’s element
type. The built-in types (e.g., int) are represented in this context by single-letter
names (e.g., I). User-deﬁned types are indicated by an L, followed by the fully
qualiﬁed class name and terminated by a semicolon. Notice the similarity of the
second example (C) to entry #12 in the constant pool of Figure 16.1: that entry
16.3 Inspection/Introspection
839
gives the parameter types (in parentheses) and return type (V means void) of
main. As every Java programmer knows, main expects an array of strings.
■
A call to o.getClass() returns information on the concrete type of the object
referred to by o, not on the abstract type of the reference o. If we want a Class
EXAMPLE 16.21
Getting information on a
particular class
object for a particular type, we can create a dummy object of that type:
Object o = new Object();
System.out.println(o.getClass().getName());
// "java.lang.Object"
Alternatively, we can append the pseudo ﬁeld name .class to the name of the
type itself:
System.out.println(Object.class.getName());
// "java.lang.Object"
In the reverse direction, we can use static method forName of class Class to
obtain a Class object for a type with a given (fully qualiﬁed) character string
name:
Class stringClass = Class.forName("java.lang.String");
Class intArrayClass = Class.forName("[I");
Method forName works only for reference types. For built-ins, one can either use
the .class syntax or the .TYPE ﬁeld of one of the standard wrapper classes:
Class intClass = Integer.TYPE;
■
Given a Class object c, one can call c.getSuperclass() to obtain a Class
object for c’s parent. In a similar vein, c.getClasses() will return an array of
Class objects, one for each public class declared within c’s class. Perhaps more
interesting, c.getMethods(), c.getFields(), and c.getConstructors() will
return arrays of objects representing all c’s public methods, ﬁelds, and construc-
tors (including those inherited from ancestor classes). The elements of these
arrays are instances of classes Method, Field, and Constructor, respectively.
These are declared in package java.lang.reflect, and serve roles analogous to
that of Class. The many methods of these classes allow one to query almost any
aspect of the Java type system, including modiﬁers (static, private, final,
abstract, etc.), type parameters of generics (but not of generic instances—those
are erased), interfaces implemented by classes, exceptions thrown by methods,
and much more. Perhaps the most conspicuous thing that is not available through
the Java reﬂection API is the bytecode that implements methods. Even this, how-
ever, can be examined using third-party tools such as the Apache Byte Code En-
gineering Library (BCEL) or ObjectWeb’s ASM, both of which are open source.
Figure 16.4 shows Java code to list the methods declared in (but not inherited
EXAMPLE 16.22
Listing the methods of a
Java class
by) a given class. Also shown is output for AccessibleObject, the parent class
of Method, Field, and Constructor. (The primary purpose of this class is to
840
Chapter 16 Run-Time Program Management
import static java.lang.System.out;
public static void listMethods(String s)
throws java.lang.ClassNotFoundException {
Class c = Class.forName(s);
// throws if class not found
for (Method m : c.getDeclaredMethods()) {
out.print(Modifier.toString(m.getModifiers()) + " ");
out.print(m.getReturnType().getName() + " ");
out.print(m.getName() + "(");
boolean first = true;
for (Class p : m.getParameterTypes()) {
if (!first) out.print(", ");
first = false;
out.print(p.getName());
}
out.println(") ");
}
}
Sample output for listMethods("java.lang.reflect.AccessibleObject"):
public java.lang.annotation.Annotation getAnnotation(java.lang.Class)
public boolean isAnnotationPresent(java.lang.Class)
public [Ljava.lang.annotation.Annotation; getAnnotations()
public [Ljava.lang.annotation.Annotation; getDeclaredAnnotations()
public static void setAccessible([Ljava.lang.reflect.AccessibleObject;, boolean)
public void setAccessible(boolean)
private static void setAccessible0(java.lang.reflect.AccessibleObject, boolean)
public boolean isAccessible()


![Figure 16.4 Java reﬂection...](images/page_873_caption_Figure%2016.4%20Java%20re%EF%AC%82ection%20code%20to%20list%20the%20methods%20of%20a%20given%20class.%20Sample%20output%20is%20shown%20below%20b.png)
*Figure 16.4 Java reﬂection code to list the methods of a given class. Sample output is shown below below the code.*

control whether the reﬂection interface can be used to override access control
[private, protected] for the given object.)
■
One can even use reﬂection to call a method of an object whose class is not
EXAMPLE 16.23
Calling a method with
reﬂection
known at compile time. Suppose that someone has created a stack containing a
single integer:
Stack s = new Stack();
s.push(new Integer(3));
Now suppose we are passed this stack as a parameter u of Object type. We can
use reﬂection to explore the concrete type of u. In the process we will discover
that its second method, named pop, takes no arguments and returns an Object
result. We can call this method using Method.invoke:
16.3 Inspection/Introspection
841
Method uMethods[] = u.getClass().getMethods();
Method method1 = uMethods[1];
Object rtn = method1.invoke(u);
// u.pop()
A call to rtn.getClass().getName() will return java.lang.Integer. A call
to ((Integer) rtn).intValue() will return the value 3 that was originally
pushed into s.
■
Other Languages
C#’s reﬂection API is similar to that of Java: System.Type is analogous to
java.lang.Class; System.Reflection is analogous to java.lang.reflect.
The pseudo function typeof plays the role of Java’s pseudo ﬁeld .class. More
substantive differences stem from the fact that PE assemblies contain a bit more
information than is found in Java class ﬁles.
We can ask for names of for-
mal parameters in C#, for example, not just their types.
More signiﬁcantly,
the use of reiﬁcation instead of erasure for generics means that we can re-
trieve precise information on the type parameters used to instantiate a given
object. Perhaps the biggest difference is that .NET provides a standard library,
System.Reflection.Emit, to create PE assemblies and to populate them with
CIL. The functionality of Reflection.Emit is roughly comparable to that of the
BCEL and ASM tools mentioned in the previous subsection. Because it is part of
the standard library, however, it is available to any program running on the CLI.
All of the major scripting languages (Perl, PHP, Tcl, Python, Ruby, JavaScript)
provide extensive reﬂection mechanisms. The precise set of capabilities varies
some from language to language, and the syntax varies quite a bit, but all allow
a program to explore its own structure and types. From the programmer’s point
of view, the principal difference between reﬂection in Java and C# on the one
hand, and in scripting languages on the other, is that the scripting languages—
like Lisp—are dynamically typed. In Ruby, for example, we can discover the class
EXAMPLE 16.24
Reﬂection facilities in Ruby
of an object, the methods of a class or object, and the number of parameters
expected by each method, but the parameters themselves are untyped until the
method is called. In the following code, method p prints its argument to standard
output, followed by a newline:
squares = {2=>4, 3=>9}
p squares.class
# Hash
p Hash.public_instance_methods.length
# 146 -- Hashes have many methods
p squares.public_methods.length
# 146 -- those same methods
m = Hash.public_instance_methods[12]
p m
# ":store"
p squares.method(m).arity
# 2 -- key and value to be stored
As in Java and C#, we can also invoke a method whose name was not known
at compile time:
842
Chapter 16 Run-Time Program Management
squares.store(1, 1)
# static invocation
p squares
# {2=>4, 3=>9, 1=>1}
squares.send(m, 0, 0)
# dynamic invocation
p squares
# {2=>4, 3=>9, 1=>1, 0=>0}
■
As suggested at the beginning of this section, reﬂection is in some sense more
“natural” in scripting languages (and in Lisp and Prolog) than it is in Java or
C#: detailed symbol table information is needed at run time to perform dynamic
type checks; in an interpreted implementation, it is also readily available. Lisp
programmers have known for decades that reﬂection was useful for many ad-
ditional purposes. The designers of Java and C# clearly felt these purposes were
valuable enough to justify adding reﬂection (with considerably higher implemen-
tation complexity) to a compiled language with static typing.
Annotations and Attributes
Both Java and C# allow the programmer to extend the metadata saved by the
compiler. In Java, these extensions take the form of annotations attached to decla-
rations. Several annotations are built into the programming language. These play
the role of pragmas. In Example C 7.65, for example, we noted that the Java com-
piler will generate warnings when a generic class is assigned into a variable of the
equivalent nongeneric class. The warning indicates that the code is not statically
type-safe, and that an error message is possible at run time. If the programmer
is certain that the error cannot arise, the compile-time warning can be disabled
by preﬁxing the method in which the assignment appears with the annotation
@SuppressWarnings("unchecked").
In general, a Java annotation resembles an interface whose methods take no
parameters, throw no exceptions, and return values of one of a limited number
of predeﬁned types. An example of a user-deﬁned annotation appears in Fig-
EXAMPLE 16.25
User-deﬁned annotations
in Java
ure 16.5. If we run the program it will print
author:
Michael Scott
date:
July, 2015
revision:
0.1
docString: Illustrates the use of annotations
■
The C# equivalent of Figure 16.5 appears in Figure 16.6. Here user-deﬁned
EXAMPLE 16.26
User-deﬁned annotations
in C#
annotations (known as attributes in C#) are classes, not interfaces, and the syntax
for attaching an attribute to a declaration uses square brackets instead of an @
sign.
■
In effect, annotations (attributes) serve as compiler-supported comments,
with well-deﬁned structure and an API that makes them accessible to automated
perusal. As we have seen, they may be read by the compiler (as pragmas) or by
reﬂective programs. They may also be read by independent tools. Such tools can
be surprisingly versatile.
An obvious use is the automated creation of documentation. Java annotations
EXAMPLE 16.27
javadoc
(ﬁrst introduced in Java 5) were inspired at least in part by experience with the
16.3 Inspection/Introspection
843
import static java.lang.System.out;
import java.lang.annotation.*;
@Retention(RetentionPolicy.RUNTIME)
@interface Documentation{
String author();
String date();
double revision();
String docString();
}
@Documentation(
author = "Michael Scott",
date = "July, 2015",
revision = 0.1,
docString = "Illustrates the use of annotations"
)
public class Annotate {
public static void main(String[] args) {
Class<Annotate> c = Annotate.class;
Documentation a = c.getAnnotation(Documentation.class);
out.println("author:
" + a.author());
out.println("date:
" + a.date());
out.println("revision:
" + a.revision());
out.println("docString: " + a.docString());
}
}


![Figure 16.5 User-deﬁned annotations...](images/page_876_caption_Figure%2016.5%20User-de%EF%AC%81ned%20annotations%20in%20Java.%20Retention%20is%20a%20built-in%20annotation%20for%20annota-%20tions.%20I.png)
*Figure 16.5 User-deﬁned annotations in Java. Retention is a built-in annotation for annota- tions. It indicates here that Documentation annotations should be saved in the class ﬁle pro- duced by the Java compiler, where they will be available to run-time reﬂection.*

earlier javadoc tool, which produces HTML-formatted documentation based on
structured comments in Java source code. The @Documented annotation, when
attached to the declaration of a user-deﬁned annotation, indicates that javadoc
should include the annotation when creating its reports. One can easily imagine
more sophisticated documentation systems that tracked the version history and
bug reports for a program over time.
■
The various communication technologies in .NET make extensive use of at-
EXAMPLE 16.28
Intercomponent
communication
tributes to indicate which methods should be available for remote execution, how
their parameters should be marshalled into messages, which classes need serial-
ization code, and so forth. Automatic tools use these attributes to create appro-
priate stubs for remote communication, as described (in language-neutral terms)
in Section C 13.5.4.
■
In a similar vein, the .NET LINQ mechanism uses attributes to deﬁne the map-
EXAMPLE 16.29
Attributes for LINQ
ping between classes in a user program and tables in a relational database, allow-
844
Chapter 16 Run-Time Program Management
using System;
using System.Reflection;
[AttributeUsage(AttributeTargets.Class)]
// Documentation attribute can applied only to classes
public class DocumentationAttribute : System.Attribute {
public string author;
public string date;
// these should perhaps be properties
public double revision;
public string docString;
public DocumentationAttribute(string a, string d, double r, string s) {
author = a;
date = d;
revision = r;
docString = s;
}
}
[Documentation("Michael Scott",
"July, 2015", 0.1, "Illustrates the use of attributes")]
public class Attr {
public static void Main(string[] args) {
System.Reflection.MemberInfo tp = typeof(Attr);
object[] attrs =
tp.GetCustomAttributes(typeof(DocumentationAttribute), false);
// false means don't search ancestor classes and interfaces
DocumentationAttribute a = (DocumentationAttribute) attrs[0];
Console.WriteLine("author:
" + a.author);
Console.WriteLine("date:
" + a.date);
Console.WriteLine("revision:
" + a.revision);
Console.WriteLine("docString: " + a.docString);
}
}


![Figure 16.6 User-deﬁned attributes...](images/page_877_caption_Figure%2016.6%20User-de%EF%AC%81ned%20attributes%20in%20C%23.%20This%20code%20is%20roughly%20equivalent%20to%20the%20Java%20version%20in%20Fig.png)
*Figure 16.6 User-deﬁned attributes in C#. This code is roughly equivalent to the Java version in Figure 16.5. AttributeUsage is a predeﬁned attribute indicating properties of the attribute to whose declaration it is attached.*

ing an automatic tool to generate SQL queries that implement iterators and other
language-level operations.
■
In an even more ambitious vein, independent tools can be used to modify or
analyze programs based on annotations. One could imagine inserting logging
code into certain annotated methods, or building a testing harness that called
annotated methods with speciﬁed arguments and checked for expected results
(Exercise 16.11). JML, the Java Modeling Language, allows the programmer to
EXAMPLE 16.30
The Java Modeling
Language
specify preconditions, post-conditions, and invariants for classes, methods, and
statements, much like those we considered under “Assertions” in Section 4.1.
JML builds on experience with an earlier multilanguage, multi-institution project
known as Larch. Like javadoc, JML uses structured comments rather than the
newer compiler-supported annotations to express its speciﬁcations, so they are
not automatically included in class ﬁles. A variety of tools can be used, however,
16.3 Inspection/Introspection
845
to verify that a program conforms to its speciﬁcations, either statically (where
possible) or at run time (via insertion of semantic checks).
■
Java 5 introduced a program called apt designed to facilitate the construction
EXAMPLE 16.31
Java annotation processors
of annotation processing tools. The functionality of this tool was subsequently
integrated into Sun’s Java 6 compiler. Its key enabling feature is a set of APIs
(in javax.annotation.processing) that allow an annotation processor class to
be added to a program in such a way that the compiler will run it at compile
time. Using reﬂection, the class can peruse the static structure of the program
(including annotations and full information on generics) in much the same way
that traditional reﬂection mechanisms allow a running program to peruse its own
types and structure.
■
16.3.2 Symbolic Debugging
Most programmers are familiar with symbolic debuggers: they are built into most
programming language interpreters, virtual machines, and integrated program
development environments. They are also available as stand-alone tools, of which
the best known is probably GNU’s gdb. The adjective symbolic refers to a debug-
ger’s understanding of high-level language syntax—the symbols in the original
program. Early debuggers understood assembly language only.
In a typical debugging session, the user starts a program under the control of
the debugger, or attaches the debugger to an already running program. The de-
bugger then allows the user to perform two main kinds of operations. One kind
inspects or modiﬁes program data; the other controls execution: starting, stop-
ping, stepping, and establishing breakpoints and watchpoints. A breakpoint speci-
ﬁes that execution should stop if it reaches a particular location in the source code.
A watchpoint speciﬁes that execution should stop if a particular variable is read
or written. Both breakpoints and watchpoints can typically be made conditional,
so that execution stops only if a particular Boolean predicate evaluates to true.
Both data and control operations depend critically on symbolic information. A
symbolic debugger needs to be able both to parse source language expressions and
to relate them to symbols in the original program. In gdb, for example, the com-
mand print a.b[i] needs to parse the to-be-printed expression; it also needs to
recognize that a and i are in scope at the point where the program is currently
stopped, and that b is an array-typed ﬁeld whose index range includes the current
value of i. Similarly, the command break 123 if i+j == 3 needs to parse the
expression i+j; it also needs to recognize that there is an executable statement at
line 123 in the current source ﬁle, and that i and j are in scope at that line.
Both data and control operations also depend on the ability to manipulate a
program from outside: to stop and start it, and to read and write its data. This
control can be implemented in at least three ways. The easiest occurs in inter-
preters. Since an interpreter has direct access to the program’s symbol table and
is “in the loop” for the execution of every statement, it is a straightforward matter
to move back and forth between the program and the debugger, and to give the
latter access to the former’s data.
846
Chapter 16 Run-Time Program Management
The technology of dynamic binary rewriting (as in Dynamo and Pin) can also
be used to implement debugger control [ZRA+08]. This technology is relatively
new, however, and is not widely employed in production debugging tools.
For compiled programs, the third implementation of debugger control is by far
the most common. It depends on support from the operating system. In Unix,
it employs a kernel service known as ptrace. The ptrace kernel call allows a
debugger to “grab” (attach to) an existing process or to start a process under its
control. The tracing process (the debugger) can intercept any signals sent to the
traced process by the operating system and can read and write its registers and
memory. If the traced process is currently running, the debugger can stop it by
sending it a signal. If it is currently stopped, the debugger can specify the address
at which it should resume execution, and can ask the kernel to run it for a single
instruction (a process known as single stepping) or until it receives another signal.
Perhaps the most mysterious parts of debugging from the user’s perspective are
the mechanisms used to implement breakpoints, watchpoints, and single step-
ping. The default implementation, which works on any modern processor, relies
DESIGN & IMPLEMENTATION
16.6 DWARF
To enable symbolic debugging, the compiler must include symbol table in-
formation in each object ﬁle, in a format the debugger can understand. The
DWARF format, used by many systems (Linux among them) is among the
most versatile available [DWA10, Eag12]. Originally developed by Brian Rus-
sell of Bell Labs in the late 1980s, DWARF is now maintained by the inde-
pendent DWARF Committee, led by Michael Eager. Version 5 is expected to
appear in late 2015.
Unlike many proprietary formats, DWARF is designed to accommodate a
very wide range of (statically typed) programming languages and an equally
wide variety of machine architectures. Among other things, it encodes the
representation of all program types, the names, types, and scopes of all pro-
gram objects (in the broad sense of the term), the layout of all stack frames,
and the mapping from source ﬁles and line numbers to instruction addresses.
Much emphasis has been placed on terse encoding. Program objects are
described hierarchically, in a manner reminiscent of an AST. Character string
names and other repeated elements are captured exactly once, and then ref-
erenced indirectly. Integer constants and references employ a variable-length
encoding, so small values take fewer bits. Perhaps most important, stack lay-
outs and source-to-address mappings are encoded not as explicit tables, but as
ﬁnite automata that generate the tables, line by line. For the tiny gcd program
of Example 1.20, a human-readable representation of the DWARF debugging
information (as produced by Linux’s readelf tool) would ﬁll more than four
full pages of this book. The binary encoding in the object ﬁle takes only 571
bytes.
16.3 Inspection/Introspection
847
on the ability to modify the memory space of the traced process—in particular,
the portion containing the program’s code. As an example, suppose the traced
EXAMPLE 16.32
Setting a breakpoint
process is currently stopped, and that before resuming it the debugger wishes to
set a breakpoint at the beginning of function foo. It does so by replacing the ﬁrst
instruction of the function’s prologue with a special kind of trap.
Trap instructions are the normal way a process requests a service from the op-
erating system. In this particular case, the kernel interprets the trap as a request to
stop the currently running process and return control to the debugger. To resume
the traced process in the wake of the breakpoint, the debugger puts back the orig-
inal instruction, asks the kernel to single-step the traced process, replaces the in-
struction yet again with a trap (to reenable the breakpoint), and ﬁnally resumes
the process. For a conditional breakpoint, the debugger evaluates the condition’s
predicate when the breakpoint occurs. If the breakpoint is unconditional, or if
the condition is true, the debugger jumps to its command loop and waits for user
input. If the predicate is false, it resumes the traced process automatically and
transparently. If the breakpoint is set in an inner loop, where control will reach it
frequently, but the condition is seldom true, the overhead of switching back and
forth between the traced process and the debugger can be very high.
■
Some processors provide hardware support to make breakpoints a bit faster.
The x86, for example, has four debugging registers that can be set (in kernel mode)
EXAMPLE 16.33
Hardware breakpoints
to contain an instruction address. If execution reaches that address, the processor
simulates a trap instruction, saving the debugger the need to modify the address
space of the traced process and eliminating the extra kernel calls (and the extra
round trip between the traced process and the debugger) needed to restore the
original instruction, single-step the process, and put the trap back in place. In a
similar vein, many processors, including the x86, can be placed in single-stepping
mode, which simulates a trap after every user-mode instruction. Without such
support, the debugger (or the kernel) must implement single-stepping by repeat-
edly placing a (temporary) breakpoint at the next instruction.
■
Watchpoints are a bit trickier. By far the easiest implementation depends on
hardware support. Suppose we want to drop into the debugger whenever the
program modiﬁes some variable x. The debugging registers of the x86 and other
EXAMPLE 16.34
Setting a watchpoint
modern processors can be set to simulate a trap whenever the program writes to
x’s address. When the processor lacks such hardware support, or when the user
asks the debugger to set more breakpoints or watchpoints than the hardware can
support, there are several alternatives, none of them attractive. Perhaps the most
obvious is to single step the process repeatedly, checking after each instruction to
see whether x has been modiﬁed. If the processor also lacks a single-step mode,
the debugger will want to place its temporary breakpoints at successive store in-
structions rather than at every instruction (it may be able to skip some of the store
instructions if it can prove they cannot possibly reach the address of x). Alterna-
tively, the debugger can modify the address space of the traced process to make
x’s page unwritable. The process will then take a segmentation fault on every
write to that page, allowing the debugger to intervene. If the write is actually to
848
Chapter 16 Run-Time Program Management
x, the debugger jumps to its command loop. Otherwise it performs the write on
the process’s behalf and asks the kernel to resume it.
■
Unfortunately, the overhead of repeated context switches between the traced
process and the debugger dramatically impacts the performance of software
watchpoints: slowdowns of 1000× are not uncommon. Debuggers based on dy-
namic binary rewriting have the potential to support arbitrary numbers of watch-
points at speeds close to those admitted by hardware watchpoint registers. The
idea is straightforward: the traced program runs as partial execution traces in a
trace cache managed by the debugger. As it generates each trace, the debugger
adds instructions at every store, in-line, to check whether it writes to x’s address
and, if so, to jump back to the command loop.
16.3.3 Performance Analysis
Before placing a debugged program into production use, one often wants to
understand—and if possible improve—its performance. Tools to proﬁle and an-
alyze programs are both numerous and varied—far too much so to even survey
them here. We focus therefore on the run-time technologies, described in this
chapter, that feature prominently in many analysis tools.
Perhaps the simplest way to measure, at least approximately, the amount of
EXAMPLE 16.35
Statistical sampling
time spent in each part of the code is to sample the program counter (PC) pe-
riodically. This approach was exempliﬁed by the classic prof tool in Unix. By
linking with a special prof library, a program could arrange to receive a periodic
timer signal—once a millisecond, say—in response to which it would increment a
counter associated with the current PC. After execution, the prof post-processor
would correlate the counters with an address map of the program’s code and pro-
duce a statistical summary of the percentage of time spent in each subroutine and
loop.
■
While simple, prof had some serious limitations. Its results were only approx-
imate, and could not capture ﬁne-grain costs. It also failed to distinguish among
calls to a given routine from multiple locations. If we want to know which of A, B,
EXAMPLE 16.36
Call graph proﬁling
and C is the biggest contributor to program run time, it is not particularly helpful
to learn that all three of them call D, where most of the time is actually spent. If
we want to know whether it is A’s Ds, B’s Ds, or C’s Ds that are so expensive, we
can use the (slightly) more recent gprof tool, which relies on compiler support to
instrument procedure prologues. As the instrumented program runs, it logs the
number of times that D is called from each location. The gprof post-processor
then assumes that the total time spent in D can accurately be apportioned among
the call sites according to the relative number of calls. More sophisticated tools
log not only the caller and callee but also the stack backtrace (the contents of the
dynamic chain), allowing them to cope with the case in which D consumes twice
as much time when called from A as it does when called from B or C (see Exer-
cise 16.13).
■
If our program is underperforming for algorithmic reasons, it may be enough
to know where it is spending the bulk of its time. We can focus our attention on
16.3 Inspection/Introspection
849
improving the source code in the places it will matter most. If the program is
underperforming for other reasons, however, we generally need to know why. Is
it cache misses due to poor locality, perhaps? Branch mispredictions? Poor use
of the processor pipeline? Tools to address these and similar questions generally
rely on more extensive instrumentation of the code or on some sort of hardware
support.
As an example of instrumentation, consider the task of identifying basic blocks
EXAMPLE 16.37
Finding basic blocks with
low IPC
that execute an unusually small number of instructions per cycle. To ﬁnd such
blocks we can combine (1) the aggregate time spent in each block (obtained by
statistical sampling), (2) a count of the number of times each block executes (ob-
tained via instrumentation), and (3) static knowledge of the number of instruc-
tions in each block. If basic block i contains ki instructions and executes ni times
during a run of a program, it contributes kini dynamic instructions to that run.
Let N = 
i kini be the total number of instructions in the run. If statistical
sampling indicates that block i accounts for xi% of the time in the run and xi is
signiﬁcantly larger than (kini)/N, then something strange is going on—probably
an unusually large number of cache misses.
■
Most modern processors provide a set of performance counters that can be used
to good effect by performance analysis tools. The Intel Haswell processor, for ex-
EXAMPLE 16.38
Haswell performance
counters
ample, has built-in counters for clock ticks (both total and when running) and
instructions executed. It also has four general-purpose counters, which can be
conﬁgured by the kernel to count any of more than 250 different kinds of events,
including branch mispredictions; TLB (address translation) misses; and various
kinds of cache misses, interrupts, executed instructions, and pipeline stalls. Fi-
nally, it has counters for the number of Joules of energy consumed by the pro-
cessor cores, caches, and memory. Unfortunately, performance counters are gen-
erally a scarce resource (one might often wish for many more of them). Their
number, type, and mode of operation varies greatly from processor to processor;
direct access to them is usually available only in kernel mode; and operating sys-
tems do not always export that access to user-level programs with a convenient or
uniform interface. Tools to access the counters and interpret their values are avail-
able from most manufacturers—Intel among them. Portable tools are an active
topic of research.
■
3CHECK YOUR UNDERSTANDING
27. What is reﬂection? What purposes does it serve?
28. Describe an inappropriate use of reﬂection.
29. Name an aspect of reﬂection supported by the CLI but not by the JVM.
30. Why is reﬂection more difﬁcult to implement in Java or C# than it is in Perl
or Ruby?
31. What are annotations (Java) or attributes (C#)? What are they used for?
850
Chapter 16 Run-Time Program Management
32. What are javadoc, apt, JML, and LINQ, and what do they have to do with
annotation?
33. Brieﬂy describe three different implementation strategies for a symbolic de-
bugger.
34. Explain the difference between breakpoints and watchpoints. Why are watch-
points potentially much more expensive?
35. Summarize the capabilities provided by the Unix ptrace mechanism.
36. What is the principal difference between the Unix prof and gprof tools?
37. For the purposes of performance analysis, summarize the relative strengths
and limitations of statistical sampling, instrumentation, and hardware perfor-
mance counters. Explain why statistical sampling and instrumentation might
proﬁtably be used together.
16.4
Summary and Concluding Remarks
We began this chapter by deﬁning a run-time system as the set of libraries, es-
sential to many language implementations, that depend on knowledge of the
compiler or the programs it produces. We distinguished these from “ordinary”
libraries, which require only the arguments they are passed.
We noted that several topics covered elsewhere in the book, including garbage
collection, variable-length argument lists, exception and event handling, corou-
tines and threads, remote procedure calls, transactional memory, and dynamic
linking are often considered the purview of the run-time system. We then turned
to virtual machines, focusing in particular on the Java Virtual Machine (JVM)
and the Common Language Infrastructure (CLI). Under the general heading of
late binding of machine code, we considered just-in-time and dynamic compila-
tion, binary translation and rewriting, and mobile code and sandboxing. Finally,
under the general heading of inspection and introspection, we considered reﬂec-
tion mechanisms, symbolic debugging, and performance analysis.
Through all these topics we have seen a steady increase in complexity over
time. Early Basic interpreters parsed and executed one source statement at a time.
Modern interpreters ﬁrst translate their source into a syntax tree. Early Java im-
plementations, while still interpreter-based, relied on a separate source-to-byte-
code compiler. Modern Java implementations, as well as implementations of the
CLI, enhance performance with a just-in-time compiler. For programs that ex-
tend themselves at run time, the CLI allows the source-to-byte-code compiler to
be invoked dynamically as well, as it is in Common Lisp. Recent systems may pro-
ﬁle and reoptimize already-running programs. Similar technology may allow sep-
arate tools to translate from one machine language to another, or to instrument
code for testing, debugging, security, performance analysis, model checking, or
16.5 Exercises
851
architectural simulation. The CLI provides extensive support for cross-language
interoperability.
Many of these developments have served to blur the line between the compiler
and the run-time system, and between compile-time and run-time operations. It
seems safe to predict that these trends will continue. More and more, programs
will come to be seen not as static artifacts, but as dynamic collections of mal-
leable components, with rich semantic structure amenable to formal analysis and
reconﬁguration.
16.5
Exercises
16.1
Write the formula of Example 15.4 as an expression tree (a syntax tree in
which each operator is represented by an internal node whose children
are its operands). Convert your tree to an expression DAG by merging
identical nodes. Comment on the redundancy in the tree and how it relates
to Figure 15.4.
16.2
We assumed in Example 15.4 and Figure 15.4 that a, b, c, and s were
all among the ﬁrst few local variables of the current method, and could
be pushed onto or popped from the operand stack with a single one-byte
instruction. Suppose that this is not the case: that is, that the push and
pop instructions require three bytes each. How many bytes will now be
required for the code on the left side of Figure 15.4?
Most stack-based languages, Java bytecode and CIL among them, pro-
vide a swap instruction that reverses the order of the top two values on the
stack, and a duplicate instruction that pushes a second copy of the value
currently at top of stack. Show how to use swap and duplicate to elimi-
nate the pop and the pushes of s in the left side of Figure 15.4. Feel free to
exploit the associativity of multiplication. How many instructions is your
new sequence? How many bytes?
16.3
The speculative optimization of Example 16.5 could in principle be stati-
cally performed. Explain why a dynamic compiler might be able to do it
more effectively.
16.4
Perhaps the most common form of run-time instrumentation counts the
the number of times that each basic block is executed. Since basic blocks
are short, adding a load-increment-store instruction sequence to each
block can have a signiﬁcant impact on run time.
We can improve performance by noting that certain blocks imply the
execution of other blocks. In an if... then ... else construct, for example,
execution of either the then part or the else part implies execution of the
conditional test. If we’re smart, we won’t actually have to instrument the
test.
Describe a general technique to minimize the number of blocks that
must be instrumented to allow a post-processor to obtain an accurate
852
Chapter 16 Run-Time Program Management
count for each block. (This is a difﬁcult problem. For hints, see the paper
by Larus and Ball [BL92].)
16.5
Visit software.intel.com/en-us/articles/pintool-downloads and download a
copy of Pin. Use it to create a tool to proﬁle loops. When given a (machine
code) program and its input, the output of the tool should list the number
of times that each loop was encountered when running the program. It
should also give a histogram, for each loop, of the number of iterations
executed.
16.6
Outline mechanisms that might be used by a binary rewriter, without
access to source code, to catch uses of uninitialized variables, “double
deletes,” and uses of deallocated memory (e.g., dangling pointers). Under
what circumstances might you be able to catch memory leaks and out-of-
bounds array accesses?
16.7
Extend the code of Figure 16.4 to print information about
(a) ﬁelds
(b) constructors
(c)
nested classes
(d) implemented interfaces
(e)
ancestor classes, and their methods, ﬁelds, and constructors
(f)
exceptions thrown by methods
(g) generic type parameters
16.8
Repeat the previous exercise in C#. Add information about parameter
names and generic instances.
16.9
Write an interactive tool that accepts keyboard commands to load speci-
ﬁed class ﬁles, create instances of their classes, invoke their methods, and
read and write their ﬁelds. Feel free to limit keyboard input to values of
built-in types, and to work only in the global scope. Based on your expe-
rience, comment on the feasibility of writing a command-line interpreter
for Java, similar to those commonly used for Lisp, Prolog, or the various
scripting languages.
16.10
In Java, if the concrete type of p is Foo, p.getClass() and Foo.class
will return the same thing. Explain why a similar equivalence could not
be guaranteed to hold in Ruby, Python, or JavaScript. For hints, see Sec-
tion 14.4.4.
16.11
Design a “test harness” system based on Java annotations. The user should
be able to attach to a method an annotation that speciﬁes parameters
to be passed to a test run of the method, and values expected to be re-
turned. For simplicity, you may assume that parameters and return val-
ues will all be strings or instances of built-in types. Using the annota-
tion processing facility of Java 6, you should automatically generate a new
method, test() in any class that has methods with @Test annotations.
16.6 Explorations
853
This method should call the annotated methods with the speciﬁed param-
eters, test the return values, and report any discrepancies. It should also
call the test methods of any nested classes. Be sure to include a mech-
anism to invoke the test method of every top-level class. For an extra
challenge, devise a way to specify multiple tests of a single method, and a
way to test exceptions thrown, in addition to values returned.
16.12
C++ provides a typeid operator that can be used to query the concrete
type of a pointer or reference variable:
if (typeid(*p) == typeid(my_derived_type)) ...
Values returned by typeid can be compared for equality but not assigned.
They also support a name() method that returns an (implementation-
dependent) character string name for the type. Give an example of a pro-
gram fragment in which these mechanisms might reasonably be used.
Unlike more extensive reﬂection mechanisms, typeid can be applied
only to (instances of) classes with at least one virtual method. Give a plau-
sible explanation for this restriction.
16.13
Suppose we wish, as described at the end of Example 16.36, to accurately
attribute sampled time to the various contexts in which a subroutine is
called. Perhaps the most straightforward approach would be to log not
only the current PC but also the stack backtrace—the contents of the dy-
namic chain—on every timer interrupt. Unfortunately, this can dramat-
ically increase proﬁling overhead. Suggest an equivalent but cheaper im-
plementation.
16.14–16.17 In More Depth.
16.6
Explorations
16.18
Learn about the Java security policy mechanism. What aspects of program
behavior can the programmer enable/proscribe? How are such policies
enforced? What is the relationship (if any) between security policies and
the veriﬁcation process described in Sidebar 16.3?
16.19
Learn about taint mode in Perl and Ruby. How does it compare to sand-
box creation via binary rewriting, as described in Sidebar 16.5? What sorts
of security problems does it catch? What sorts of problems does it not
catch?
16.20
Learn about proof-carrying code, a technique in which the supplier of mo-
bile code includes a proof of its safety, and the user simply veriﬁes the
proof, rather than regenerating it (start with the work of Necula [Nec97]).
How does this technique compare to other forms of sandboxing? What
properties can it guarantee?
854
Chapter 16 Run-Time Program Management
16.21
Investigate the MetaObject Protocol (MOP), which underlies the Common
Lisp Object System. How does it compare to the reﬂection mechanisms of
Java and C#? What does it allow you to do that these other languages do
not?
16.22
When using a symbolic debugger and moving through a program with
breakpoints, one often discovers that one has gone “too far,” and must start
the program over from the beginning. This may mean losing all the effort
that had been put into reaching a particular point. Consider what it would
take to be able to run the program not only forward but backward as well.
Such a reverse execution facility might allow the user to narrow in on the
source of bugs much as one narrows the range in binary search. Consider
both the loss of information that happens when data is overridden and the
nondeterminism that arises in parallel and event-driven programs.
16.23
Download and experiment with one of the several available packages
for performance counter sampling in Linux (try sourceforge.net/projects/
perfctr/, perfmon2.sourceforge.net/, or www.intel.com/software/pcm). What
do these packages allow you to measure? How might you use the informa-
tion? (Note: you may need to install a kernel patch to make the program
counters available to user-level code.)
16.24–16.25 In More Depth.
16.7
Bibliographic Notes
Aycock [Ayc03] surveys the history of just-in-time compilation. Documentation
on the HotSpot compiler and JVM can be found at Oracle’s web site: www.oracle.
com/technetwork/articles/javase/index-jsp-136373.html. The JVM speciﬁcation is
by Lindholm et al. [LYBB14]. Sources of information on the CLI include the
ECMA standard [Int12a, MR04] and the .NET pages at msdn.microsoft.com.
Arnold et al. [AFG+05] provide an extensive survey of adaptive optimization
techniques for programs on virtual machines. Deutsch and Schiffman [DS84]
describe the ParcPlace Smalltalk virtual machine, which pioneered such mecha-
nisms as just-in-time compilation and the caching of JIT-compiled machine code.
Various articles discuss the binary translation technology of Apple’s 68K emula-
tor [Tho95], DEC’s FX!32 [HH97b], and its earlier VEST and mx [SCK+93].
Probably the best source of information on binary rewriting is the text by
Hazelwood [Haz11]. The original papers on Dynamo, Atom, Pin, and QEMU
are by Bala et al. [BDB00], Srivastava and Eustace [SE94], Luk et al. [LCM+05],
and Bellard [Bel05], respectively. Duesterwald [Due05] surveys issues in the de-
sign of a dynamic binary optimizer, drawing on her experience with the Dynamo
project. Early work on sandboxing via binary rewriting is reported by Wahbe et
al. [WLAG93].
16.7 Bibliographic Notes
855
The DWARF standard is available from dwarfstd.org [DWA10]; Eager pro-
vides a gentler introduction [Eag12]. Ball and Larus [BL92] describe the min-
imal instrumentation required to proﬁle the execution of basic blocks. Zhao
et al. [ZRA+08] describe the use of dynamic instrumentation (based on Dy-
namoRIO) to implement watchpoints efﬁciently.
Martonosi et al. [MGA92]
describe a performance analysis tool that builds on the idea outlined in Exam-
ple 16.37.
This page intentionally left blank
