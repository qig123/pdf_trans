# Chapter 16: Run-Time Program Management

## **16**

## **Run-Time Program Management**

Every nontrivial implementation of a high-level programming language makes
extensive use of libraries. Some library routines are very simple: they may copy
memory from one place to another, or perform arithmetic functions not directly
supported by the hardware. Others are more sophisticated. Heap management
routines, for example, maintain signiﬁcant amounts of internal state, as do li-
braries for buffered or graphical I/O.
In general, we use the term* run-time system* (or sometimes just* runtime*, with-
out the hyphen) to refer to the set of libraries on which the language implemen-
tation depends for correct operation. Some parts of the runtime, like heap man-
agement, obtain all the information they need from subroutine arguments, and
can easily be replaced with alternative implementations. Others, however, require
more extensive knowledge of the compiler or the generated program. In simpler
cases, this knowledge is really just a set of conventions (e.g., for the subroutine
calling sequence) that the compiler and runtime both respect. In more complex
cases, the compiler generates program-speciﬁc* metadata* that the runtime must
inspect to do its job. A tracing garbage collector (Section 8.5.3), for example, de-
pends on metadata identifying all the “root pointers” in the program (all global,
static, and stack-based pointer or reference variables), together with the type of
every reference and of every allocated block.
Many examples of compiler/runtime integration have been discussed in previ-
ous chapters; we review these in Sidebar 16.1. The length and complexity of the
list generally means that the compiler and the run-time system must be developed
together.
Some languages (notably C) have very small run-time systems: most of the
user-level code required to execute a given source program is either generated
directly by the compiler or contained in language-independent libraries. Other
languages have extensive run-time systems. C#, for example, is heavily dependent
on a run-time system deﬁned by the Common Language Infrastructure (CLI)
standard [Int12a].
Like any run-time system, the CLI depends on data generated by the com-
**EXAMPLE** 16.1

The CLI as a run-time
system and virtual machine
piler (e.g., type descriptors, lists of exception handlers, and certain content from
the symbol table). It also makes extensive assumptions about the structure of

**807**

```
Garbage Collection (Section 8.5.3). As noted in the chapter introduction,
a tracing garbage collector must be able to ﬁnd all the “root pointers” in the
program, and to identify the type of every reference and every allocated block.
A compacting collector must be able to modify every pointer in the program.
A generational collector must have access to a list of old-to-new pointer ref-
erences, maintained by write barriers in the main program. A collector for a
language like Java must call appropriate finalize methods. And in imple-
mentations that support concurrent or incremental collection, the main pro-
gram and the collector must agree on some sort of locking protocol to preserve
the consistency of the heap.
```

```
Variable Numbers of Arguments (Section 9.3.3). Several languages allow the
programmer to declare functions that take an arbitrary number of arguments,
of arbitrary type. In C, a call to va_arg(my_args, arg_type) must return the
next argument in the previously identiﬁed list my_args. To ﬁnd the argument,
va_arg must understand which arguments are passed in which registers, and
which arguments are passed on the stack (with what alignment, padding, and
offset). If the code for va_arg is generated entirely in-line, this knowledge may
be embedded entirely in the compiler. If any of the code is in library routines,
however, those routines are compiler-speciﬁc, and thus a (simple) part of the
run-time system.
```

```
Exception Handling (Section 9.4). Exception propagation requires that we
“unwind” the stack whenever control escapes the current subroutine. Code
to deallocate the current frame may be generated by the compiler on a
subroutine-by-subroutine basis. Alternatively, a general-purpose routine to
deallocate any given frame may be part of the run-time system. In a similar
vein, the closest exception handler around a given point in the program may
be found by compiler-generated code that maintains a stack of active handlers,
or by a general-purpose run-time routine that inspects a table of program-
counter-to-handler mappings generated at compile time. The latter approach
avoids any run-time cost when entering and leaving a protected region (try
block).
```

*Event Handling (Section 9.6).* Events are commonly implemented as “spon-
taneous” subroutine calls in a single-threaded program, or as “callbacks” in
a separate, dedicated thread of a concurrent program. Depending on imple-
mentation strategy, they may be able to exploit knowledge of the compiler’s

```
n.next = t;
```

```
} // else v already in set
}
```

```
Code:
Stack=3, Locals=4, Args_size=2
0:
aload_0
// this
1:
getfield
#4; //Field head:LLLset$node;
4:
astore_2
5:
aload_2
// n
6:
getfield
#5; //Field LLset$node.next:LLLset$node;
9:
ifnull
31
// conditional branch
12:
aload_2
13:
getfield
#5; //Field LLset$node.next:LLLset$node;
16:
getfield
#6; //Field LLset$node.val:I
19:
iload_1
// v
20:
if_icmpge
31
23:
aload_2
24:
getfield
#5; //Field LLset$node.next:LLLset$node;
27:
astore_2
28:
goto
5
31:
aload_2
32:
getfield
#5; //Field LLset$node.next:LLLset$node;
35:
ifnull
49
38:
aload_2
39:
getfield
#5; //Field LLset$node.next:LLLset$node;
42:
getfield
#6; //Field LLset$node.val:I
45:
iload_1
46:
if_icmple
76
49:
new
#2; //class LLset$node
52:
dup
53:
aload_0
54:
invokespecial #3; //Method LLset$node."<init>":(LLLset;)V
57:
astore_3
58:
aload_3
// t
59:
iload_1
60:
putfield
#6; //Field LLset$node.val:I
63:
aload_3
64:
aload_2
65:
getfield
#5; //Field LLset$node.next:LLLset$node;
68:
putfield
#5; //Field LLset$node.next:LLLset$node;
71:
aload_2
72:
aload_3
73:
putfield
#5; //Field LLset$node.next:LLLset$node;
76:
return
```


![Figure 16.2 Java source...](images/page_852_vector_564.png)
*Figure 16.2 Java source and bytecode for a list insertion method. Output on the right was produced by Oracle’s javac (compiler) and javap (disassembler) tools, with additional comments inserted by hand.*

```
Various methods of library class Expression can now be used to explore and
manipulate the tree. When desired, the tree can be converted to CIL code:
```

```
square_func = square_tree.Compile();
```

These operations are roughly analogous to the following in Scheme:

pN

loop head


![Figure 16.3 Creation of...](images/page_865_vector_360.png)
*Figure 16.3 Creation of a partial execution trace. Procedure print matching (shown at top) is often called with a particular predicate, p, which is usually false. The control ﬂow graph (left, with hot blocks in bold and the hot path in grey) can be reorganized at run time to improve instruction-cache locality and to optimize across abstraction boundaries (right).*

loop-termination and predicate-checking tests) jump either to other traces or, if
appropriate ones have not yet been created, back into Dynamo.
By identifying and optimizing traces, Dynamo is able to signiﬁcantly improve
locality in the instruction cache, and to apply standard code improvement tech-
niques across the boundaries between separately compiled modules and dynam-
ically loaded libraries. In Figure 16.3, for example, it will perform register allo-
cation jointly across print matchings and the predicate p. It can even perform
instruction scheduling across basic blocks if it inserts appropriate* compensating*
*code* on branches out of the trace. An instruction in block test2, for example,
can be moved into the loop footer if a copy is placed on the branch to the right.
Traces have proved to be a very powerful technique. They are used not only by dy-
namic optimizers, but by dynamic translators like Rosetta as well, and by binary
instrumentation tools like Pin (to be discussed in Section 16.2.3).
■

