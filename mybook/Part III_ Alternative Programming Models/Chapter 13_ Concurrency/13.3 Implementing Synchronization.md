# 13.3 Implementing Synchronization

**652**
Chapter 13* Concurrency*

```
3CHECK YOUR UNDERSTANDING
11. Explain the differences among coroutines, threads, lightweight processes, and
heavyweight processes.
```

12. What is* quasiparallelism*?
13. Describe the* bag of tasks* programming model.

14. What is* busy-waiting*? What is its principal alternative?
15. Name four explicitly concurrent programming languages.

16. Why don’t message-passing programs require explicit synchronization mech-
anisms?

17. What are the tradeoffs between language-based and library-based implemen-
tations of concurrency?

18. Explain the difference between* data parallelism* and* task parallelism*.
19. Describe six different mechanisms commonly used to create new threads of
control in a concurrent program.
20. In what sense is fork/join more powerful than* co-begin*?

21. What is a* thread pool* in Java? What purpose does it serve?
22. What is meant by a* two-level* thread implementation?
23. What is a* ready list*?

24. Describe the progressive implementation of scheduling, preemption, and
(true) parallelism on top of coroutines.

## 13.3

**Implementing Synchronization**
As noted in Section 13.2.1, synchronization is the principal semantic challenge for
shared-memory concurrent programs. Typically, synchronization serves either to
make some operation* atomic* or to delay that operation until some necessary pre-
condition holds. As noted in Section 13.1, atomicity is most commonly achieved
with* mutual exclusion locks*. Mutual exclusion ensures that only one thread is ex-
ecuting some* critical section* of code at a given point in time. Critical sections
typically transform a shared data structure from one consistent state to another.
*Condition synchronization* allows a thread to wait for a precondition, often ex-
pressed as a predicate on the value(s) in one or more shared variables. It is tempt-
ing to think of mutual exclusion as a form of condition synchronization (don’t
proceed until no other thread is in its critical section), but this sort of condition
would require* consensus* among all extant threads, something that condition syn-
chronization doesn’t generally provide.

13.3.3** Memory Consistency**

In all our discussions so far, we have depended, implicitly, on hardware memory
coherence. Unfortunately, coherence alone is not enough to make a multipro-
cessor or even a single multicore processor behave as most programmers would
expect. We must also worry, when more than one location is written at about the
same time, about the* order* in which the writes become visible to different cores.
Intuitively, most programmers expect shared memory to be* sequentially con-*
*sistent*—to make all writes visible to all cores in the same order, and to make any
given core’s writes visible in the order they were performed. Unfortunately, this
behavior turns out to be very hard to implement efﬁciently—hard enough that
most hardware designers simply don’t provide it. Instead, they provide one of sev-
eral* relaxed memory models*, in which certain loads and stores may appear to occur
“out of order.” Relaxed consistency has important ramiﬁcations for language de-
signers, compiler writers, and the implementors of synchronization mechanisms
and nonblocking algorithms.

**The Cost of Ordering**

The fundamental problem with sequential consistency is that straightforward im-
plementations require both hardware and compilers to serialize operations that
we would rather be able to perform in arbitrary order.
Consider, for example, the implementation of an ordinary store instruction.
**EXAMPLE** 13.31

Write buffers and
consistency
In the event of a cache miss, this instruction can take hundreds of cycles to com-
plete.
Rather than wait, most processors are designed to continue executing

For programs running on a single core, regardless of the complexity of the un-
derlying pipeline and memory hierarchy, every manufacturer guarantees that in-
structions will appear to occur in* program order*: no instruction will fail to see
the effects of some prior instruction, nor will it see the effects of any subsequent
instruction. For programs running on a muilticore or multiprocessor machine,
manufacturers also guarantee, under certain circumstances, that instructions ex-
ecuted on one core will be seen, in order, by instructions on other cores. Unfor-
tunately, these circumstances vary from one machine to another. Some imple-
mentations of the MIPS and PA-RISC processors were sequentially consistent, as
are IBM’s z-Series mainframe machines: if a load on core B sees a value written

