652
Chapter 13 Concurrency
3CHECK YOUR UNDERSTANDING
11. Explain the differences among coroutines, threads, lightweight processes, and
heavyweight processes.
12. What is quasiparallelism?
13. Describe the bag of tasks programming model.
14. What is busy-waiting? What is its principal alternative?
15. Name four explicitly concurrent programming languages.
16. Why don’t message-passing programs require explicit synchronization mech-
anisms?
17. What are the tradeoffs between language-based and library-based implemen-
tations of concurrency?
18. Explain the difference between data parallelism and task parallelism.
19. Describe six different mechanisms commonly used to create new threads of
control in a concurrent program.
20. In what sense is fork/join more powerful than co-begin?
21. What is a thread pool in Java? What purpose does it serve?
22. What is meant by a two-level thread implementation?
23. What is a ready list?
24. Describe the progressive implementation of scheduling, preemption, and
(true) parallelism on top of coroutines.
13.3
Implementing Synchronization
As noted in Section 13.2.1, synchronization is the principal semantic challenge for
shared-memory concurrent programs. Typically, synchronization serves either to
make some operation atomic or to delay that operation until some necessary pre-
condition holds. As noted in Section 13.1, atomicity is most commonly achieved
with mutual exclusion locks. Mutual exclusion ensures that only one thread is ex-
ecuting some critical section of code at a given point in time. Critical sections
typically transform a shared data structure from one consistent state to another.
Condition synchronization allows a thread to wait for a precondition, often ex-
pressed as a predicate on the value(s) in one or more shared variables. It is tempt-
ing to think of mutual exclusion as a form of condition synchronization (don’t
proceed until no other thread is in its critical section), but this sort of condition
would require consensus among all extant threads, something that condition syn-
chronization doesn’t generally provide.
13.3 Implementing Synchronization
653
Our implementation of parallel threads, sketched at the end of Section 13.2.4,
requires both atomicity and condition synchronization. Atomicity of operations
on the ready list and related data structures ensures that they always satisfy a set
of logical invariants: the lists are well formed, each thread is either running or
resides in exactly one list, and so forth. Condition synchronization appears in the
requirement that a process in need of a thread to run must wait until the ready
list is nonempty.
It is worth emphasizing that we do not in general want to overly synchronize
programs. To do so would eliminate opportunities for parallelism, which we gen-
erally want to maximize in the interest of performance. Moreover not all races are
bad. If two processes are racing to dequeue the last thread from the ready list, we
don’t generally care which succeeds and which waits for another thread. We do
care that the implementation of dequeue does not have internal, instruction-level
races that might compromise the ready list’s integrity. In general, our goal is to
provide only as much synchronization as is necessary to eliminate “bad” races—
those that might otherwise cause the program to produce incorrect results.
In the ﬁrst subsection below we consider busy-wait synchronization. In the
second we present an alternative, called nonblocking synchronization, in which
atomicity is achieved without the need for mutual exclusion. In the third sub-
section we return to the subject of memory consistency (originally mentioned in
Section 13.1.2), and discuss its implications for the semantics and implementa-
tion of language-level synchronization mechanisms. Finally, in Sections 13.3.4
and 13.3.5, we use busy-waiting among processes to implement a parallelism-safe
thread scheduler, and then use this scheduler in turn to implement the most basic
scheduler-based synchronization mechanism: namely, semaphores.
13.3.1 Busy-Wait Synchronization
Busy-wait condition synchronization is easy if we can cast a condition in the form
of “location X contains value Y”: a thread that needs to wait for the condition can
simply read X in a loop, waiting for Y to appear. To wait for a condition involving
more than one location, one needs atomicity to read the locations together, but
given that, the implementation is again a simple loop.
Other forms of busy-wait synchronization are somewhat trickier. In the re-
mainder of this section we consider spin locks, which provide mutual exclusion,
and barriers, which ensure that no thread continues past a given point in a pro-
gram until all threads have reached that point.
Spin Locks
Dekker is generally credited with ﬁnding the ﬁrst two-thread mutual exclu-
sion algorithm that requires no atomic instructions other than load and store.
Dijkstra [Dij65] published a version that works for n threads in 1965. Peterson
[Pet81] published a much simpler two-thread algorithm in 1981. Building on
654
Chapter 13 Concurrency
type lock = Boolean := false;
procedure acquire lock(ref L : lock)
while not test and set(L)
while L
–– nothing –– spin
procedure release lock(ref L : lock)
L := false


![Figure 13.8 A simple...](images/page_687_caption_Figure%2013.8%20A%20simple%20test-and-test_and_set%20lock.%20Waiting%20processes%20spin%20with%20ordinary%20read%20%28load%29%20in.png)
*Figure 13.8 A simple test-and-test_and_set lock. Waiting processes spin with ordinary read (load) instructions until the lock appears to be free, then use test_and_set to acquire it. The very ﬁrst access is a test_and_set, for speed in the common (no competition) case.*

Peterson’s algorithm, one can construct a hierarchical n-thread lock, but it re-
quires O(n log n) space and O(log n) time to get one thread into its critical sec-
tion [YA93]. Lamport [Lam87]2 published an n-thread algorithm in 1987 that
takes O(n) space and O(1) time in the absence of competition for the lock. Un-
fortunately, it requires O(n) time when multiple threads attempt to enter their
critical section at once.
While all of these algorithms are historically important, a practical spin lock
needs to run in constant time and space, and for this one needs an atomic in-
struction that does more than load or store. Beginning in the 1960s, hardware
designers began to equip their processors with instructions that read, modify, and
write a memory location as a single atomic operation. The simplest such instruc-
EXAMPLE 13.24
The basic test and set lock
tion is known as test_and_set. It sets a Boolean variable to true and returns an
indication of whether the variable was previously false. Given test_and_set,
acquiring a spin lock is almost trivial:
while not test and set(L)
–– nothing –– spin
■
In practice, embedding test_and_set in a loop tends to result in unaccept-
able amounts of communication on a multicore or multiprocessor machine, as
the cache coherence mechanism attempts to reconcile writes by multiple cores at-
tempting to acquire the lock. This overdemand for hardware resources is known
as contention, and is a major obstacle to good performance on large machines.
To reduce contention, the writers of synchronization libraries often employ a
EXAMPLE 13.25
Test-and-test and set
test-and-test_and_set lock, which spins with ordinary reads (satisﬁed by the
cache) until it appears that the lock is free (see Figure 13.8). When a thread re-
leases a lock there still tends to be a ﬂurry of bus or interconnect activity as waiting
2
Leslie Lamport (1941–) has made a variety of seminal contributions to the theory of parallel and
distributed computing, including synchronization algorithms, the notion of “happens-before”
causality, Byzantine agreement, the Paxos consensus algorithm, and the temporal logic of actions.
He also created the LATEX macro package, with which this book was typeset. He received the ACM
Turing Award in 2013.
13.3 Implementing Synchronization
655
threads perform their test_and_sets, but at least this activity happens only at
the boundaries of critical sections. On a large machine, contention can be further
reduced by implementing a backoff strategy, in which a thread that is unsuccessful
in attempting to acquire a lock waits for a while before trying again.
■
Many processors provide atomic instructions more powerful than test_and_
set. Some can swap the contents of a register and a memory location atomically.
Some can add a constant to a memory location atomically, returning the previous
value. Several processors, including the x86, the IA-64, and the SPARC, provide
a particularly useful instruction called compare_and_swap (CAS). This instruc-
tion takes three arguments: a location, an expected value, and a new value. It
checks to see whether the expected value appears in the speciﬁed location, and
if so replaces it with the new value, atomically. In either case, it returns an in-
dication of whether the change was made. Using instructions like atomic_add
or compare_and_swap, one can build spin locks that are fair, in the sense that
threads are guaranteed to acquire the lock in the order in which they ﬁrst attempt
to do so. One can also build locks that work well—with no contention, even at
release time—on arbitrarily large machines [MCS91, Sco13]. These topics are be-
yond the scope of the current text. (It is perhaps worth mentioning that fairness
is a two-edged sword: while it may be desirable from a semantic point of view, it
tends to undermine cache locality, and interacts very badly with preemption.)
An important variant on mutual exclusion is the reader–writer lock [CHP71].
Reader–writer locks recognize that if several threads wish to read the same data
structure, they can do so simultaneously without mutual interference. It is only
when a thread wants to write the data structure that we need to prevent other
threads from reading or writing simultaneously.
Most busy-wait mutual ex-
clusion locks can be extended to allow concurrent access by readers (see Exer-
cise 13.8).
Barriers
Data-parallel algorithms are often structured as a series of high-level steps, or
phases, typically expressed as iterations of some outermost loop. Correctness of-
ten depends on making sure that every thread completes the previous step before
any moves on to the next. A barrier serves to provide this synchronization.
As a concrete example, ﬁnite element analysis models a physical object—a
EXAMPLE 13.26
Barriers in ﬁnite element
analysis
bridge, let us say—as an enormous collection of tiny fragments. Each fragment of
the bridge imparts forces to the fragments adjacent to it. Gravity exerts a down-
ward force on all fragments. Abutments exert an upward force on the fragments
that make up base plates. The wind exerts forces on surface fragments. To eval-
uate stress on the bridge as a whole (e.g., to assess its stability and resistance to
failures), a ﬁnite element program might divide the fragments among a large col-
lection of threads (probably one per core). Beginning with the external forces,
the program would then proceed through a sequence of iterations. In each it-
eration, each thread would recompute the forces on its fragments based on the
forces found in the previous iteration. Between iterations, the threads would
656
Chapter 13 Concurrency
shared count : integer := n
shared sense : Boolean := true
per-thread private local sense : Boolean := true
procedure central barrier()
local sense := not local sense
–– each thread toggles its own sense
if fetch and decrement(count) = 1
–– last arriving thread
count := n
–– reinitialize for next iteration
sense := local sense
–– allow other threads to proceed
else
repeat
–– spin
until sense = local sense


![Figure 13.9 A simple...](images/page_689_caption_Figure%2013.9%20A%20simple%20%E2%80%9Csense-reversing%E2%80%9D%20barrier.%20Each%20thread%20has%20its%20own%20copy%20of%20local%20sense.%20Threads.png)
*Figure 13.9 A simple “sense-reversing” barrier. Each thread has its own copy of local sense. Threads share a single copy of count and sense.*

synchronize with a barrier. The program would halt when no thread found a
signiﬁcant change in any forces during the last iteration.
■
The simplest way to implement a busy-wait barrier is to use a globally shared
EXAMPLE 13.27
The “sense-reversing”
barrier
counter, modiﬁed by an atomic fetch_and_decrement instruction. The counter
begins at n, the number of threads in the program. As each thread reaches the
barrier it decrements the counter. If it is not the last to arrive, the thread then
spins on a Boolean ﬂag. The ﬁnal thread (the one that changes the counter from
1 to 0) ﬂips the Boolean ﬂag, allowing the other threads to proceed. To make it
easy to reuse the barrier data structures in successive iterations (known as barrier
episodes), threads wait for alternating values of the ﬂag each time through. Code
for this simple barrier appears in Figure 13.9.
■
Like a simple spin lock, the “sense-reversing” barrier can lead to unacceptable
levels of contention on large machines. Moreover the serialization of access to
the counter implies that the time to achieve an n-thread barrier is O(n). It is
possible to do better, but even the fastest software barriers require O(log n) time
to synchronize n threads [MCS91]. Large multiprocessors sometimes provide
special hardware to reduce this bound to close to constant time.
The Java 7 Phaser class provides unusually ﬂexible barrier support. The set of
EXAMPLE 13.28
Java 7 phasers
participating threads can change from one phaser episode to another. When the
number is large, the phaser can be tiered to run in logarithmic time. Moreover,
arrival and departure can be speciﬁed as separate operations: in between, a thread
can do work that (a) does not require that all other threads have arrived, and (b)
does not have to be completed before any other threads depart.
■
13.3 Implementing Synchronization
657
13.3.2 Nonblocking Algorithms
When a lock is acquired at the beginning of a critical section, and released at
the end, no other thread can execute a similarly protected piece of code at the
same time. As long as every thread follows the same conventions, code within the
critical section is atomic—it appears to happen all at once. But this is not the only
possible way to achieve atomicity. Suppose we wish to make an arbitrary update
EXAMPLE 13.29
Atomic update with CAS
to a shared location:
x := foo(x);
Note that this update involves at least two accesses to x: one to read the old value
and one to write the new. We could protect the sequence with a lock:
acquire(L)
r1 := x
r2 := foo(r1)
–– probably a multi-instruction sequence
x := r2
release(L)
But we can also do this without a lock, using compare_and_swap:
start:
r1 := x
r2 := foo(r1)
–– probably a multi-instruction sequence
r2 := CAS(x, r1, r2)
–– replace x if it hasn’t changed
if !r2 goto start
If several cores execute this code simultaneously, one of them is guaranteed to suc-
ceed the ﬁrst time around the loop. The others will fail and try again. This exam-
ple illustrates that CAS is a universal primitive for single-location atomic update.
A similar primitive, known as load_linked/store_conditional, is available
on ARM, MIPS, and Power processors; we consider it in Exercise 13.7.
■
In our discussions thus far, we have used a deﬁnition of “blocking” that comes
from operating systems: a thread that blocks gives up the core instead of actively
spinning. An alternative deﬁnition comes from the theory of concurrent algo-
rithms. Here the choice between spinning and giving up the core is immaterial: a
thread is said to be “blocked” if it cannot make forward progress without action
by other threads. Conversely, an operation is said to be nonblocking if in every
reachable state of the system, any thread executing that operation is guaranteed
to complete in a ﬁnite number of steps if it gets to run by itself (without further
interference by other threads).
In this theoretical sense of the word, locks are inherently blocking, regardless of
implementation: if one thread holds a lock, no other thread that needs that lock
can proceed. By contrast, the CAS-based code of Example 13.29 is nonblocking:
if the CAS operation fails, it is because some other thread has made progress.
658
Chapter 13 Concurrency
Moreover if all threads but one stop running (e.g., because of preemption), the
remaining thread is guaranteed to make progress.
We can generalize from Example 13.29 to design a variety of special-purpose
concurrent data structures that operate without locks. Modiﬁcations to these
structures often (though not always) follow the pattern
repeat
prepare
CAS
–– (or some other atomic operation)
until success
clean up
If it reads more than one location, the “prepare” part of the algorithm may need
to double-check to make sure that none of the values has changed (i.e., that all
were read consistently) before moving on to the CAS. A read-only operation may
simply return once this double-checking is successful.
In the CAS-based update of Example 13.29, the “prepare” part of the algorithm
reads the old value of x and ﬁgures out what the new value ought to be; the “clean
up” part is empty. In other algorithms there may be signiﬁcant cleanup. In all
cases, the keys to correctness are that (1) the “prepare” part is harmless if we need
to repeat; (2) the CAS, if successful, logically completes the operation in a way that
is visible to all other threads; and (3) the “clean up,” if needed, can be performed
by any thread if the original thread is delayed. Performing cleanup for another
thread’s operation is often referred to as helping.
Figure 13.10 illustrates a widely used nonblocking concurrent queue. The
EXAMPLE 13.30
The M&S queue
dequeue operation does not require cleanup, but the enqueue operation does.
To add an element to the end of the queue, a thread reads the current tail pointer
to ﬁnd the last node in the queue, and uses a CAS to change the next pointer of
that node to point to the new node instead of being null. If the CAS succeeds (no
other thread has already updated the relevant next pointer), then the new node
has been inserted. As cleanup, the tail pointer must be updated to point to the
new node, but any thread can do this—and will, if it discovers that tail–>next is
not null.
■
Nonblocking algorithms have several advantages over blocking algorithms.
They are inherently tolerant of page faults and preemption: if a thread stops run-
ning partway through an operation, it never prevents other threads from making
progress. Nonblocking algorithms can also safely be used in signal (event) and
interrupt handlers, avoiding problems like the one described in Example 13.22.
For several important data structures and algorithms, including stacks, queues,
sorted lists, priority queues, hash tables, and memory management, nonblocking
algorithms can also be faster than locks. Unfortunately, these algorithms tend to
be exceptionally subtle and difﬁcult to devise. They are used primarily in the im-
plementation of language-level concurrency mechanisms and in standard library
packages.
13.3 Implementing Synchronization
659
Dequeue
Enqueue
head
head
tail
tail
. . .
. . .
step 1
head
tail
step 2
. . .


![Figure 13.10 Operations on...](images/page_692_caption_Figure%2013.10%20Operations%20on%20a%20nonblocking%20concurrent%20queue.%20In%20the%20dequeue%20operation%20%28left%29%2C%20a%20single.png)
*Figure 13.10 Operations on a nonblocking concurrent queue. In the dequeue operation (left), a single CAS swings the head pointer to the next node in the queue. In the enqueue operation (right), a ﬁrst CAS changes the next pointer of the tail node to point at the new node, at which point the operation is said to have logically completed. A subsequent “cleanup” CAS, which can be performed by any thread, swings the tail pointer to point at the new node as well.*

13.3.3 Memory Consistency
In all our discussions so far, we have depended, implicitly, on hardware memory
coherence. Unfortunately, coherence alone is not enough to make a multipro-
cessor or even a single multicore processor behave as most programmers would
expect. We must also worry, when more than one location is written at about the
same time, about the order in which the writes become visible to different cores.
Intuitively, most programmers expect shared memory to be sequentially con-
sistent—to make all writes visible to all cores in the same order, and to make any
given core’s writes visible in the order they were performed. Unfortunately, this
behavior turns out to be very hard to implement efﬁciently—hard enough that
most hardware designers simply don’t provide it. Instead, they provide one of sev-
eral relaxed memory models, in which certain loads and stores may appear to occur
“out of order.” Relaxed consistency has important ramiﬁcations for language de-
signers, compiler writers, and the implementors of synchronization mechanisms
and nonblocking algorithms.
The Cost of Ordering
The fundamental problem with sequential consistency is that straightforward im-
plementations require both hardware and compilers to serialize operations that
we would rather be able to perform in arbitrary order.
Consider, for example, the implementation of an ordinary store instruction.
EXAMPLE 13.31
Write buffers and
consistency
In the event of a cache miss, this instruction can take hundreds of cycles to com-
plete.
Rather than wait, most processors are designed to continue executing
660
Chapter 13 Concurrency
subsequent instructions while the store completes “in the background.” Stores
that are not yet visible in even the L1 cache (or that occurred after a store that
is not yet visible) are kept in a queue called the write buffer. Loads are checked
against the entries in this buffer, so a core always sees its own previous stores, and
sequential programs execute correctly.
But consider a concurrent program in which thread A sets a ﬂag (call it
inspected) to true and then reads location X. At roughly the same time, thread B
updates X from 0 to 1 and then reads the ﬂag. If B’s read reveals that inspected
has not yet been set, the programmer might naturally assume that A is going to
read new value (1) for X: after all, B updates X before checking the ﬂag, and A sets
the ﬂag before reading X, so A cannot have read X already. On most machines,
however, A can read X while its write of inspected is still in its write buffer. Like-
wise, B can read inspected while its write of X is still in its write buffer. The result
can be very unintuitive behavior:
Initially:  inspected = false;  X = 0
Core A:
Core B:
inspected := true
X := 1
xa := X
ib := inspected
A’s write of inspected precedes its read of X in program order. B’s write of X
precedes its read of inspected in program order. B’s read of inspected appears to
precede A’s write of inspected, because it sees the unset value. And yet A’s read of
X appears to precede B’s write of X as well, leaving us with xA = 0 and ib = false.
This sort of “temporal loop” may also be caused by standard compiler opti-
mizations. Traditionally, a compiler is free to reorder instructions (in the ab-
sence of a data dependence) to improve the expected performance of the proces-
sor pipelines. In this example, a compiler that generates code for either A or B
(without considering the other) may choose to reverse the order of operations on
inspected and X, producing an apparent temporal loop even in the absence of
hardware reordering.
■
Forcing Order with Fences and Synchronization Instructions
To avoid temporal loops, implementors of concurrent languages and libraries
must generally use special synchronization or memory fence instructions. At some
expense, these force orderings not normally guaranteed by the hardware.3 Their
presence also inhibits instruction reordering in the compiler.
In Example 13.31, both A and B must prevent their read from bypassing (com-
pleting before) the logically earlier write. Typically this can be accomplished by
3
Fences are also sometimes known as memory barriers. They are unrelated to the garbage col-
lection barriers of Section 8.5.3 (“Tracing Collection”), the synchronization barriers of Sec-
tion 13.3.1, or the RTL barriers of Section C 15.2.1.
13.3 Implementing Synchronization
661
Initially:  X = Y = 0
Core A:
Core B:
X := 1
Y := 1
Core C:
Core D:
cx := X
dy := Y
cy := Y
dx := X


![Figure 13.11 Concurrent propagation...](images/page_694_caption_Figure%2013.11%20Concurrent%20propagation%20of%20writes.%20On%20some%20machines%2C%20it%20is%20possible%20for%20concur-%20rent%20wri.png)
*Figure 13.11 Concurrent propagation of writes. On some machines, it is possible for concur- rent writes to reach cores in different orders. Arrows show apparent temporal ordering. Here C may read cy = 0 and cx = 1, while D reads dx = 0 and dy = 1.*

identifying either the read or the write as a synchronization instruction (e.g., by
implementing it with an XCHG instruction on the x86) or by inserting a fence
between them (e.g., membar StoreLoad on the SPARC).
Sometimes, as in Example 13.31, the use of synchronization or fence instruc-
tions is enough to restore intuitive behavior. Other cases, however, require more
signiﬁcant changes to the program. An example appears in Figure 13.11. Cores
EXAMPLE 13.32
Distributed consistency
A and B write locations X and Y, respectively. Both locations are read by cores
C and D. If C is physically close to A in a distributed memory machine, and D
is close to B, and if coherence messages propagate concurrently, we must con-
sider the possibility that C and D will see the writes in opposite orders, leading to
another temporal loop.
On machines where this problem arises, fences and synchronization instruc-
tions may not sufﬁce to solve the problem. The language or library implementor
(or even the application programmer) may need to bracket the writes of X and Y
with (fenced) writes to some common location, to ensure that one of the origi-
nal writes completes before the other starts. The most straightforward way to do
this is to enclose the writes in a lock-based critical section. Even then, additional
measures may be needed to ensure that the reads of X and Y are not executed out
of order by either C or D.
■
Simplifying Language-Level Reasoning
For programs running on a single core, regardless of the complexity of the un-
derlying pipeline and memory hierarchy, every manufacturer guarantees that in-
structions will appear to occur in program order: no instruction will fail to see
the effects of some prior instruction, nor will it see the effects of any subsequent
instruction. For programs running on a muilticore or multiprocessor machine,
manufacturers also guarantee, under certain circumstances, that instructions ex-
ecuted on one core will be seen, in order, by instructions on other cores. Unfor-
tunately, these circumstances vary from one machine to another. Some imple-
mentations of the MIPS and PA-RISC processors were sequentially consistent, as
are IBM’s z-Series mainframe machines: if a load on core B sees a value written
662
Chapter 13 Concurrency
by a store on core A, then, transitively, everything before the store on A is guar-
anteed to have happened before everything after the load on B. Other processors
and implementations are more relaxed. In particular, most machines admit the
loop of Example 13.31. The SPARC and x86 preclude the loop of Example 13.32
(Figure 13.11), but the Power, ARM, and Itanium all allow it.
Given this variation across machines, what is a language designer to do? The
answer, ﬁrst suggested by Sarita Adve and now embedded in Java, C++, and (less
formally) other languages, is to deﬁne a memory model that captures the notion
of a “properly synchronized” program, and then provide the illusion of sequen-
tial consistency for all such programs. In effect, the memory model constitutes a
contract between the programmer and the language implementation: if the pro-
grammer follows the rules of the contract, the implementation will hide all the
ordering eccentricities of the underlying hardware.
In the usual formulation, memory models distinguish between “ordinary”
variable accesses and special synchronization accesses; the latter include not only
lock acquire and release, but also reads and writes of any variable declared with
a special synchronization keyword (volatile in Java or C#, atomic in C++).
Ordering of operations across cores is based solely on synchronization accesses.
We say that operation A happens before operation C (A ≺HB C) if (1) A comes
before C in program order in a single thread; (2) A and C are synchronization
operations and the language deﬁnition says that A comes before C; or (3) there
exists an operation B such that A ≺HB B and B ≺HB C.
Two ordinary accesses are said to conﬂict if they occur in different threads, they
refer to the same location, and at least one of them is a write. They are said to con-
stitute a data race if they conﬂict and they are not ordered—the implementation
might allow either one to happen ﬁrst, and program behavior might change as
a result. Given these deﬁnitions, the memory model contract is straightforward:
executions of data-race–free programs are always sequentially consistent.
In most cases, an acquire of a mutual exclusion lock is ordered after the most
recent prior release. A read of a volatile (atomic) variable is ordered after
the write that stored the value that was read. Various other operations (e.g., the
transactions we will study in Section 13.4.4) may also contribute to cross-thread
ordering.
A simple example of ordering appears in Figure 13.12, where thread A sets
EXAMPLE 13.33
Using volatile to avoid a
data race
variable initialized to indicate that it is safe for thread B to use reference p. If
initialized had not been declared as volatile, there would be no cross-thread or-
dering between A’s write of true and B’s loop-terminating read. Access to both
initialized and p would then be data races. Under the hood, the compiler would
have been free to move the write of true before the initialization of p (remem-
ber, threads are often separately compiled, and the compiler has no obvious way
to tell that the writes to p and initialized have anything to do with one another).
Similarly, on a machine with a relaxed hardware memory model, the processor
and memory system would have been free to perform the writes in either order,
regardless of the order speciﬁed by the compiler in machine code. On B’s core, it
would also have been possible for either the compiler or the hardware to read p
13.3 Implementing Synchronization
663
Initially:  p : foo = null
               initialized : volatile Boolean = false
Thread A:
p:= new foo()
Thread B:
repeat
    −− nothing −− spin
until initialized
synchronization
order
initialized := true
x := p.a
program order


![Figure 13.12 Protecting initialization...](images/page_696_caption_Figure%2013.12%20Protecting%20initialization%20with%20a%20volatile%20%EF%AC%82ag.%20Here%20labeling%20initialized%20as%20volatile%20av.png)
*Figure 13.12 Protecting initialization with a volatile ﬂag. Here labeling initialized as volatile avoids a data race, and ensures that B will not access p until it is safe to do so.*

before conﬁrming that initialized was true. The volatile declaration precludes all
these undesirable possibilities.
Returning to Example 13.31, we might avoid a temporal loop by declaring both
X and inspected as volatile, or by enclosing accesses to them in atomic oper-
ations, bracketed by lock acquire and release. In Example 13.32, volatile dec-
larations on X and Y will again sufﬁce to ensure sequential consistency, but the
cost may be somewhat higher: on some machines, the compiler may need to use
extra locks or special instructions to force a total order among writes to disjoint
locations.
■
Synchronization races are common in multithreaded programs. Whether they
are bugs or expected behavior depends on the application. Data races, on the
other hand, are almost always program bugs. They are so hard to reason about—
and so rarely useful—that the C++ memory model outlaws them altogether:
given a program with a data race, a C++ implementation is permitted to display
any behavior whatsoever. Ada has similar rules. For Java, by contrast, an empha-
sis on embedded applications motivated the language designers to constrain the
behavior of racy programs in ways that would preserve the integrity of the under-
lying virtual machine. A Java program that contains a data race must continue to
follow all the normal language rules, and any read that is not ordered with respect
to a unique preceding write must return a value that might have been written by
some previous write to the same location, or by a write that is unordered with
respect to the read. We will return to the Java Memory Model in Section 13.4.3,
after we have discussed the language’s synchronization mechanisms.
13.3.4 Scheduler Implementation
To implement user-level threads, OS-level processes must synchronize access to
the ready list and condition queues, generally by means of spinning. Code for
EXAMPLE 13.34
Scheduling threads on
processes
a simple reentrant thread scheduler (one that can be “reentered” safely by a sec-
ond process before the ﬁrst one has returned) appears in Figure 13.13. As in the
code in Section 13.2.4, we disable timer signals before entering scheduler code, to
664
Chapter 13 Concurrency
shared scheduler lock : low level lock
shared ready list : queue of thread
per-process private current thread : thread
procedure reschedule()
–– assume that scheduler lock is already held and that timer signals are disabled
t : thread
loop
t := dequeue(ready list)
if t ̸= null
exit
–– else wait for a thread to become runnable
release lock(scheduler lock)
–– window allows another thread to access ready list (no point in reenabling
–– signals; we’re already trying to switch to a different thread)
acquire lock(scheduler lock)
transfer(t)
–– caller must release scheduler lock and reenable timer signals after we return
procedure yield()
disable signals()
acquire lock(scheduler lock)
enqueue(ready list, current thread)
reschedule()
release lock(scheduler lock)
reenable signals()
procedure sleep on(ref Q : queue of thread)
–– assume that caller has already disabled timer signals and acquired
–– scheduler lock, and will reverse these actions when we return
enqueue(Q, current thread)
reschedule()


![Figure 13.13 Pseudocode for...](images/page_697_caption_Figure%2013.13%20Pseudocode%20for%20part%20of%20a%20simple%20reentrant%20%28parallelism-safe%29%20scheduler.%20Every%20process%20h.png)
*Figure 13.13 Pseudocode for part of a simple reentrant (parallelism-safe) scheduler. Every process has its own copy of current thread. There is a single shared scheduler lock and a single ready list. If processes have dedicated cores, then the low level lock can be an ordinary spin lock; otherwise it can be a “spin-then-yield” lock (Figure 13.14). The loop inside reschedule busy-waits until the ready list is nonempty. The code for sleep on cannot disable timer signals and acquire the scheduler lock itself, because the caller needs to test a condition and then block as a single atomic operation.*

protect the ready list and condition queues from concurrent access by a process
and its own signal handler.
■
Our code assumes a single “low-level” lock (scheduler lock) that protects the
entire scheduler. Before saving its context block on a queue (e.g., in yield or
EXAMPLE 13.35
A race condition in thread
scheduling
sleep on), a thread must acquire the scheduler lock. It must then release the lock
after returning from reschedule. Of course, because reschedule calls transfer,
the lock will usually be acquired by one thread (the same one that disables timer
13.3 Implementing Synchronization
665
signals) and released by another (the same one that reenables timer signals). The
code for yield can implement synchronization itself, because its work is self-
contained. The code for sleep on, on the other hand, cannot, because a thread
must generally check a condition and block if necessary as a single atomic opera-
tion:
disable signals()
acquire lock(scheduler lock)
if not desired condition
sleep on(condition queue)
release lock(scheduler lock)
reenable signals()
If the signal and lock operations were moved inside of sleep on, the following
race could arise: thread A checks the condition and ﬁnds it to be false; thread B
makes the condition true, but ﬁnds the condition queue to be empty; thread A
sleeps on the condition queue forever.
■
A spin lock will sufﬁce for the “low-level” lock that protects the ready list and
condition queues, so long as every process runs on a different core. As we noted in
Section 13.2.1, however, it makes little sense to spin for a condition that can only
be made true by some other process using the core on which we are spinning. If
we know that we’re running on a uniprocessor, then we don’t need a lock on the
scheduler (just the disabled signals). If we might be running on a uniprocessor,
EXAMPLE 13.36
A “spin-then-yield” lock
however, or on a multiprocessor with fewer cores than processes, then we must
be prepared to give up the core if unable to obtain a lock. The easiest way to do
this is with a “spin-then-yield” lock, ﬁrst suggested by Ousterhout [Ous82]. A
simple example of such a lock appears in Figure 13.14. On a multiprogrammed
machine, it might also be desirable to relinquish the core inside reschedule when
the ready list is empty: though no other process of the current application will
be able to do anything, overall system throughput may improve if we allow the
operating system to give the core to a process from another application.
■
On a large multiprocessor we might increase concurrency by employing a sep-
arate lock for each condition queue, and another for the ready list. We would
have to be careful, however, to make sure it wasn’t possible for one process to put
a thread into a condition queue (or the ready list) and for another process to at-
tempt to transfer into that thread before the ﬁrst process had ﬁnished transferring
out of it (see Exercise 13.13).
Scheduler-Based Synchronization
The problem with busy-wait synchronization is that it consumes processor
cycles—cycles that are therefore unavailable for other computation. Busy-wait
synchronization makes sense only if (1) one has nothing better to do with the
current core, or (2) the expected wait time is less than the time that would be
required to switch contexts to some other thread and then switch back again. To
ensure acceptable performance on a wide variety of systems, most concurrent
666
Chapter 13 Concurrency
type lock = Boolean := false;
procedure acquire lock(ref L : lock)
while not test and set(L)
count := TIMEOUT
while L
count −:= 1
if count = 0
OS yield()
–– relinquish core and drop priority
count := TIMEOUT
procedure release lock(ref L : lock)
L := false


![Figure 13.14 A simple...](images/page_699_caption_Figure%2013.14%20A%20simple%20spin-then-yield%20lock%2C%20designed%20for%20execution%20on%20a%20multiprocessor%20that%20may%20be%20m.png)
*Figure 13.14 A simple spin-then-yield lock, designed for execution on a multiprocessor that may be multiprogrammed (i.e., on which OS-level processes may be preempted). If unable to acquire the lock in a ﬁxed, short amount of time, a process calls the OS scheduler to yield its core and to lower its priority enough that other processes (if any) will be allowed to run. Hopefully the lock will be available the next time the yielding process is scheduled for execution.*

programming languages employ scheduler-based synchronization mechanisms,
which switch to a different thread when the one that was running blocks.
In the following subsection we consider semaphores, the most common form of
scheduler-based synchronization. In Section 13.4 we consider the higher-level no-
tions of monitors, conditional critical regions, and transactional memory. In each
case, scheduler-based synchronization mechanisms remove the waiting thread
from the scheduler’s ready list, returning it only when the awaited condition is
true (or is likely to be true). By contrast, a spin-then-yield lock is still a busy-wait
mechanism: the currently running process relinquishes the core, but remains on
the ready list. It will perform a test_and_set operation every time the lock
appears to be free, until it ﬁnally succeeds. It is worth noting that busy-wait syn-
chronization is generally “level-independent”—it can be thought of as synchro-
nizing threads, processes, or cores, as desired. Scheduler-based synchronization
is “level-dependent”—it is speciﬁc to threads when implemented in the language
run-time system, or to processes when implemented in the operating system.
We will use a bounded buffer abstraction to illustrate the semantics of various
EXAMPLE 13.37
The bounded buffer
problem
scheduler-based synchronization mechanisms. A bounded buffer is a concurrent
queue of limited size into which producer threads insert data, and from which
consumer threads remove data. The buffer serves to even out ﬂuctuations in the
relative rates of progress of the two classes of threads, increasing system through-
put. A correct implementation of a bounded buffer requires both atomicity and
condition synchronization: the former to ensure that no thread sees the buffer in
an inconsistent state in the middle of some other thread’s operation; the latter to
force consumers to wait when the buffer is empty and producers to wait when the
buffer is full.
■
13.3 Implementing Synchronization
667
type semaphore = record
N : integer –– always non-negative
Q : queue of threads
procedure P(ref S : semaphore)
disable signals()
acquire lock(scheduler lock)
if S.N > 0
S.N −:= 1
else
sleep on(S.Q)
release lock(scheduler lock)
reenable signals()
procedure V(ref S : semaphore)
disable signals()
acquire lock(scheduler lock)
if S.Q is nonempty
enqueue(ready list, dequeue(S.Q))
else
S.N +:= 1
release lock(scheduler lock)
reenable signals()


![Figure 13.15 Semaphore operations,...](images/page_700_caption_Figure%2013.15%20Semaphore%20operations%2C%20for%20use%20with%20the%20scheduler%20code%20of%20Figure%2013.13.png)
*Figure 13.15 Semaphore operations, for use with the scheduler code of Figure 13.13.*

13.3.5 Semaphores
Semaphores are the oldest of the scheduler-based synchronization mechanisms.
They were described by Dijkstra in the mid-1960s [Dij68a], and appear in Al-
gol 68. They are still heavily used today, particularly in library-based implemen-
tations of concurrency.
A semaphore is basically a counter with two associated operations, P and V.4 A
EXAMPLE 13.38
Semaphore
implementation
thread that calls V atomically increments the counter. A thread that calls P waits
until the counter is positive and then decrements it. We generally require that
semaphores be fair, in the sense that threads complete P operations in the order
that they started them. Implementations of P and V in terms of our scheduler
operations appear in Figure 13.15. Note that we have elided the matching incre-
ment and decrement when a V allows a thread that is waiting in P to continue
right away.
■
A semaphore whose counter is initialized to one and for which P and V oper-
ations always occur in matched pairs is known as a binary semaphore. It serves as
4
P stands for the Dutch word passeren (to pass) or proberen (to test); V stands for vrijgeven (to
release) or verhogen (to increment). To keep them straight, speakers of English may wish to think
of P as standing for “pause,” since a thread will pause at a P operation if the semaphore count is
negative. Algol 68 calls the P and V operations down and up, respectively.
668
Chapter 13 Concurrency
shared buf : array [1..SIZE] of bdata
shared next full, next empty : integer := 1, 1
shared mutex : semaphore := 1
shared empty slots, full slots : semaphore := SIZE, 0
procedure insert(d : bdata)
P(empty slots)
P(mutex)
buf[next empty] := d
next empty := next empty mod SIZE + 1
V(mutex)
V(full slots)
function remove() : bdata
P(full slots)
P(mutex)
d : bdata := buf[next full]
next full := next full mod SIZE + 1
V(mutex)
V(empty slots)
return d


![Figure 13.16 Semaphore-based code...](images/page_701_caption_Figure%2013.16%20Semaphore-based%20code%20for%20a%20bounded%20buffer.%20The%20mutex%20binary%20semaphore%20protects%20the%20data.png)
*Figure 13.16 Semaphore-based code for a bounded buffer. The mutex binary semaphore protects the data structure proper. The full slots and empty slots general semaphores ensure that no operation starts until it is safe to do so.*

a scheduler-based mutual exclusion lock: the P operation acquires the lock; V re-
leases it. More generally, a semaphore whose counter is initialized to k can be used
to arbitrate access to k copies of some resource. The value of the counter at any
particular time indicates the number of copies not currently in use. Exercise 13.19
notes that binary semaphores can be used to implement general semaphores, so
the two are of equal expressive power, if not of equal convenience.
Figure 13.16 shows a semaphore-based solution to the bounded buffer prob-
EXAMPLE 13.39
Bounded buffer with
semaphores
lem. It uses a binary semaphore for mutual exclusion, and two general (or count-
ing) semaphores for condition synchronization. Exercise 13.17 considers the use
of semaphores to construct an n-thread barrier.
■
3CHECK YOUR UNDERSTANDING
25. What is mutual exclusion? What is a critical section?
26. What does it mean for an operation to be atomic? Explain the difference be-
tween atomicity and condition synchronization.
27. Describe the behavior of a test_and_set instruction. Show how to use it to
build a spin lock.
28. Describe the behavior of the compare_and_swap instruction. What advan-
tages does it offer in comparison to test_and_set?
