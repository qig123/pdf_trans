# 13.6 Summary and Concluding Remarks

**688**
Chapter 13* Concurrency*

**IN MORE DEPTH**

Three central issues in message-based concurrency—naming, sending, and
receiving—are explored on the companion site. A name may refer directly to a
process, to some communication resource associated with a process (often called
an* entry* or* port*), or to an independent* socket* or* channel* abstraction. A* send* op-
eration may be entirely asynchronous, in which case the sender continues while
the underlying system attempts to deliver the message, or the sender may wait,
typically for acknowledgment of receipt or for the return of a reply. A* receive*
operation, for its part, may be executed explicitly, or it may implicitly trigger
execution of some previously speciﬁed handler routine. When implicit receipt
is coupled with senders waiting for replies, the combination is typically known
as* remote procedure call* (RPC). In addition to message-passing libraries, RPC
systems typically rely on a language-aware tool known as a* stub compiler*.

## 13.6

**Summary and Concluding Remarks**
Concurrency and parallelism have become ubiquitous in modern computer sys-
tems. It is probably safe to say that most computer research and development
today involves concurrency in one form or another. High-end computer systems
have always been parallel, and multicore PCs and cellphones are now ubiquitous.
Even on uniprocessors, graphical and networked applications are typically con-
current.
In this chapter we have provided an introduction to concurrent programming
with an emphasis on programming language issues. We began with an overview
of the motivations for concurrency and of the architecture of modern multipro-
cessors. We then surveyed the fundamentals of concurrent software, including
communication, synchronization, and the creation and management of threads.
We distinguished between shared-memory and message-passing models of com-
munication and synchronization, and between language- and library-based im-
plementations of concurrency.
Our survey of thread creation and management described some six different
constructs for creating threads: co-begin, parallel loops, launch-at-elaboration,
fork/join, implicit receipt, and early reply. Of these fork/join is the most com-
mon; it is found in a host of languages, and in library-based packages such as
MPI and OpenMP. RPC systems typically use fork/join internally to implement
implicit receipt. Regardless of the thread-creation mechanism, most concurrent
programming systems implement their language- or library-level threads on top
of a collection of OS-level processes, which the operating system implements in
a similar manner on top of a collection of hardware cores. We built our sam-
ple implementation in stages, beginning with coroutines on a uniprocessor, then
adding a ready list and scheduler, then timers for preemption, and ﬁnally parallel
scheduling on multiple cores.

