624
Chapter 13 Concurrency
any given time, all but one of them is stopped at a well-known place. A concurrent
system is parallel if more than one task can be physically active at once; this re-
quires more than one processor. The distinction is purely an implementation and
performance issue: from a semantic point of view, there is no difference between
true parallelism and the “quasiparallelism” of a system that switches between tasks
at unpredictable times. A parallel system is distributed if its processors are associ-
ated with people or devices that are physically separated from one another in the
real world. Under these deﬁnitions, “concurrent” applies to all three motivations
above. “Parallel” applies to the second and third; “distributed” applies to only the
third.
We will focus in this chapter on concurrency and parallelism. Parallelism has
become a pressing concern since 2005 or so, with the proliferation of multicore
processors. We will have less occasion to touch on distribution. While languages
have been designed for distributed computing, most distributed systems run sep-
arate programs on every networked processor, and use message-passing library
routines to communicate among them.
We begin our study with an overview of the ways in which parallelism may
be used in modern programs. Our overview will touch on the motivation for
concurrency (even on uniprocessors) and the concept of races, which are the
principal source of complexity in concurrent programs.
We will also brieﬂy
survey the architectural features of modern multicore and multiprocessor ma-
chines. In Section 13.2 we consider the contrast between shared-memory and
message-passing models of concurrency, and between language and library-based
implementations.
Building on coroutines, we explain how a language or li-
brary can create and schedule threads. Section 13.3 focuses on low-level mecha-
nisms for shared-memory synchronization. Section 13.4 extends the discussion
to language-level constructs. Message-passing models of concurrency are consid-
ered in Section 13.5 (mostly on the companion site).
13.1
Background and Motivation
Concurrency is not a new idea. Much of the theoretical groundwork was laid in
the 1960s, and Algol 68 includes concurrent programming features. Widespread
interest in concurrency is a relatively recent phenomenon, however; it stems in
part from the availability of low-cost multicore and multiprocessor machines, and
in part from the proliferation of graphical, multimedia, and web-based applica-
tions, all of which are naturally represented by concurrent threads of control.
Levels of Parallelism
Parallelism arises at every level of a modern computer system. It is comparatively
easy to exploit at the level of circuits and gates, where signals can propagate down
thousands of connections at once. As we move up ﬁrst to processors and cores,
and then to the many layers of software that run on top of them, the granularity
13.1 Background and Motivation
625
of parallelism—the size and complexity of tasks—increases at every level, and it
becomes increasingly difﬁcult to ﬁgure out what work should be done by each
task and how tasks should coordinate.
For 40 years, microarchitectural research was largely devoted to ﬁnding more
and better ways to exploit the instruction-level parallelism (ILP) available in ma-
chine language programs. As we saw in Chapter 5, the combination of deep,
superscalar pipelines and aggressive speculation allows a modern processor to
track dependences among hundreds of “in-ﬂight” instructions, make progress on
scores of them, and complete several in every cycle. Shortly after the turn of the
century, it became apparent that a limit had been reached: there simply wasn’t
any more instruction-level parallelism available in conventional programs.
At the next higher level of granularity, so-called vector parallelism is available
in programs that perform operations repeatedly on every element of a very large
data set. Processors designed to exploit this parallelism were the dominant form
of supercomputer from the late 1960s through the early 1990s. Their legacy lives
on in the vector instructions of mainstream processors (e.g., the MMX, SSE, and
AVX extensions to the x86 instruction set), and in modern graphical processing
units (GPUs), whose peak performance can exceed that of the typical CPU (cen-
tral processing unit—a conventional core) by a factor of more than 100.
Unfortunately, vector parallelism arises in only certain kinds of programs.
Given the end of ILP, and the limits on clock frequency imposed by heat dissi-
pation (Section C 5.4.4), general-purpose computing today must obtain its per-
formance improvements from multicore processors, which require coarser-grain
thread-level parallelism. The move to multicore has thus entailed a fundamental
shift in the nature of programming: where parallelism was once a largely invisible
implementation detail, it must now be written explicitly into high-level program
structure.
Levels of Abstraction
On today’s multicore machines, different kinds of programmers need to under-
stand concurrency at different levels of detail, and use it in different ways.
The simplest, most abstract case arises when using “black box” parallel li-
braries. A sorting routine or a linear algebra package, for example, may execute
in parallel without its caller needing to understand how. In the database world,
queries expressed in SQL (Structured Query Language) often execute in paral-
lel as well. Microsoft’s .NET Framework includes a Language-Integrated Query
mechanism (LINQ) that allows database-style queries to be made of program
data structures, again with parallelism “under the hood.”
At a slightly less abstract level, a programmer may know that certain tasks
are mutually independent (because, for example, they access disjoint sets of
626
Chapter 13 Concurrency
variables). Such tasks can safely execute in parallel.1 In C#, for example, we can
EXAMPLE 13.1
Independent tasks in C#
write the following using the Task Parallel Library:
Parallel.For(0, 100, i => { A[i] = foo(A[i]); });
The ﬁrst two arguments to Parallel.For are “loop” bounds; the third is a dele-
gate, here written as a lambda expression. Assuming A is a 100-element array, and
that the invocations of foo are truly independent, this code will have the same ef-
fect as the obvious traditional for loop, except that it will run faster, making use
of as many cores as possible (up to 100).
■
If our tasks are not independent, it may still be possible to run them in parallel
if we explicitly synchronize their interactions. Synchronization serves to eliminate
races between threads by controlling the ways in which their actions can interleave
in time. Suppose function foo in the previous example subtracts 1 from A[i]
EXAMPLE 13.2
A simple race condition
and also counts the number of times that the result is zero. Naively we might
implement foo as
int zero_count;
public static int foo(int n) {
int rtn = n - 1;
if (rtn == 0) zero_count++;
return rtn;
}
Consider now what may happen when two or more instances of this code run
concurrently:
Thread 1
. . .
Thread 2
r1 := zero count
. . .
r1 := r1 + 1
r1 := zero count
zero count := r1
r1 := r1 + 1
. . .
zero count := r1
. . .
If the instructions interleave roughly as shown, both threads may load the same
value of zero count, both may increment it by one, and both may store the (only
one greater) value back into zero count. The result may be less than what we
expect.
In general, a race condition occurs whenever two or more threads are “racing”
toward points in the code at which they touch some common object, and the
behavior of the system depends on which thread gets there ﬁrst. In this particular
example, the store of zero count in Thread 1 is racing with the load in Thread 2.
1
Ideally, we might like the compiler to ﬁgure this out automatically, but the problem of indepen-
dence is undecidable in the general case.
13.1 Background and Motivation
627
If Thread 1 gets there ﬁrst, we will get the “right” result; if Thread 2 gets there
ﬁrst, we won’t.
■
The most common purpose of synchronization is to make some sequence of
instructions, known as a critical section, appear to be atomic—to happen “all at
once” from the point of view of every other thread. In our example, the critical
section is a load, an increment, and a store. The most common way to make
the sequence atomic is with a mutual exclusion lock, which we acquire before the
ﬁrst instruction of the sequence and release after the last. We will study locks in
Sections 13.3.1 and 13.3.5. In Sections 13.3.2 and 13.4.4 we will also consider
mechanisms that achieve atomicity without locks.
At lower levels of abstraction, expert programmers may need to understand
hardware and run-time systems in sufﬁcient detail to implement synchronization
mechanisms. This chapter should convey a sense of the issues, but a full treatment
at this level is beyond the scope of the current text.
13.1.1 The Case for Multithreaded Programs
Our ﬁrst motivation for concurrency—to capture the logical structure of certain
applications—has arisen several times in earlier chapters. In Section C 8.7.1 we
noted that interactive I/O must often interrupt the execution of the current pro-
gram. In a video game, for example, we must handle keystrokes and mouse or
joystick motions while continually updating the image on the screen. The stan-
dard way to structure such a program, as described in Section 9.6.2, is to execute
the input handlers in a separate thread of control, which coexists with one or
more threads responsible for updating the screen. In Section 9.5, we considered
a screen saver program that used coroutines to interleave “sanity checks” on the
ﬁle system with updates to a moving picture on the screen. We also considered
discrete-event simulation, which uses coroutines to represent the active entities
of some real-world system.
The semantics of discrete-event simulation require that events occur atomi-
cally at ﬁxed points in time. Coroutines provide a natural implementation, be-
cause they execute one at a time: so long as we never switch coroutines in the mid-
dle of a to-be-atomic operation, all will be well. In our other examples, however—
and indeed in most “naturally concurrent” programs—there is no need for corou-
tine semantics. By assigning concurrent tasks to threads instead of to coroutines,
we acknowledge that those tasks can proceed in parallel if more than one core is
available. We also move responsibility for ﬁguring out which thread should run
when from the programmer to the language implementation. In return, we give
up any notion of trivial atomicity.
The need for multithreaded programs is easily seen in web-based applications.
EXAMPLE 13.3
Multithreaded web
browser
In a browser such as Chrome or Firefox (see Figure 13.1), there are typically many
different threads simultaneously active, each of which is likely to communicate
with a remote (and possibly very slow) server several times before completing its
task. When the user clicks on a link, the browser creates a thread to request the
628
Chapter 13 Concurrency
speciﬁed document. For all but the tiniest pages, this thread will then receive a se-
ries of message “packets.” As these packets begin to arrive the thread must format
them for presentation on the screen. The formatting task is akin to typesetting:
the thread must access fonts, assemble words, and break the words into lines. For
many special tags within the page, the formatting thread will spawn additional
threads: one for each image, one for the background if any, one to format each
table, and possibly more to handle separate frames. Each spawned thread will
communicate with the server to obtain the information it needs (e.g., the con-
tents of an image) for its particular task. The user, meanwhile, can access items in
menus to create new browser windows, edit bookmarks, change preferences, and
so on, all in “parallel” with the rendering of page elements.
■
The use of many threads ensures that comparatively fast operations (e.g., dis-
play of text) do not wait for slow operations (e.g., display of large images). When-
ever one thread blocks (waits for a message or I/O), the run-time or operating
system will automatically switch execution on the core to run a different thread.
In a preemptive thread package, these context switches will occur at other times
as well, to prevent any one thread from hogging processor resources. Any reader
who remembersearly, more sequential browsers will appreciate the difference that
multithreading makes in perceived performance and responsiveness, even on a
single-core machine.
The Dispatch Loop Alternative
Without language or library support for threads, a browser must either adopt a
EXAMPLE 13.4
Dispatch loop web
browser
more sequential structure, or centralize the handling of all delay-inducing events
in a single dispatch loop (see Figure 13.2). Data structures associated with the
dispatch loop keep track of all the tasks the browser has yet to complete. The
state of a task may be quite complicated. For the high-level task of rendering a
page, the state must indicate which packets have been received and which are still
outstanding. It must also identify the various subtasks of the page (images, tables,
frames, etc.) so that we can ﬁnd them all and reclaim their state if the user clicks
on a “stop” button.
To guarantee good interactive response, we must make sure that no subaction
of continue task takes very long to execute. Clearly we must end the current ac-
tion whenever we wait for a message. We must also end it whenever we read from
a ﬁle, since disk operations are slow. Finally, if any task needs to compute for
longer than about a tenth of a second (the typical human perceptual threshold),
then we must divide the task into pieces, between which we save state and return
to the top of the loop. These considerations imply that the condition at the top
of the loop must cover the full range of asynchronous events, and that evalua-
tions of the condition must be interleaved with continued execution of any tasks
that were subdivided due to lengthy computation. (In practice we would proba-
bly need a more sophisticated mechanism than simple interleaving to ensure that
neither input-driven nor compute-bound tasks hog more than their share of re-
sources.)
■
13.1 Background and Motivation
629
procedure parse page(address : url)
contact server, request page contents
parse html header()
while current token in {“<p>”, “<h1>”, “<ul>”, . . . ,
“<background”, “<image”, “<table”, “<frameset”, . . . }
case current token of
“<p>”
: break paragraph()
“<h1>”
: format heading(); match(“< /h1>”)
“<ul>”
: format list(); match(“< /ul>”)
. . .
“<background” :
a : attributes := parse attributes()
fork render background(a)
“<image” : a : attributes := parse attributes()
fork render image(a)
“<table”
: a : attributes := parse attributes()
scan forward for “< /table>” token
token stream s :=. . .
–– table contents
fork format table(s, a)
“<frameset” :
a : attributes := parse attributes()
parse frame list(a)
match(“< /frameset>”)
. . .
. . .
procedure parse frame list(a1 : attributes)
while current token in {“<frame”, “<frameset”, “<noframes>”}
case current token of
“<frame” : a2 : attributes := parse attributes()
fork format frame(a1, a2)
. . .


![Figure 13.1 Thread-based code...](images/page_662_caption_Figure%2013.1%20Thread-based%20code%20from%20a%20hypothetical%20Web%20browser.%20To%20%EF%AC%81rst%20approximation%2C%20the%20parse%20page.png)
*Figure 13.1 Thread-based code from a hypothetical Web browser. To ﬁrst approximation, the parse page subroutine is the root of a recursive descent parser for HTML. In several cases, however, the actions associated with recognition of a construct (background, image, table, frame- set) proceed concurrently with continued parsing of the page itself. In this example, concurrent threads are created with the fork operation. An additional thread would likely execute in re- sponse to keyboard and mouse events.*

The principal problem with a dispatch loop—beyond the complexity of subdi-
viding tasks and saving state—is that it hides the algorithmic structure of the pro-
gram. Every distinct task (retrieving a page, rendering an image, walking through
nested menus) could be described elegantly with standard control-ﬂow mech-
anisms, if not for the fact that we must return to the top of the dispatch loop
at every delay-inducing operation. In effect, the dispatch loop turns the program
“inside out,” making the management of tasks explicit and the control ﬂow within
tasks implicit. The resulting complexity is similar to what we encountered when
630
Chapter 13 Concurrency
type task descriptor = record
–– ﬁelds in lieu of thread-local variables, plus control-ﬂow information
. . .
ready tasks : queue of task descriptor
. . .
procedure dispatch()
loop
–– try to do something input-driven
if a new event E (message, keystroke, etc.) is available
if an existing task T is waiting for E
continue task(T, E)
else if E can be handled quickly, do so
else
allocate and initialize new task T
continue task(T, E)
–– now do something compute bound
if ready tasks is nonempty
continue task(dequeue(ready tasks), ‘ok’)
procedure continue task(T : task, E : event)
if T is rendering an image
and E is a message containing the next block of data
continue image render(T, E)
else if T is formatting a page
and E is a message containing the next block of data
continue page parse(T, E)
else if T is formatting a page
and E is ‘ok’
–– we’re compute bound
continue page parse(T, E)
else if T is reading the bookmarks ﬁle
and E is an I/O completion event
continue goto page(T, E)
else if T is formatting a frame
and E is a push of the “stop” button
deallocate T and all tasks dependent upon it
else if E is the “edit preferences” menu item
edit preferences(T, E)
else if T is already editing preferences
and E is a newly typed keystroke
edit preferences(T, E)
. . .


![Figure 13.2 Dispatch loop...](images/page_663_caption_Figure%2013.2%20Dispatch%20loop%20from%20a%20hypothetical%20non-thread-based%20Web%20browser.%20The%20clauses%20in%20continue%20.png)
*Figure 13.2 Dispatch loop from a hypothetical non-thread-based Web browser. The clauses in continue task must cover all possible combinations of task state and triggering event. The code in each clause performs the next coherent unit of work for its task, returning when (1) it must wait for an event, (2) it has consumed a signiﬁcant amount of compute time, or (3) the task is complete. Prior to returning, respectively, code (1) places the task in a dictionary (used by dispatch) that maps awaited events to the tasks that are waiting for them, (2) enqueues the task in ready tasks, or (3) deallocates the task.*

13.1 Background and Motivation
631
trying to enumerate a recursive set with iterator objects in Section 6.5.3, only
worse. Like true iterators, a thread package turns the program “right side out,”
making the management of tasks (threads) implicit and the control ﬂow within
threads explicit.
13.1.2 Multiprocessor Architecture
Parallel computer hardware is enormously diverse. A distributed system—one
that we think of in terms of interactions among separate programs running on
separate machines—may be as large as the Internet, or as small as the components
of a cell phone. A parallel but nondistributed system—one that we think of in
terms of a single program running on a single machine—may still be very large.
China’s Tianhe-2 supercomputer, for example, has more than 3 million cores,
consumes over 17 MW of power, and occupies 720 square meters of ﬂoor space
(about a ﬁfth of an acre).
Historically, most parallel but nondistributed machines were homogeneous—
their processors were all identical. In recent years, many machines have added
programmable GPUs, ﬁrst as separate processors, and more recently as separate
portions of a single processor chip. While the cores of a GPU are internally homo-
geneous, they are very different from those of the typical CPU, leading to a glob-
ally heterogeneous system. Future systems may have cores of many other kinds as
well, each specialized to particular kinds of programs or program components.
In an ideal world, programming languages and runtimes would map program
fragments to suitable cores at suitable times, but this sort of automation is still
very much a research goal. As of 2015, programmers who want to make use of
the GPU write appropriate portions of their code in special-purpose languages
like OpenCL or CUDA, which emphasize repetitive operations over vectors. A
main program, running on the CPU, then ships the resulting “kernels” to the
GPU explicitly.
In the remainder of this chapter, we will concentrate on thread-level paral-
lelism for homogeneous machines. For these, many of the most important archi-
tectural questions involve the memory system. In some machines, all of physical
memory is accessible to every core, and the hardware guarantees that every write
is quickly visible everywhere. At the other extreme, some machines partition
main memory among processors, forcing cores to interact through some sepa-
rate message-passing mechanism. In intermediate designs, some machines share
memory in a noncoherent fashion, making writes on one core visible to another
only when both have explicitly ﬂushed their caches.
From the point of view of language or library implementation, the principal
distinction between shared-memory and message-passing hardware is that mes-
sages typically require the active participation of cores at both ends of the con-
nection: one to send, the other to receive. On a shared-memory machine, a core
can read and write remote memory without any other core’s assistance.
On small machines (2–4 processors, say), main memory may be uniform—
equally distant from all processors. On larger machines (and even on some very
632
Chapter 13 Concurrency
small machines), memory may be nonuniform instead—each bank may be physi-
cally adjacent to a particular processor or small group of processors. Cores in any
processor can then access the memory of any other, but local memory is faster.
Assuming all memory is cached, of course, the difference appears only on cache
misses, where the penalty for local memory is lower.
Memory Coherence
As suggested by the notion of noncoherent memory, caches introduce a serious
problem for shared-memory machines: unless we do something special, a core
that has cached a particular memory location may run for an arbitrarily long
time without seeing changes that have been made to that location by other cores.
This problem—how to keep cached copies of a memory location consistent with
EXAMPLE 13.5
The cache coherence
problem
DESIGN & IMPLEMENTATION
13.1 What, exactly, is a processor?
From roughly 1975 to 2005, a processor typically ran only one thread at a
time, and occupied one full chip. Today, most vendors still use the term “pro-
cessor” to refer to the physical device that “does the computing,” and whose
pins connect it to the rest of the computer, but the internal structure is much
more complicated: there may be more than one chip inside the physical pack-
age, each chip may have multiple cores (each of which would have been called
a “processor” in previous hardware generations), and each core may have
multiple hardware threads (independent register sets, which allow the core’s
pipeline(s) to run a mix of instructions drawn from multiple software threads).
A modern processor may also include many megabytes of on-chip cache, or-
ganized into multiple levels, and physically distributed and shared among the
cores in complicated ways. Increasingly, processors may incorporate on-chip
memory controllers, network interfaces, graphical processing units, or other
formerly “peripheral” components, making continued use of the term “pro-
cessor” problematic but no less common.
From a software perspective, the good news is that operating systems and
programming languages generally model every concurrent activity as a thread,
regardless of whether it shares a core, a chip, or a package with other threads.
We will follow this convention for most of the rest of this chapter, ignoring the
complexity of the underlying hardware. When we need to refer to the hardware
on which a thread runs, we will usually call it a “core.” The bad news is that
a model of computing in which “everything is just a thread” hides details that
are crucial to understanding and improving performance. Future chips are
likely to include ever larger numbers of heterogeneous cores and complex on-
chip networks. To use these chips effectively, language implementations will
need to become much more sophisticated about scheduling threads onto the
underlying hardware. How much of the task will need to be visible to the
application programmer remains to be determined.
13.1 Background and Motivation
633
Core A
Core B
Core Z
Cache
X : 4
Cache
X : 3
Cache
...
Bus
Memory
X : 3


![Figure 13.3 The cache...](images/page_666_caption_Figure%2013.3%20The%20cache%20coherence%20problem%20for%20shared-memory%20multicore%20and%20multiprocessor%20machines.%20Her.png)
*Figure 13.3 The cache coherence problem for shared-memory multicore and multiprocessor machines. Here cores A and B have both read variable X from memory. As a side effect, a copy of X has been created in the cache of each core. If A now changes X to 4 and B reads X again, how do we ensure that the result is a 4 and not the still-cached 3? Similarly, if Z reads X into its cache, how do we ensure that it obtains the 4 from A’s cache instead of the stale 3 from memory?*

one another—is known as the coherence problem (see Figure 13.3). On a simple
bus-based machine, the problem is relatively easy to solve: the broadcast nature
of the communication medium allows cache controllers to eavesdrop (snoop) on
the memory trafﬁc of other cores. When a core needs to write a cache line, it
requests an exclusive copy, and waits for other cores to invalidate their copies. On
a bus the waiting is trivial, and the natural ordering of messages determines who
wins in the event of near-simultaneous requests. Cores that try to access a line in
the wake of invalidation must go back to memory (or to another core’s cache) to
obtain an up-to-date copy.
■
Bus-based cache coherence algorithms are now a standard, built-in part of
most commercial microprocessors. On large machines, the lack of a broadcast bus
makes cache coherence a signiﬁcantly more difﬁcult problem; commercial imple-
mentations are available, but they are complex and expensive. On both small and
large machines, the fact that coherence is not instantaneous (it takes time for no-
tiﬁcations to propagate) means that we must consider the order in which updates
to different locations appear to occur from the point of view of different proces-
sors. Ensuring a consistent view is a surprisingly difﬁcult problem; we will return
to it in Section 13.3.3.
As of 2015, there are multicore versions of every major instruction set ar-
chitecture, including ARM, x86, Power, SPARC, x86-64, and IA-64 (Itanium).
Small, cache-coherent multiprocessors built from these are available from dozens
of manufacturers. Larger, cache-coherent shared-memory multiprocessors are
available from several manufacturers, including Oracle, HP, IBM, and SGI.
Supercomputers
Though dwarfed ﬁnancially by the rest of the computer industry, supercomput-
ing has always played a disproportionate role in the development of computer
634
Chapter 13 Concurrency
technology and the advancement of human knowledge. Supercomputers have
changed dramatically over time, and they continue to evolve at a very rapid pace.
They have always, however, been parallel machines.
Because of the complexity of cache coherence, it is difﬁcult to build large
shared-memory machines. SGI sells machines with as many as 256 processors
(2048 cores). Cray builds even larger shared-memory machines, but without the
ability to cache remote locations. For the most part, however, the vector super-
computers of the 1960s–80s were displaced not by large multiprocessors, but by
modest numbers of smaller multiprocessors or by very large numbers of com-
modity (mainstream) processors, connected by custom high-performance net-
works. As network technology “trickled down” into the broader market, these
machines in turn gave way to clusters composed of both commodity multicore
processors and commodity networks (Gigabit Ethernet or Inﬁniband). As of
2015, clusters have come to dominate everything from modest server farms up
to all but the very fastest supercomputer sites. Large-scale on-line services like
Google, Amazon, or Facebook are typically backed by clusters with tens or hun-
dreds of thousands of cores (in Google’s case, probably millions).
Today’s fastest machines are constructed from special high-density multicore
chips with low per-core operating power. The Tianhe-2 (the fastest machine in
the world as of June 2015) uses a 2:3 mix of Intel 12-core Ivy Bridge and 61-core
Phi processors, at 10 W and 5 W per core, respectively. Given current trends, it
seems likely that future machines, both high-end and commodity, will be increas-
ingly dense and increasingly heterogeneous.
From a programming language perspective, the special challenge of supercom-
puting is to accommodate nonuniform access times and (in most cases) the lack
of hardware support for shared memory across the full machine. Today’s su-
percomputers are programmed mostly with message-passing libraries (MPI in
particular) and with languages and libraries in which there is a clear syntactic
distinction between local and remote memory access.
3CHECK YOUR UNDERSTANDING
1.
Explain the distinctions among concurrent, parallel, and distributed.
2.
Explain the motivation for concurrency. Why do people write concurrent
programs? What accounts for the increased interest in concurrency in recent
years?
3.
Describe the implementation levels at which parallelism appears in modern
systems, and the levels of abstraction at which it may be considered by the
programmer.
4.
What is a race condition? What is synchronization?
5.
What is a context switch? Preemption?
6.
Explain the concept of a dispatch loop. What are its advantages and disadvan-
tages with respect to multithreaded code?
