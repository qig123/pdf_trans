# 13.1 Background and Motivation

**624**
Chapter 13* Concurrency*

any given time, all but one of them is stopped at a well-known place. A concurrent
system is* parallel* if more than one task can be physically* active* at once; this re-
quires more than one processor. The distinction is purely an implementation and
performance issue: from a semantic point of view, there is no difference between
true parallelism and the “quasiparallelism” of a system that switches between tasks
at unpredictable times. A parallel system is* distributed* if its processors are associ-
ated with people or devices that are physically separated from one another in the
real world. Under these deﬁnitions, “concurrent” applies to all three motivations
above. “Parallel” applies to the second and third; “distributed” applies to only the
third.
We will focus in this chapter on concurrency and parallelism. Parallelism has
become a pressing concern since 2005 or so, with the proliferation of multicore
processors. We will have less occasion to touch on distribution. While languages
have been designed for distributed computing, most distributed systems run sep-
arate programs on every networked processor, and use message-passing library
routines to communicate among them.
We begin our study with an overview of the ways in which parallelism may
be used in modern programs. Our overview will touch on the motivation for
concurrency (even on uniprocessors) and the concept of* races*, which are the
principal source of complexity in concurrent programs.
We will also brieﬂy
survey the architectural features of modern multicore and multiprocessor ma-
chines. In Section 13.2 we consider the contrast between shared-memory and
message-passing models of concurrency, and between language and library-based
implementations.
Building on coroutines, we explain how a language or li-
brary can create and schedule threads. Section 13.3 focuses on low-level mecha-
nisms for shared-memory synchronization. Section 13.4 extends the discussion
to language-level constructs. Message-passing models of concurrency are consid-
ered in Section 13.5 (mostly on the companion site).
## 13.1

**Background and Motivation**
Concurrency is not a new idea. Much of the theoretical groundwork was laid in
the 1960s, and Algol 68 includes concurrent programming features. Widespread
interest in concurrency is a relatively recent phenomenon, however; it stems in
part from the availability of low-cost multicore and multiprocessor machines, and
in part from the proliferation of graphical, multimedia, and web-based applica-
tions, all of which are naturally represented by concurrent threads of control.

**Levels of Parallelism**

Parallelism arises at every level of a modern computer system. It is comparatively
easy to exploit at the level of circuits and gates, where signals can propagate down
thousands of connections at once. As we move up ﬁrst to processors and cores,
and then to the many layers of software that run on top of them, the* granularity*

```
A simple race condition
and also counts the number of times that the result is zero. Naively we might
implement foo as
```

```
int zero_count;
public static int foo(int n) {
int rtn = n - 1;
if (rtn == 0) zero_count++;
return rtn;
}
```

Consider now what may happen when two or more instances of this code run
concurrently:

**Thread 1**
. . .
**Thread 2**
r1 := zero count
. . .
r1 := r1 + 1
r1 := zero count
zero count := r1
r1 := r1 + 1
. . .
zero count := r1
. . .

If the instructions interleave roughly as shown, both threads may load the same
value of zero count, both may increment it by one, and both may store the (only
one greater) value back into zero count. The result may be less than what we
expect.
In general, a* race condition* occurs whenever two or more threads are “racing”
toward points in the code at which they touch some common object, and the
behavior of the system depends on which thread gets there ﬁrst. In this particular
example, the store of zero count in Thread 1 is racing with the load in Thread 2.

**1**
Ideally, we might like the compiler to ﬁgure this out automatically, but the problem of indepen-
dence is undecidable in the general case.

