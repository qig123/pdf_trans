13.8 Explorations
695
13.29
In the previous exercise you may have noticed that the dining philosophers
are prone to deadlock. One has to worry about the possibility that all ﬁve
of them will pick up their right-hand forks simultaneously, and then wait
forever for their left-hand neighbors to ﬁnish eating.
Discuss as many strategies as you can think of to address the deadlock
problem. Can you describe a solution in which it is provably impossible
for any philosopher to go hungry forever? Can you describe a solution that
is fair in a strong sense of the word (i.e., in which no one philosopher gets
more chance to eat than some other over the long term)? For a particularly
elegant solution, see the paper by Chandy and Misra [CM84].
13.30
In some concurrent programming systems, global variables are shared by
all threads. In others, each newly created thread has a separate copy of
the global variables, commonly initialized to the values of the globals of
the creating thread. Under this private globals approach, shared data must
be allocated from a special heap. In still other programming systems, the
programmer can specify which global variables are to be private and which
are to be shared.
Discuss the tradeoffs between private and shared global variables.
Which would you prefer to have available, for which sorts of programs?
How would you implement each? Are some options harder to implement
than others? To what extent do your answers depend on the nature of
processes provided by the operating system?
13.31
Rewrite Example 13.51 in Java.
13.32
AND parallelism in logic languages is analogous to the parallel evaluation
of arguments in a functional language (e.g., Multilisp). Does OR par-
allelism have a similar analog? (Hint: Think about special forms [Sec-
tion 11.5].) Can you suggest a way to obtain the effect of OR parallelism
in Multilisp?
13.33
In Section 13.4.5 we claimed that both AND parallelism and OR paral-
lelism were problematic in Prolog, because they failed to adhere to the
deterministic search order required by language semantics. Elaborate on
this claim. What speciﬁcally can go wrong?
13.34–13.38 In More Depth.
13.8
Explorations
13.39
The MMX, SSE, and AVX extensions to the x86 instruction set and the Al-
tiVec extensions to the Power instruction set make vector operations avail-
able to general-purpose code. Learn about these instructions and research
their history. What sorts of code are they used for? How are they related
to vector supercomputers? To modern graphics processors?
696
Chapter 13 Concurrency
13.40
The “Top 500” list (top500.org) maintains information, over time, on the
500 most powerful computers in the world, as measured on the Linpack
performance benchmark. Explore the site. Pay particular attention to the
historical trends in the kinds of machines deployed. Can you explain these
trends? How many cases can you ﬁnd of supercomputer technology mov-
ing into the mainstream, and vice versa?
13.41
In Section 13.3.3 we noted that different processors provide different lev-
els of memory consistency and different mechanisms to force additional
ordering when needed. Learn more about these hardware memory mod-
els. You might want to start with the tutorial by Adve and Gharachor-
loo [AG96].
13.42
In Sections 13.3.3 and 13.4.3 we presented a very high-level summary of
the Java and C++ memory models. Learn their details. Also investigate the
(more loosely speciﬁed) models of Ada and C#. How do these compare?
How efﬁciently can each be implemented on various real machines? What
are the challenges for implementors? For Java, explore the controversy that
arose around the memory model in the original deﬁnition of the language
(updated in Java 5—see the paper by Manson et al. [MPA05] for a discus-
sion). For C++, pay particular attention to the ability to specify weakened
consistency on loads and stores of atomic variables.
13.43
In Section 13.3.2 we presented a brief introduction to the design of non-
blocking concurrent data structures, which work correctly without locks.
Learn more about this topic. How hard is it to write correct nonblocking
code? How does the performance compare to that of lock-based code? You
might want to start with the work of Michael [MS98] and Sundell [Sun04].
For a more theoretical foundation, start with Herlihy’s original article on
wait freedom [Her91] and the more recent concept of obstruction free-
dom [HLM03], or check out the text by Herlihy and Shavit [HS12].
13.44
As possible improvements to reader-writer locks, learn about sequence
locks [Lam05] and the RCU (read-copy update) synchronization id-
iom [MAK+01]. Both of these are heavily used in the operating systems
community. Discuss the challenges involved in applying them to code
written by “nonexperts.”
13.45
The ﬁrst software transactional memory systems grew out of work on non-
blocking concurrent data structures, and were in fact nonblocking. Most
recent systems, however, are lock based. Read the position paper by En-
nals [Enn06] and the more recent papers of Marathe and Moir [MM08]
and Tabba et al. [TWGM07]. What do you think? Should TM systems be
nonblocking?
13.46
The most widely used language-level transactional memory is the STM
monad of Haskell, supported by the Glasgow Haskell compiler and run-
time system. Read up on its syntax and implementation [HMPH05]. Pay
