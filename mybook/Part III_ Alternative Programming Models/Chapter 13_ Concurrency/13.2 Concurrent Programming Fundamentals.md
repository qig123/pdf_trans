# 13.2 Concurrent Programming Fundamentals

13.2 Concurrent Programming Fundamentals
**635**

7.
Explain the distinction between a* multiprocessor* and a* cluster*; between a* pro-*
*cessor* and a* core*.
8.
What does it mean for memory in a multiprocessor to be* uniform*? What is
the alternative?
9.
Explain the* coherence problem* for multicore and multiprocessor caches.

10. What is a* vector machine*? Where does vector technology appear in modern
systems?

## 13.2

**Concurrent Programming Fundamentals**
Within a concurrent program, we will use the term* thread* to refer to the active
entity that the programmer thinks of as running concurrently with other threads.
In most systems, the threads of a given program are implemented on top of one or
more* processes* provided by the operating system. OS designers often distinguish
between a* heavyweight* process, which has its own address space, and a collection
of* lightweight* processes, which may share an address space. Lightweight processes
were added to most variants of Unix in the late 1980s and early 1990s, to accom-
modate the proliferation of shared-memory multiprocessors.
We will sometimes use the word* task* to refer to a well-deﬁned unit of work
that must be performed by some thread. In one common programming idiom, a
collection of threads shares a common “bag of tasks”—a list of work to be done.
Each thread repeatedly removes a task from the bag, performs it, and goes back
for another. Sometimes the work of a task entails adding new tasks to the bag.
Unfortunately, terminology is inconsistent across systems and authors. Several
languages call their threads processes. Ada calls them tasks. Several operating sys-
tems call lightweight processes threads. The Mach OS, from which OSF Unix and
Mac OS X are derived, calls the address space shared by lightweight processes a
task. A few systems try to avoid ambiguity by coining new words, such as “actors,”
“ﬁbers,” or “ﬁlaments.” We will attempt to use the deﬁnitions of the preceding
two paragraphs consistently, and to identify cases in which the terminology of
particular languages or systems differs from this usage.

13.2.1** Communication and Synchronization**

In any concurrent programming model, two of the most crucial issues to be ad-
dressed are* communication* and* synchronization*. Communication refers to any
mechanism that allows one thread to obtain information produced by another.
Communication mechanisms for imperative programs are generally based on
either* shared memory* or* message passing*. In a shared-memory programming
model, some or all of a program’s variables are accessible to multiple threads.

13.2.2** Languages and Libraries**

```
Thread-level concurrency can be provided to the programmer in the form of ex-
plicitly concurrent languages, compiler-supported extensions to traditional se-
quential languages, or library packages outside the language proper. All three
options are widely used, though shared-memory languages are more common at
the “low end” (for multicore and small multiprocessor machines), and message-
passing libraries are more common at the “high end” (for massively parallel su-
percomputers). Examples of systems in widespread use are categorized in Fig-
ure 13.4.
For many years, almost all parallel programming employed traditional sequen-
tial languages (largely C and Fortran) augmented with libraries for synchroniza-
tion or message passing, and this approach still dominates today. In the Unix
world, shared memory parallelism has largely converged on the POSIX pthreads
standard, which includes mechanisms to create, destroy, schedule, and synchro-
nize threads. This standard became an ofﬁcial part of both C and C++ as of
their 2011 versions. Similar functionality for Windows machines is provided by
Microsoft’s thread package and compilers. For high-end scientiﬁc computing,
message-based parallelism has likewise converged on the MPI (Message Passing
Interface) standard, with open-source and commercial implementations available
for almost every platform.
While language support for concurrency goes back all the way to Algol 68 (and
coroutines to Simula), and while such support was widely available in Ada by
the late 1980s, widespread interest in these features didn’t really arise until the
mid-1990s, when the explosive growth of the World Wide Web began to drive
the development of parallel servers and concurrent client programs. This devel-
opment coincided nicely with the introduction of Java, and Microsoft followed
with C# a few years later. Though not yet as inﬂuential, many other languages,
including Erlang, Go, Haskell, Rust, and Scala, are explicitly parallel as well.
```

```
A parallel loop in OpenMP
#pragma omp parallel for
for (int i = 0; i < 3; i++) {
printf("thread %d here\n", i);
}
■
```

In C# with the Task Parallel Library, the equivalent code looks like this:
**EXAMPLE** 13.9

```
A parallel loop in C#
Parallel.For(0, 3, i => {
Console.WriteLine("Thread " + i + "here");
});
```

```
The third argument to Parallel.For is a delegate, in this case a lambda ex-
pression. A similar Foreach method expects two arguments—an iterator and a
delegate.
■
In many systems it is the programmer’s responsibility to make sure that con-
current execution of the loop iterations is safe, in the sense that correctness will
never depend on the outcome of race conditions. Access to global variables, for
example, must generally be synchronized, to make sure that iterations do not
```

## ...

## ...

Thread Ma
Process Ma

Process Mj

Thread Mb

Thread Ml

### ...

### ...


![Figure 13.6 Two-level implementation...](images/page_681_vector_312.png)
*Figure 13.6 Two-level implementation of threads. A thread scheduler, implemented in a library or language run-time package, multiplexes threads on top of one or more kernel-level processes, just as the process scheduler, implemented in the operating system kernel, multiplexes processes on top of one or more physical cores.*

top of one or more physical cores. We will use the terminology of threads on top
of processes in the remainder of this section.
The typical implementation starts with coroutines (Section 9.5). Recall that
coroutines are a sequential control-ﬂow mechanism: the programmer can sus-
pend the current coroutine and resume a speciﬁc alternative by calling the
transfer operation. The argument to transfer is typically a pointer to the con-
text block of the coroutine.
To turn coroutines into threads, we proceed in a series of three steps. First, we
hide the argument to transfer by implementing a* scheduler* that chooses which
thread to run next when the current thread yields the core. Second, we imple-
ment a* preemption* mechanism that suspends the current thread automatically on
a regular basis, giving other threads a chance to run. Third, we allow the data
structures that describe our collection of threads to be shared by more than one
OS process, possibly on separate cores, so that threads can run on any of the pro-
cesses.

**Uniprocessor Scheduling**


![Figure 13.7 illustrates the...](images/page_681_vector_576.png)
*Figure 13.7 illustrates the data structures employed by a simple scheduler. At any EXAMPLE 13.21*

Cooperative
multithreading on a
uniprocessor

particular time, a thread is either* blocked* (i.e., for synchronization) or* runnable*.
A runnable thread may actually be running on some process or it may be awaiting

